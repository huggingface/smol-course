# ä½¿ç”¨ transformers çš„ pipeline è¿›è¡ŒåŸºæœ¬çš„æ¨¡å‹æ¨ç†

ğŸ¤— Transformers ä¸­çš„ `pipeline` æŠ½è±¡ï¼Œä¸ºä½¿ç”¨ Hugging Face æ¨¡å‹åº“ä¸­çš„ä»»ä½•æ¨¡å‹è¿›è¡Œæ¨ç†æä¾›äº†ä¸€ç§ç®€å•çš„æ–¹æ³•ã€‚å®ƒå¤„ç†äº†æ‰€æœ‰çš„é¢„å¤„ç†å’Œåå¤„ç†æ­¥éª¤ï¼Œä½¿å¾—åœ¨æ— éœ€æ·±å…¥äº†è§£æ¨¡å‹æ¶æ„æˆ–è¦æ±‚çš„æƒ…å†µä¸‹å°±èƒ½è½»æ¾ä½¿ç”¨è¿™äº›æ¨¡å‹ã€‚

## pipeline çš„å·¥ä½œåŸç†

Hugging Face çš„ pipeline é€šè¿‡å°†åŸå§‹è¾“å…¥å’Œäººç±»å¯è¯»è¾“å‡ºä¹‹é—´çš„ä¸‰ä¸ªå…³é”®é˜¶æ®µè‡ªåŠ¨åŒ–ï¼Œç®€åŒ–äº†æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ï¼š

**é¢„å¤„ç†é˜¶æ®µ**
pipeline é¦–å…ˆå°†ä½ çš„åŸå§‹è¾“å…¥ä¸ºæ¨¡å‹åšå¥½å‡†å¤‡ã€‚è¿™ä¼šå› è¾“å…¥ç±»å‹è€Œå¼‚ï¼š
- æ–‡æœ¬è¾“å…¥ä¼šç»è¿‡åˆ†è¯å¤„ç†ï¼Œå°†å•è¯è½¬æ¢ä¸ºå¯¹æ¨¡å‹å‹å¥½çš„ token ID
- å›¾åƒä¼šè¢«è°ƒæ•´å¤§å°å¹¶è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œä»¥ç¬¦åˆæ¨¡å‹è¦æ±‚
- éŸ³é¢‘ä¼šé€šè¿‡ç‰¹å¾æå–è¿›è¡Œå¤„ç†ï¼Œä»¥åˆ›å»ºé¢‘è°±å›¾æˆ–å…¶ä»–è¡¨ç¤ºå½¢å¼

**æ¨¡å‹æ¨ç†**
åœ¨æ­£å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œpipeline å®ç°äº†è¿™äº›äº‹æƒ…ï¼š
- è‡ªåŠ¨è¿›è¡Œè¾“å…¥çš„ batch å¤„ç†ï¼Œä»¥å®ç°é«˜æ•ˆå¤„ç†
- é€‰æ‹©æœ€ä¼˜è®¡ç®—è®¾å¤‡ï¼ˆCPU/GPUï¼‰è¿›è¡Œè®¡ç®—
- å¦‚æœç¡¬ä»¶å¯ä»¥æ”¯æŒï¼Œä¼šä½¿ç”¨è¯¸å¦‚åŠç²¾åº¦ï¼ˆFP16ï¼‰æ¨ç†ç­‰æŠ€æœ¯è¿›è¡Œæ€§èƒ½ä¼˜åŒ–

**åå¤„ç†é˜¶æ®µ**
æœ€åï¼Œpipeline å°†åŸå§‹çš„æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºæœ‰ç”¨çš„ç»“æœï¼š
- å°† token ID è§£ç å›å¯è¯»æ–‡æœ¬
- å°† logits å€¼è½¬æ¢ä¸ºæ¦‚ç‡å€¼
- æ ¹æ®å…·ä½“ä»»åŠ¡ï¼ˆä¾‹å¦‚åˆ†ç±»æ ‡ç­¾ã€ç”Ÿæˆæ–‡æœ¬ï¼‰ï¼Œå¯¹è¾“å‡ºè¿›è¡Œæ ¼å¼åŒ–

è¿™ç§æŠ½è±¡è®©ä½ å¯ä»¥ä¸“æ³¨äºåº”ç”¨ç¨‹åºé€»è¾‘ï¼Œè€Œç®¡é“ä¼šå¤„ç†æ¨¡å‹æ¨ç†çš„æŠ€æœ¯å¤æ‚æ€§ã€‚

## åŸºæœ¬ç”¨æ³•

ä¸‹é¢ç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨ pipeline è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼š

```python
from transformers import pipeline

# Create a pipeline with a specific model
generator = pipeline(
    "text-generation",
    model="HuggingFaceTB/SmolLM2-1.7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)

# Generate text
response = generator(
    "Write a short poem about coding:",
    max_new_tokens=100,
    do_sample=True,
    temperature=0.7
)
print(response[0]['generated_text'])
```

## å…³é”®é…ç½®

### è½½å…¥æ¨¡å‹
```python
# CPU inference
generator = pipeline("text-generation", model="HuggingFaceTB/SmolLM2-1.7B-Instruct", device="cpu")

# GPU inference (device 0)
generator = pipeline("text-generation", model="HuggingFaceTB/SmolLM2-1.7B-Instruct", device=0)

# Automatic device placement
generator = pipeline(
    "text-generation",
    model="HuggingFaceTB/SmolLM2-1.7B-Instruct",
    device_map="auto",
    torch_dtype="auto"
)
```

### ç”Ÿæˆç›¸å…³çš„å‚æ•°

```python
response = generator(
    "Translate this to French:",
    max_new_tokens=100,     # ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦
    do_sample=True,         # è§£ç æ—¶ç”¨é‡‡æ ·çš„ç­–ç•¥ï¼Œè€Œä¸æ˜¯è´ªå¿ƒç­–ç•¥
    temperature=0.7,        # è¿™ä¸ªå‚æ•°å¯ä»¥æ§åˆ¶éšæœºæ€§ï¼Œå€¼è¶Šå¤§è¶Šéšæœº
    top_k=50,               # é‡‡æ ·æ—¶ï¼Œåªè€ƒè™‘æœ€é å‰çš„å‰ k ä¸ª token
    top_p=0.95,             # é‡‡æ ·æ—¶ï¼Œæ¦‚ç‡å€¼çš„é˜ˆå€¼
    num_return_sequences=1  # é’ˆå¯¹ä¸€ä¸ªè¾“å…¥è¾“å‡ºå‡ ä¸ªè¾“å‡º
)
```

## åŒæ—¶å¤„ç†å¤šä¸ªè¾“å…¥

Pipeline å¯ä»¥å€ŸåŠ© batch çš„æŠ€æœ¯ï¼Œé«˜æ•ˆåœ°åŒæ—¶å¤„ç†å¤šä¸ªè¾“å…¥ï¼š

```python
# Prepare multiple prompts
prompts = [
    "Write a haiku about programming:",
    "Explain what an API is:",
    "Write a short story about a robot:"
]

# Process all prompts efficiently
responses = generator(
    prompts,
    batch_size=4,              # Number of prompts to process together
    max_new_tokens=100,
    do_sample=True,
    temperature=0.7
)

# Print results
for prompt, response in zip(prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response[0]['generated_text']}\n")
```

## é›†æˆå…¥ç½‘é¡µç«¯æœåŠ¡

ä¸‹é¢ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•å°† pipeline é›†æˆå…¥ FastAPI åº”ç”¨ï¼š

```python
from fastapi import FastAPI, HTTPException
from transformers import pipeline
import uvicorn

app = FastAPI()

# Initialize pipeline globally
generator = pipeline(
    "text-generation",
    model="HuggingFaceTB/SmolLM2-1.7B-Instruct",
    device_map="auto"
)

@app.post("/generate")
async def generate_text(prompt: str):
    try:
        if not prompt:
            raise HTTPException(status_code=400, detail="No prompt provided")
            
        response = generator(
            prompt,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7
        )
        
        return {"generated_text": response[0]['generated_text']}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=5000)
```

## å±€é™æ€§

è™½ç„¶ Pipeline å¯¹äºåŸå‹è®¾è®¡å’Œå°è§„æ¨¡éƒ¨ç½²å¾ˆæœ‰ç”¨ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼š

- ä¸ä¸“ç”¨æœåŠ¡è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼Œä¼˜åŒ–é€‰é¡¹æœ‰é™ã€‚
- ä¸æ”¯æŒåŠ¨æ€ batch å¤„ç†ç­‰é«˜çº§ç‰¹æ€§ã€‚
- å¯èƒ½ä¸é€‚åˆé«˜ååé‡çš„ç”Ÿäº§å·¥ä½œè´Ÿè½½ã€‚

å¯¹äºæœ‰é«˜ååé‡è¦æ±‚çš„ç”Ÿäº§éƒ¨ç½²ï¼Œå¯è€ƒè™‘ä½¿ç”¨ TGI æˆ–å…¶ä»–ä¸“é—¨çš„æœåŠ¡è§£å†³æ–¹æ¡ˆã€‚


## å‚è€ƒèµ„æ–™

- [Hugging Face çš„ Pipeline æ•™ç¨‹](https://huggingface.co/docs/transformers/en/pipeline_tutorial)
- [Pipeline API å‚è€ƒèµ„æ–™](https://huggingface.co/docs/transformers/en/main_classes/pipelines)
- [Text Generation å‚æ•°æ–‡æ¡£](https://huggingface.co/docs/transformers/en/main_classes/text_generation)
- [æ¨¡å‹é‡åŒ–æŒ‡å—](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one)