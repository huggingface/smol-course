# Suy luáº­n cÆ¡ báº£n vá»›i Pipeline Transformers

Abstraction `pipeline` trong ğŸ¤— Transformers cung cáº¥p má»™t cÃ¡ch Ä‘Æ¡n giáº£n Ä‘á»ƒ thá»±c hiá»‡n suy luáº­n vá»›i báº¥t ká»³ mÃ´ hÃ¬nh nÃ o tá»« Hugging Face Hub. NÃ³ xá»­ lÃ½ táº¥t cáº£ cÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ vÃ  háº­u xá»­ lÃ½, giÃºp ngÆ°á»i dÃ¹ng dá»… dÃ ng sá»­ dá»¥ng mÃ´ hÃ¬nh mÃ  khÃ´ng cáº§n kiáº¿n thá»©c sÃ¢u vá» kiáº¿n trÃºc hay yÃªu cáº§u cá»§a chÃºng.

## CÃ¡ch hoáº¡t Ä‘á»™ng cá»§a Pipelines

Pipelines cá»§a Hugging Face Ä‘Æ¡n giáº£n hÃ³a quy trÃ¬nh há»c mÃ¡y báº±ng cÃ¡ch tá»± Ä‘á»™ng hÃ³a ba giai Ä‘oáº¡n quan trá»ng giá»¯a Ä‘áº§u vÃ o thÃ´ vÃ  Ä‘áº§u ra cÃ³ thá»ƒ Ä‘á»c Ä‘Æ°á»£c:

**Giai Ä‘oáº¡n Tiá»n xá»­ lÃ½**
Pipeline sáº½ chuáº©n bá»‹ cÃ¡c Ä‘áº§u vÃ o thÃ´ cho mÃ´ hÃ¬nh. Äiá»u nÃ y thay Ä‘á»•i tÃ¹y thuá»™c vÃ o loáº¡i Ä‘áº§u vÃ o:
- Äáº§u vÃ o vÄƒn báº£n tráº£i qua quÃ¡ trÃ¬nh phÃ¢n tÃ¡ch (tokenization) Ä‘á»ƒ chuyá»ƒn cÃ¡c tá»« thÃ nh cÃ¡c ID token phÃ¹ há»£p vá»›i mÃ´ hÃ¬nh
- HÃ¬nh áº£nh Ä‘Æ°á»£c thay Ä‘á»•i kÃ­ch thÆ°á»›c vÃ  chuáº©n hÃ³a Ä‘á»ƒ phÃ¹ há»£p vá»›i yÃªu cáº§u cá»§a mÃ´ hÃ¬nh
- Ã‚m thanh Ä‘Æ°á»£c xá»­ lÃ½ thÃ´ng qua viá»‡c trÃ­ch xuáº¥t tÃ­nh nÄƒng Ä‘á»ƒ táº¡o ra phá»• táº§n (spectrograms) hoáº·c cÃ¡c biá»ƒu diá»…n khÃ¡c

**Suy luáº­n MÃ´ hÃ¬nh**
Trong quÃ¡ trÃ¬nh cháº¡y mÃ´ hÃ¬nh, pipeline:
- Tá»± Ä‘á»™ng xá»­ lÃ½ batching cÃ¡c Ä‘áº§u vÃ o Ä‘á»ƒ tÄƒng hiá»‡u quáº£
- Äáº·t tÃ­nh toÃ¡n lÃªn thiáº¿t bá»‹ tá»‘i Æ°u (CPU/GPU)
- Ãp dá»¥ng cÃ¡c tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t nhÆ° suy luáº­n vá»›i Ä‘á»™ chÃ­nh xÃ¡c ná»­a (FP16) khi Ä‘Æ°á»£c há»— trá»£

**Giai Ä‘oáº¡n Háº­u xá»­ lÃ½**
Cuá»‘i cÃ¹ng, pipeline chuyá»ƒn Ä‘á»•i Ä‘áº§u ra thÃ´ tá»« mÃ´ hÃ¬nh thÃ nh káº¿t quáº£ há»¯u Ã­ch:
- Giáº£i mÃ£ cÃ¡c ID token trá»Ÿ láº¡i thÃ nh vÄƒn báº£n cÃ³ thá»ƒ Ä‘á»c Ä‘Æ°á»£c
- Biáº¿n Ä‘á»•i cÃ¡c logits thÃ nh Ä‘iá»ƒm xÃ¡c suáº¥t
- Äá»‹nh dáº¡ng Ä‘áº§u ra theo tÃ¡c vá»¥ cá»¥ thá»ƒ (vÃ­ dá»¥: nhÃ£n phÃ¢n loáº¡i, vÄƒn báº£n sinh ra)

Abstraction nÃ y giÃºp báº¡n táº­p trung vÃ o logic á»©ng dá»¥ng trong khi pipeline xá»­ lÃ½ sá»± phá»©c táº¡p ká»¹ thuáº­t cá»§a suy luáº­n mÃ´ hÃ¬nh.

## CÃ¡ch sá»­ dá»¥ng cÆ¡ báº£n

DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch sá»­ dá»¥ng pipeline Ä‘á»ƒ táº¡o vÄƒn báº£n:

```python
from transformers import pipeline

# Táº¡o pipeline vá»›i má»™t mÃ´ hÃ¬nh cá»¥ thá»ƒ
generator = pipeline(
    "text-generation",
    model="HuggingFaceTB/SmolLM2-1.7B-Instruct",
    torch_dtype="auto",
    device_map="auto"
)

# Táº¡o vÄƒn báº£n
response = generator(
    "Write a short poem about coding:",
    max_new_tokens=100,
    do_sample=True,
    temperature=0.7
)
print(response[0]['generated_text'])
```

## CÃ¡c tÃ¹y chá»n cáº¥u hÃ¬nh chÃ­nh

### Táº£i mÃ´ hÃ¬nh
```python
# Suy luáº­n trÃªn CPU
generator = pipeline("text-generation", model="HuggingFaceTB/SmolLM2-1.7B-Instruct", device="cpu")

# Suy luáº­n trÃªn GPU
generator = pipeline("text-generation", model="HuggingFaceTB/SmolLM2-1.7B-Instruct", device=0)

# Äáº·t thiáº¿t bá»‹ tá»± Ä‘á»™ng
generator = pipeline(
    "text-generation",
    model="HuggingFaceTB/SmolLM2-1.7B-Instruct",
    device_map="auto",
    torch_dtype="auto"
)
```

### Tham sá»‘ táº¡o vÄƒn báº£n

```python
response = generator(
    "Translate this to French:",
    max_new_tokens=100,     # Äá»™ dÃ i tá»‘i Ä‘a cá»§a vÄƒn báº£n sinh ra
    do_sample=True,         # Sá»­ dá»¥ng sampling thay vÃ¬ greedy decoding
    temperature=0.7,        # Äiá»u chá»‰nh Ä‘á»™ ngáº«u nhiÃªn (cao hÆ¡n = ngáº«u nhiÃªn hÆ¡n)
    top_k=50,               # Giá»›i háº¡n Ä‘áº¿n top k token
    top_p=0.95,             # NgÆ°á»¡ng sampling háº¡t nhÃ¢n
    num_return_sequences=1  # Sá»‘ lÆ°á»£ng vÄƒn báº£n sinh ra khÃ¡c nhau
)
```

## Xá»­ lÃ½ nhiá»u Ä‘áº§u vÃ o

Pipelines cÃ³ thá»ƒ xá»­ lÃ½ nhiá»u Ä‘áº§u vÃ o má»™t cÃ¡ch hiá»‡u quáº£ thÃ´ng qua batching:

```python
# Chuáº©n bá»‹ nhiá»u prompt
prompts = [
    "Write a haiku about programming:",
    "Explain what an API is:",
    "Write a short story about a robot:"
]

# Xá»­ lÃ½ táº¥t cáº£ cÃ¡c prompt má»™t cÃ¡ch hiá»‡u quáº£
responses = generator(
    prompts,
    batch_size=4,              # Sá»‘ lÆ°á»£ng prompt xá»­ lÃ½ cÃ¹ng má»™t lÃºc
    max_new_tokens=100,
    do_sample=True,
    temperature=0.7
)

# In káº¿t quáº£
for prompt, response in zip(prompts, responses):
    print(f"Prompt: {prompt}")
    print(f"Response: {response[0]['generated_text']}\n")
```

## TÃ­ch há»£p vá»›i Web Server

DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡ch tÃ­ch há»£p pipeline vÃ o má»™t á»©ng dá»¥ng FastAPI:

```python
from fastapi import FastAPI, HTTPException
from transformers import pipeline
import uvicorn

app = FastAPI()

# Khá»Ÿi táº¡o pipeline toÃ n cá»¥c
generator = pipeline(
    "text-generation",
    model="HuggingFaceTB/SmolLM2-1.7B-Instruct",
    device_map="auto"
)

@app.post("/generate")
async def generate_text(prompt: str):
    try:
        if not prompt:
            raise HTTPException(status_code=400, detail="No prompt provided")
            
        response = generator(
            prompt,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7
        )
        
        return {"generated_text": response[0]['generated_text']}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=5000)
```

## Háº¡n cháº¿

Máº·c dÃ¹ pipelines ráº¥t há»¯u Ã­ch cho viá»‡c táº¡o máº«u vÃ  triá»ƒn khai quy mÃ´ nhá», chÃºng cÃ³ má»™t sá»‘ háº¡n cháº¿:

- CÃ¡c tÃ¹y chá»n tá»‘i Æ°u hÃ³a háº¡n cháº¿ so vá»›i cÃ¡c giáº£i phÃ¡p phá»¥c vá»¥ chuyÃªn dá»¥ng
- KhÃ´ng há»— trá»£ cÃ¡c tÃ­nh nÄƒng nÃ¢ng cao nhÆ° batching Ä‘á»™ng
- CÃ³ thá»ƒ khÃ´ng phÃ¹ há»£p cho cÃ¡c khá»‘i lÆ°á»£ng cÃ´ng viá»‡c sáº£n xuáº¥t yÃªu cáº§u tá»‘c Ä‘á»™ cao

Äá»‘i vá»›i cÃ¡c triá»ƒn khai sáº£n xuáº¥t vá»›i yÃªu cáº§u throughput cao, báº¡n cÃ³ thá»ƒ xem xÃ©t sá»­ dá»¥ng Suy luáº­n Sinh vÄƒn Báº±ng VÄƒn báº£n (TGI) hoáº·c cÃ¡c giáº£i phÃ¡p phá»¥c vá»¥ chuyÃªn biá»‡t khÃ¡c.

## TÃ i nguyÃªn

- [HÆ°á»›ng dáº«n Pipeline Hugging Face.](https://huggingface.co/docs/transformers/en/pipeline_tutorial)
- [Tham kháº£o API Pipeline.](https://huggingface.co/docs/transformers/en/main_classes/pipelines)
- [Tham sá»‘ Sinh vÄƒn báº£n.](https://huggingface.co/docs/transformers/en/main_classes/text_generation)
- [HÆ°á»›ng dáº«n Quantization mÃ´ hÃ¬nh.](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one)