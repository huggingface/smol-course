{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-6LLOPZouLg"
   },
   "source": [
    "# C√°ch tinh ch·ªânh m√¥ h√¨nh LLM v·ªõi b·ªô ƒëi·ªÅu h·ª£p LoRA s·ª≠ d·ª•ng Hugging Face TRL\n",
    "\n",
    "Notebook n√†y tr√¨nh b√†y c√°ch tinh ch·ªânh hi·ªáu qu·∫£ c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn s·ª≠ d·ª•ng b·ªô ƒëi·ªÅu h·ª£p LoRA (Th√≠ch nghi h·∫°ng th·∫•p). LoRA l√† m·ªôt k·ªπ thu·∫≠t tinh ch·ªânh hi·ªáu qu·∫£ v·ªÅ tham s·ªë v·ªõi c√°c ƒë·∫∑c ƒëi·ªÉm:\n",
    "- ƒê√≥ng bƒÉng tr·ªçng s·ªë m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán tr∆∞·ªõc\n",
    "- Th√™m c√°c ma tr·∫≠n ph√¢n r√£ h·∫°ng nh·ªè c√≥ th·ªÉ hu·∫•n luy·ªán v√†o c√°c l·ªõp attention\n",
    "- Th∆∞·ªùng gi·∫£m kho·∫£ng 90% s·ªë tham s·ªë c·∫ßn hu·∫•n luy·ªán\n",
    "- Duy tr√¨ hi·ªáu su·∫•t m√¥ h√¨nh trong khi v·∫´n ti·∫øt ki·ªám b·ªô nh·ªõ\n",
    "\n",
    "Ch√∫ng ta s·∫Ω t√¨m hi·ªÉu:\n",
    "1. Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng ph√°t tri·ªÉn v√† c·∫•u h√¨nh LoRA\n",
    "2. T·∫°o v√† chu·∫©n b·ªã t·∫≠p d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán b·ªô ƒëi·ªÅu h·ª£p\n",
    "3. Tinh ch·ªânh s·ª≠ d·ª•ng `trl` v√† `SFTTrainer` v·ªõi b·ªô ƒëi·ªÅu h·ª£p LoRA\n",
    "4. Ki·ªÉm th·ª≠ m√¥ h√¨nh v√† k·∫øt h·ª£p b·ªô ƒëi·ªÅu h·ª£p (t√πy ch·ªçn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXqd9BXgouLi"
   },
   "source": [
    "## 1. Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng \n",
    "\n",
    "B∆∞·ªõc ƒë·∫ßu ti√™n l√† c√†i ƒë·∫∑t Th∆∞ vi·ªán Hugging Face v√† Pyroch, bao g·ªìm trl, transformers v√† datasets. N·∫øu b·∫°n ch∆∞a nghe n√≥i v·ªÅ trl, ƒë·ª´ng lo l·∫Øng. ƒê√¢y l√† m·ªôt th∆∞ vi·ªán m·ªõi tr√™n ƒë·∫ßu transformers v√† datasets, gi√∫p tinh ch·ªânh c√°c LLM m·ªü d·ªÖ d√†ng h∆°n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKvGVxImouLi"
   },
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# ƒêƒÉng nh·∫≠p v√†o Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()\n",
    "\n",
    "# ƒê·ªÉ ti·ªán l·ª£i, h√£y t·∫°o m·ªôt bi·∫øn m√¥i tr∆∞·ªùng l∆∞u token c·ªßa b·∫°n nh∆∞ l√† HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHUzfwpKouLk"
   },
   "source": [
    "## 2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "z4p6Bvo7ouLk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 2260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T·∫£i b·ªô d·ªØ li·ªáu t·ª´ Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: h√£y thay ƒë·ªïi path v√† name cho ph√π h·ª£p v·ªõi b·ªô d·ªØ li·ªáu b·∫°n mu·ªën t·∫£i\n",
    "dataset = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TOhJdtsouLk"
   },
   "source": [
    "## 3. Tinh ch·ªânh LLM s·ª≠ d·ª•ng `trl` v√† `SFTTrainer` v·ªõi LoRA\n",
    "\n",
    "[SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) t·ª´ `trl` cung c·∫•p t√≠ch h·ª£p v·ªõi b·ªô ƒëi·ªÅu h·ª£p LoRA th√¥ng qua th∆∞ vi·ªán [PEFT](https://huggingface.co/docs/peft/en/index). Nh·ªØng l·ª£i th·∫ø ch√≠nh c·ªßa c√°ch thi·∫øt l·∫≠p n√†y bao g·ªìm:\n",
    "\n",
    "1. **Hi·ªáu qu·∫£ b·ªô nh·ªõ**: \n",
    "   - Ch·ªâ c√°c tham s·ªë b·ªô ƒëi·ªÅu h·ª£p ƒë∆∞·ª£c l∆∞u trong b·ªô nh·ªõ GPU\n",
    "   - Tr·ªçng s·ªë m√¥ h√¨nh c∆° s·ªü v·∫´n ƒë∆∞·ª£c ƒë√≥ng bƒÉng v√† c√≥ th·ªÉ ƒë∆∞·ª£c t·∫£i v·ªõi ƒë·ªô ch√≠nh x√°c th·∫•p h∆°n\n",
    "   - Cho ph√©p tinh ch·ªânh c√°c m√¥ h√¨nh l·ªõn tr√™n GPU th√¥ng d·ª•ng\n",
    "\n",
    "2. **T√≠nh nƒÉng hu·∫•n luy·ªán**:\n",
    "   - T√≠ch h·ª£p s·∫µn PEFT/LoRA v·ªõi thi·∫øt l·∫≠p t·ªëi thi·ªÉu\n",
    "   - H·ªó tr·ª£ QLoRA (LoRA l∆∞·ª£ng t·ª≠ h√≥a) ƒë·ªÉ hi·ªáu qu·∫£ b·ªô nh·ªõ t·ªët h∆°n n·ªØa\n",
    "\n",
    "3. **Qu·∫£n l√Ω b·ªô ƒëi·ªÅu h·ª£p**:\n",
    "   - L∆∞u tr·ªçng s·ªë b·ªô ƒëi·ªÅu h·ª£p trong c√°c ƒëi·ªÉm ki·ªÉm tra\n",
    "   - C√°c t√≠nh nƒÉng ƒë·ªÉ k·∫øt h·ª£p b·ªô ƒëi·ªÅu h·ª£p tr·ªü l·∫°i m√¥ h√¨nh c∆° s·ªü\n",
    "\n",
    "Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng LoRA trong v√≠ d·ª• c·ªßa m√¨nh, k·∫øt h·ª£p LoRA v·ªõi l∆∞·ª£ng t·ª≠ h√≥a 4-bit ƒë·ªÉ gi·∫£m th√™m m·ª©c s·ª≠ d·ª•ng b·ªô nh·ªõ m√† kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn hi·ªáu su·∫•t. Thi·∫øt l·∫≠p ch·ªâ y√™u c·∫ßu m·ªôt v√†i b∆∞·ªõc c·∫•u h√¨nh:\n",
    "1. X√°c ƒë·ªãnh c·∫•u h√¨nh LoRA (h·∫°ng, alpha, dropout)\n",
    "2. T·∫°o SFTTrainer v·ªõi c·∫•u h√¨nh PEFT\n",
    "3. Hu·∫•n luy·ªán v√† l∆∞u tr·ªçng s·ªë b·ªô ƒëi·ªÅu h·ª£p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# T·∫£i m√¥ h√¨nh v√† tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Tinh ch·ªânh ƒë·ªãnh d·∫°ng\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# ƒê·∫∑t t√™n cho m√¥ h√¨nh tinh ch·ªânh\n",
    "finetune_name = \"SmolLM2-FT-MyDataset\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbuVArTHouLk"
   },
   "source": [
    "`SFTTrainer` h·ªó tr·ª£ t√≠ch h·ª£p s·∫µn v·ªõi `peft`, ƒëi·ªÅu n√†y gi√∫p vi·ªác tinh ch·ªânh hi·ªáu qu·∫£ c√°c m√¥ h√¨nh LLM tr·ªü n√™n c·ª±c k·ª≥ d·ªÖ d√†ng, v√≠ d·ª• nh∆∞ s·ª≠ d·ª•ng LoRA. Ch√∫ng ta ch·ªâ c·∫ßn t·∫°o `LoraConfig` v√† cung c·∫•p n√≥ cho tr√¨nh hu·∫•n luy·ªán.\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>B√†i t·∫≠p: X√°c ƒë·ªãnh tham s·ªë LoRA cho tinh ch·ªânh</h2>\n",
    "    <p>L·∫•y m·ªôt t·∫≠p d·ªØ li·ªáu t·ª´ Hugging Face hub v√† tinh ch·ªânh m·ªôt m√¥ h√¨nh tr√™n ƒë√≥.</p>\n",
    "    <p><b>C√°c m·ª©c ƒë·ªô kh√≥</b></p>\n",
    "    <p>üê¢ S·ª≠ d·ª•ng c√°c tham s·ªë chung cho m·ªôt l·∫ßn tinh ch·ªânh b·∫•t k·ª≥</p>\n",
    "    <p>üêï ƒêi·ªÅu ch·ªânh c√°c tham s·ªë v√† ƒë√°nh gi√° trong weights & biases</p>\n",
    "    <p>ü¶Å ƒêi·ªÅu ch·ªânh c√°c tham s·ªë v√† hi·ªÉn th·ªã s·ª± thay ƒë·ªïi trong k·∫øt qu·∫£ suy lu·∫≠n</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blDSs9swouLk"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# TODO: Tinh ch·ªânh c√°c tham s·ªë LoRA\n",
    "# r: h·∫°ng c·ªßa ma tr·∫≠n LoRA (th∆∞·ªùng n·∫±m trong kho·∫£ng 4-32) , nh·ªè h∆°n t·ª©c l√† s·∫Ω n√©n nhi·ªÅu h∆°n\n",
    "rank_dimension = 6\n",
    "# lora_alpha: h·ªá s·ªë t·ª∑ l·ªá cho c√°c l·ªõp LoRA (cao = s·ª± th√≠ch nghi m·∫°nh m·∫Ω h∆°n)\n",
    "lora_alpha = 8\n",
    "# lora_dropout: t·ªâ l·ªá dropout cho c√°c l·ªõp LoRA (gi√∫p m√¥ h√¨nh tr√°nh t√¨nh tr·∫°ng overfitting)\n",
    "\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # h·∫°ng c·ªßa ma tr·∫≠n LoRA\n",
    "    lora_alpha=lora_alpha,  # h·ªá s·ªë t·ª∑ l·ªá cho c√°c l·ªõp LoRA\n",
    "    lora_dropout=lora_dropout,  # t·ªâ l·ªá dropout cho c√°c l·ªõp LoRA\n",
    "    bias=\"none\",  # Lo·∫°i bias cho LoRA, nh·ªØng bias s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t trong qu√° tr√¨nh hu·∫•n luy·ªán\n",
    "    target_modules=\"all-linear\",  # Nh·ªØng module m√† LoRA s·∫Ω ƒë∆∞·ª£c √°p d·ª•ng\n",
    "    task_type=\"CAUSAL_LM\",  # Lo·∫°i b√†i to√°n m√† m√¥ h√¨nh ƒëang gi·∫£i quy·∫øt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5NUDPcaouLl"
   },
   "source": [
    "Tr∆∞·ªõc khi ch√∫ng ta c√≥ th·ªÉ b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán, ch√∫ng ta c·∫ßn x√°c ƒë·ªãnh c√°c si√™u tham s·ªë (`TrainingArguments`) m√† ch√∫ng ta mu·ªën s·ª≠ d·ª•ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqT28VZlouLl"
   },
   "outputs": [],
   "source": [
    "# Nh·ªØng c√†i ƒë·∫∑t cho qu√° tr√¨nh tinh ch·ªânh\n",
    "# Nh·ªØng si√™u tham s·ªë d·ª±a v√†o g·ª£i √Ω t·ª´ b√†i b√°o QLoRA\n",
    "args = SFTConfig(\n",
    "    # C√†i ƒë·∫∑t ƒë·∫ßu ra\n",
    "    output_dir=finetune_name,  # ƒê∆∞·ªùng d·∫´n l∆∞u  m√¥ h√¨nh tinh ch·ªânh\n",
    "    # Th·ªùi gian hu·∫•n luy·ªán\n",
    "    num_train_epochs=1,  # S·ªë epoch\n",
    "    # C√†i ƒë·∫∑t batch_size\n",
    "    per_device_train_batch_size=2,  # Batch size tr√™n m·ªói GPU\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients cho batch_size l·ªõn h∆°n\n",
    "    # T·ªëi ∆∞u b·ªô nh·ªõ\n",
    "    gradient_checkpointing=True,  # S·ª≠ d·ª•ng gradient checkpointing ƒë·ªÉ gi·∫£m b·ªô nh·ªõ nh∆∞ng tƒÉng th·ªùi gian hu·∫•n luy·ªán\n",
    "    # Tinh ch·ªânh t·ªëi ∆∞u h√≥a\n",
    "    optim=\"adamw_torch_fused\",  # S·ª≠ d·ª•ng AdamW\n",
    "    learning_rate=2e-4,  # H·ªá s·ªë learning_rate (QLoRA paper)\n",
    "    max_grad_norm=0.3,  # Gi·ªõi h·∫°n gradient l·ªõn nh·∫•t\n",
    "    # C√†i ƒë·∫∑t learning rate\n",
    "    warmup_ratio=0.03,  # Ph·∫ßn trƒÉm s·ªë b∆∞·ªõc h·ªçc tƒÉng d·∫ßn\n",
    "    lr_scheduler_type=\"constant\",  # Gi·ªØ nguy√™n learning rate\n",
    "    # Logging v√† l∆∞u tr·ªØ\n",
    "    logging_steps=10,  # In ra th√¥ng tin sau m·ªói b∆∞·ªõc\n",
    "    save_strategy=\"epoch\",  # L∆∞u m√¥ h√¨nh sau m·ªói epoch\n",
    "    # Precision settings\n",
    "    bf16=True,  # S·ª≠ d·ª•ng bfloat16\n",
    "    # Nh·ªØng c√†i ƒë·∫∑t kh√°c\n",
    "    push_to_hub=False,  # Kh√¥ng t·∫£i l√™n HuggingFace Hub\n",
    "    report_to=\"none\",  # Kh√¥ng g·ª≠i k·∫øt qu·∫£ l√™n c√°c n·ªÅn t·∫£ng kh√°c\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGhR7uFBouLl"
   },
   "source": [
    "B√¢y gi·ªù ch√∫ng ta c√≥ m·ªçi kh·ªëi c·∫ßn thi·∫øt ƒë·ªÉ t·∫°o¬†`SFTTrainer`¬†b·∫Øt ƒë·∫ßu ƒë√†o t·∫°o m√¥ h√¨nh c·ªßa ch√∫ng ta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M00Har2douLl"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 1512  # ƒë·ªô d√†i chu·ªói t·ªëi ƒëa cho m√¥ h√¨nh v√† ƒë√≥ng g√≥i b·ªô d·ªØ li·ªáu\n",
    "\n",
    "# T·∫°o SFTTrainer v·ªõi c·∫•u h√¨nh LoRA\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,  # C·∫•u h√¨nh LoRA\n",
    "    max_seq_length=max_seq_length,  # ƒê·ªô d√†i chu·ªói t·ªëi ƒëa\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,  # B·∫≠t ƒë√≥ng g√≥i ƒë·∫ßu v√†o ƒë·ªÉ tƒÉng hi·ªáu qu·∫£\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # C√°c token ƒë·∫∑c bi·ªát ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi template\n",
    "        \"append_concat_token\": False,  # Kh√¥ng c·∫ßn th√™m k√Ω t·ª± ph√¢n t√°ch\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ_kRN24ouLl"
   },
   "source": [
    "B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh c·ªßa ch√∫ng ta b·∫±ng c√°ch g·ªçi ph∆∞∆°ng th·ª©c `train()` tr√™n ƒë·ªëi t∆∞·ª£ng `Trainer`. ƒêi·ªÅu n√†y s·∫Ω b·∫Øt ƒë·∫ßu v√≤ng l·∫∑p hu·∫•n luy·ªán v√† hu·∫•n luy·ªán m√¥ h√¨nh c·ªßa ch√∫ng ta trong 3 epoch. V√¨ ch√∫ng ta ƒëang s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p PEFT, ch√∫ng ta s·∫Ω ch·ªâ l∆∞u c√°c tr·ªçng s·ªë m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh v√† kh√¥ng l∆∞u to√†n b·ªô m√¥ h√¨nh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tq4nIYqKouLl"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300e5dfbb4b54750b77324345c7591f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=72, training_loss=1.6402628521124523, metrics={'train_runtime': 195.2398, 'train_samples_per_second': 1.485, 'train_steps_per_second': 0.369, 'total_flos': 282267289092096.0, 'train_loss': 1.6402628521124523, 'epoch': 0.993103448275862})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán\n",
    "trainer.train()\n",
    "\n",
    "# l∆∞u m√¥ h√¨nh\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4HHSYYzouLl"
   },
   "source": [
    "Vi·ªác hu·∫•n luy·ªán v·ªõi Flash Attention trong 3 epoch v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm 15k m·∫´u m·∫•t 4:14:36 tr√™n `g5.2xlarge`. Chi ph√≠ c·ªßa phi√™n b·∫£n n√†y l√† `1.21$/h`, t·ªïng chi ph√≠ ch·ªâ kho·∫£ng ~`5.3$`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C309KsXjouLl"
   },
   "source": [
    "### H·ª£p nh·∫•t adapter LoRA v√†o m√¥ h√¨nh g·ªëc\n",
    "\n",
    "Khi s·ª≠ d·ª•ng LoRA, ch√∫ng ta ch·ªâ hu·∫•n luy·ªán c√°c tr·ªçng s·ªë c·ªßa adapter trong khi gi·ªØ nguy√™n m√¥ h√¨nh g·ªëc. Trong qu√° tr√¨nh hu·∫•n luy·ªán, ch√∫ng ta ch·ªâ l∆∞u c√°c tr·ªçng s·ªë nh·∫π c·ªßa adapter (~2-10MB) thay v√¨ m·ªôt b·∫£n sao ƒë·∫ßy ƒë·ªß c·ªßa m√¥ h√¨nh. Tuy nhi√™n, ƒë·ªÉ tri·ªÉn khai, b·∫°n c√≥ th·ªÉ mu·ªën h·ª£p nh·∫•t c√°c adapter tr·ªü l·∫°i m√¥ h√¨nh g·ªëc v√¨:\n",
    "\n",
    "1. **Tri·ªÉn khai ƒë∆°n gi·∫£n**: M·ªôt t·ªáp m√¥ h√¨nh thay v√¨ m√¥ h√¨nh g·ªëc + b·ªô adapter\n",
    "2. **T·ªëc ƒë·ªô suy lu·∫≠n**: Kh√¥ng c·∫ßn t√≠nh to√°n adapter\n",
    "3. **T∆∞∆°ng th√≠ch v·ªõi framework**: T∆∞∆°ng th√≠ch t·ªët h∆°n v·ªõi c√°c framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "\n",
    "# T·∫£i PEFT v√†o CPU\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# K·∫øt h·ª£p c√°c tr·ªçng s·ªë trong Adapter v√† LoRA sau ƒë√≥ l∆∞u m√¥ h√¨nh\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\n",
    "    args.output_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yO6E9quouLl"
   },
   "source": [
    "## 3. Ki·ªÉm th·ª≠ m√¥ h√¨nh v√† ch·∫°y th·ª≠\n",
    "\n",
    "Sau khi hu·∫•n luy·ªán xong, ch√∫ng ta mu·ªën ki·ªÉm th·ª≠ m√¥ h√¨nh c·ªßa m√¨nh. Ch√∫ng ta s·∫Ω t·∫£i c√°c m·∫´u kh√°c nhau t·ª´ b·ªô d·ªØ li·ªáu g·ªëc v√† ƒë√°nh gi√° m√¥ h√¨nh tr√™n c√°c m·∫´u ƒë√≥, s·ª≠ d·ª•ng m·ªôt v√≤ng l·∫∑p ƒë∆°n gi·∫£n v√† ƒë·ªô ch√≠nh x√°c l√†m th∆∞·ªõc ƒëo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>B√†i t·∫≠p b·ªï sung: t·∫£i LoRA Adapter</h2>\n",
    "    <p>S·ª≠ d·ª•ng nh·ªØng g√¨ b·∫°n h·ªçc ƒë∆∞·ª£c t·ª´ ghi ch√∫ ƒë·ªÉ t·∫£i LoRA Adapter ƒë√£ ƒë∆∞·ª£c ƒë√†o t·∫°o c·ªßa b·∫°n ƒë·ªÉ ch·∫°y</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "I5B494OdouLl"
   },
   "outputs": [],
   "source": [
    "# d·ªçn d·∫πp b·ªô nh·ªõ\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1UhohVdouLl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetune_name)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    finetune_name, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=merged_model, tokenizer=tokenizer, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99uFDAuuouLl"
   },
   "source": [
    "H√£y th·ª≠ m·ªôt s·ªë v√≠ d·ª• ƒë·ªÉ xem m√¥ h√¨nh ho·∫°t ƒë·ªông"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-shSmUbvouLl",
    "outputId": "16d97c61-3b31-4040-c780-3c4de75c3824"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is the capital of Germany? Explain why thats the case and if it was different in the past?\",\n",
    "    \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\",\n",
    "    \"What is the difference between a fruit and a vegetable? Give examples of each.\",\n",
    "]\n",
    "\n",
    "\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][len(prompt) :].strip()\n",
    "\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
