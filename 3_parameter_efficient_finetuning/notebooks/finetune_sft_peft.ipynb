{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-6LLOPZouLg"
   },
   "source": [
    "# åœ¨ TRL æ¡†æ¶ä¸‹ç”¨ LoRA å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹\n",
    "\n",
    "è¿™ä¸ª notebook å±•ç¤ºå¦‚ä½•ç”¨ LoRA é«˜æ•ˆå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ã€‚LoRA æ˜¯ä¸€ç§é«˜æ•ˆçš„å‚æ•°å¾®è°ƒæ–¹æ³•ï¼Œæœ‰å¦‚ä¸‹ä¼˜ç‚¹ï¼š\n",
    "- ä¸æ›´æ–°é¢„è®­ç»ƒæ¨¡å‹æƒé‡\n",
    "- ä»…åœ¨æ³¨æ„åŠ›å±‚æ·»åŠ å°‘é‡ä½ç§©åˆ†è§£çŸ©é˜µä½œä¸ºè®­ç»ƒå‚æ•°\n",
    "- åŸºæœ¬èƒ½å‡å°‘ 90% è®­ç»ƒå‚æ•°\n",
    "- èƒ½ä¿ç•™æ¨¡å‹åŸæœ‰çš„èƒ½åŠ›\n",
    "\n",
    "æœ¬æ–‡æ¶µç›–è¿™äº›æ­¥éª¤ï¼š\n",
    "1. é…ç½®å¼€å‘ç¯å¢ƒã€è®¾å®š LoRA ç›¸å…³é…ç½®\n",
    "2. å‡†å¤‡æ•°æ®é›†\n",
    "3. ä½¿ç”¨ `trl` æ¡†æ¶ä¸‹çš„ `SFTTrainer` è¿›è¡Œ LoRA å¾®è°ƒ\n",
    "4. æµ‹è¯•æ¨¡å‹æ€§èƒ½ã€å­¦ä¹ åŠ è½½ adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXqd9BXgouLi"
   },
   "source": [
    "## 1. é…ç½®å¼€å‘ç¯å¢ƒ\n",
    "\n",
    "æˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£… PyTorch å’Œ Hugging Face ç›¸å…³çš„åº“ï¼Œè¿™åŒ…æ‹¬ `trl`ã€`transformers`ã€`datasets`ã€‚å…¶ä¸­ `trl` åŸºäº `transformers` å’Œ `datasets`ï¼Œç”¨ä»¥å¾®è°ƒæ¨¡å‹ã€è¿›è¡Œ RLHFã€å¯¹é½ LLM ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKvGVxImouLi"
   },
   "outputs": [],
   "source": [
    "# Install the requirements in Google Colab\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# Authenticate to Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()\n",
    "\n",
    "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHUzfwpKouLk"
   },
   "source": [
    "## 2. è½½å…¥æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "z4p6Bvo7ouLk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 2260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: ä½ ä¹Ÿå¯ä»¥ç”¨è‡ªå·±çš„æ•°æ®é›†\n",
    "dataset = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TOhJdtsouLk"
   },
   "source": [
    "## 3. åœ¨ `trl` æ¡†æ¶ä¸‹ç”¨ `SFTTrainer` å®ç°å¤§è¯­è¨€æ¨¡å‹çš„ LoRA å¾®è°ƒ\n",
    "\n",
    "åœ¨ `trl` ä¸­ï¼Œ[SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) é€šè¿‡ [PEFT](https://huggingface.co/docs/peft/en/index) æä¾›äº† LoRA Adapter çš„é›†æˆã€‚è¿™æ ·çš„è®¾å®šæœ‰ä»¥ä¸‹å‡ ä¸ªå¥½å¤„ï¼š\n",
    "\n",
    "1. **é«˜æ•ˆåˆ©ç”¨å†…å­˜**ï¼š\n",
    "   - ä»… Adapter çš„å‚æ•°ä¼šä¿å­˜åœ¨ GPU æ˜¾å­˜ä¸­\n",
    "   - åŸæ¨¡å‹å‚æ•°è¢«å†»ç»“ï¼Œæ‰€ä»¥å¯ä»¥ç”¨ä½ç²¾åº¦è½½å…¥\n",
    "   - è¿™ä½¿å¾—åœ¨æ¶ˆè´¹çº§æ˜¾å¡ä¸Šä¹Ÿå¯ä»¥å¾®è°ƒå¤§æ¨¡å‹\n",
    "2. **è®­ç»ƒå±‚é¢**ï¼š\n",
    "   - åŸç”Ÿ PEFT/LoRA é›†æˆï¼Œç”¨æˆ·å¼€å‘æ‰€éœ€ä»£ç é‡å°‘\n",
    "   - æ”¯æŒ QLoRAï¼ˆé‡åŒ–ç‰ˆ LoRAï¼‰ï¼Œå¯ä»¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨\n",
    "\n",
    "3. **Adapter ç®¡ç†**ï¼š\n",
    "   - å¯ä»¥æ–¹ä¾¿åœ°è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜ Adapter\n",
    "   - å¯ä»¥æ–¹ä¾¿åœ°æŠŠ Adapter èåˆè¿›åŸæ¨¡å‹\n",
    "\n",
    "æœ¬æ–‡å°†ä¼šè¿›è¡Œ LoRA å¾®è°ƒï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥å°è¯• 4-bit é‡åŒ–æ¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚é…ç½®æ­¥éª¤åŒ…å«ä»¥ä¸‹å‡ æ­¥ï¼š\n",
    "1. å®šä¹‰å¥½ LoRA çš„ç›¸å…³å‚æ•°ï¼ˆä¸»è¦æ˜¯ rankã€alphaã€dropoutï¼‰\n",
    "2. åˆ›å»ºä¸€ä¸ª SFTTrainer çš„å®ä¾‹\n",
    "3. è®­ç»ƒæ¨¡å‹ã€ä¿å­˜ adapter çš„å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-MyDataset\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbuVArTHouLk"
   },
   "source": [
    "ç”±äº `SFTTrainer`Â  åŸç”Ÿæ”¯æŒÂ `peft`ï¼Œä½¿ç”¨ LoRA è®­ç»ƒ LLM å°±å˜å¾—éå¸¸ç®€å•ã€‚æˆ‘ä»¬éœ€è¦é…ç½®çš„åªæœ‰ `LoraConfig`ã€‚\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>ç»ƒä¹ ï¼šä¸ºå¾®è°ƒå®šä¹‰å¥½ LoRA ç›¸å…³å‚æ•°</h2>\n",
    "    <p>ä» Hugging Face hub æ‰¾ä¸€ä¸ªåˆé€‚çš„æ•°æ®ç„¶åå¾®è°ƒæ¨¡å‹ </p> \n",
    "    <p><b>éš¾åº¦ç­‰çº§</b></p>\n",
    "    <p>ğŸ¢ ä½¿ç”¨é»˜è®¤çš„ LoRA è¶…å‚æ•°ç›´æ¥å¾®è°ƒ</p>\n",
    "    <p>ğŸ• æ”¹å˜ä¸€äº›è¶…å‚æ•°ï¼Œå­¦ä¹ é€šè¿‡ weights & biases å¹³å°æŸ¥çœ‹</p>\n",
    "    <p>ğŸ¦ æ”¹å˜ä¸€äº›è¶…å‚æ•°ï¼Œè®­ç»ƒåæŸ¥çœ‹è¿™äº›æ”¹å˜æ˜¯å¦å½±å“äº†æ¨ç†æ€§èƒ½</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blDSs9swouLk"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# TODO: é…ç½® LoRA å‚æ•°\n",
    "# r: ä½ç§©åˆ†è§£çš„ç§©ï¼Œè¶Šå°åˆ™éœ€è¦è®­ç»ƒçš„å‚æ•°é‡è¶Šå°‘\n",
    "rank_dimension = 6\n",
    "# lora_alpha: å°†è®­ç»ƒå¥½çš„å‚æ•°åŠ åˆ°åŸæ¨¡å‹ä¸Šæ—¶çš„ç¼©æ”¾å€æ•°ï¼Œè¶Šå¤§åˆ™å¾®è°ƒå‚æ•°ä½œç”¨è¶Šæ˜æ˜¾\n",
    "lora_alpha = 8\n",
    "# lora_dropout: LoRA ç›¸å…³å±‚çš„ dropoutï¼Œå¯ä»¥ç”¨æ¥åº”å¯¹è¿‡æ‹Ÿåˆ\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # ä¸€èˆ¬é€‰æ‹© 4 åˆ° 32\n",
    "    lora_alpha=lora_alpha,  # ä¸€èˆ¬æ˜¯ rank çš„ 2 å€\n",
    "    lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n",
    "    target_modules=\"all-linear\",  # æ¨¡å‹å“ªäº›å±‚ä¼šæ·»åŠ  LoRA Adapter\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for model architecture\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5NUDPcaouLl"
   },
   "source": [
    "æ­¤å¤–æˆ‘ä»¬è¿˜éœ€å®šä¹‰è®­ç»ƒçš„è¶…å‚æ•°ï¼ˆ`TrainingArguments`ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqT28VZlouLl"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "# Hyperparameters based on QLoRA paper recommendations\n",
    "args = SFTConfig(\n",
    "    # Output settings\n",
    "    output_dir=finetune_name,  # Directory to save model checkpoints\n",
    "    # Training duration\n",
    "    num_train_epochs=1,  # Number of training epochs\n",
    "    # Batch size settings\n",
    "    per_device_train_batch_size=2,  # Batch size per GPU\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients for larger effective batch\n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,  # Trade compute for memory savings\n",
    "    # Optimizer settings\n",
    "    optim=\"adamw_torch_fused\",  # Use fused AdamW for efficiency\n",
    "    learning_rate=2e-4,  # Learning rate (QLoRA paper)\n",
    "    max_grad_norm=0.3,  # Gradient clipping threshold\n",
    "    # Learning rate schedule\n",
    "    warmup_ratio=0.03,  # Portion of steps for warmup\n",
    "    lr_scheduler_type=\"constant\",  # Keep learning rate constant after warmup\n",
    "    # Logging and saving\n",
    "    logging_steps=10,  # Log metrics every N steps\n",
    "    save_strategy=\"epoch\",  # Save checkpoint every epoch\n",
    "    # Precision settings\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    # Integration settings\n",
    "    push_to_hub=False,  # Don't push to HuggingFace Hub\n",
    "    report_to=None,  # Disable external logging\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGhR7uFBouLl"
   },
   "source": [
    "é…ç½®å®Œæ¯•ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºÂ `SFTTrainer`Â æ¥è®­ç»ƒæ¨¡å‹äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M00Har2douLl"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 1512  # max sequence length for model and packing of the dataset\n",
    "\n",
    "# Create SFTTrainer with LoRA configuration\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,  # LoRA configuration\n",
    "    max_seq_length=max_seq_length,  # Maximum sequence length\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,  # Enable input packing for efficiency\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # Special tokens handled by template\n",
    "        \"append_concat_token\": False,  # No additional separator needed\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ_kRN24ouLl"
   },
   "source": [
    "é€šè¿‡å¯åŠ¨ `train()` å‡½æ•°ï¼Œæˆ‘ä»¬å¼€å§‹è®­ç»ƒã€‚æœ¬æ¬¡è®­ç»ƒåŒ…å« 3 ä¸ª epochã€‚ç”±äºæˆ‘ä»¬ç”¨äº† PEFTï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆ–ç»“æŸåï¼Œåªä¿å­˜ adapter çš„å‚æ•°ï¼Œæ— éœ€ä¿å­˜åŸæ¨¡å‹å‚æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tq4nIYqKouLl"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300e5dfbb4b54750b77324345c7591f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=72, training_loss=1.6402628521124523, metrics={'train_runtime': 195.2398, 'train_samples_per_second': 1.485, 'train_steps_per_second': 0.369, 'total_flos': 282267289092096.0, 'train_loss': 1.6402628521124523, 'epoch': 0.993103448275862})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4HHSYYzouLl"
   },
   "source": [
    "æˆ‘ä»¬æ¨¡å‹ä½¿ç”¨äº† Flash Attention åŠ é€Ÿè®­ç»ƒã€‚åœ¨å½“å‰æ•°æ®é›†ï¼ˆ15k çš„æ ·æœ¬é‡ï¼‰è®­ç»ƒäº† 3 è½®ï¼Œåœ¨ä¸€ä¸ª `g5.2xlarge` æœºå™¨ä¸Šç”¨äº† 4 å°æ—¶ 14 åˆ†é’Ÿ 36 ç§’ã€‚è¯¥æœºå™¨æŠ¥ä»· `1.21$/h`ï¼Œæ‰€ä»¥æˆ‘ä»¬æ€»èŠ±è´¹ä»… `5.3$`ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C309KsXjouLl"
   },
   "source": [
    "### å°† LoRA Adapter èå…¥åŸæ¨¡å‹\n",
    "\n",
    "è®­ç»ƒ LoRA æ—¶ï¼Œæˆ‘ä»¬åªè®­ç»ƒ adapter é‡Œçš„å‚æ•°ï¼Œè€Œä¸è®­ç»ƒåŸæ¨¡å‹ã€‚æ‰€ä»¥ä¿å­˜çš„å‚æ•°ä¹Ÿåªæœ‰ adapter é‡Œçš„å‚æ•°ï¼ˆå¯èƒ½ä¹Ÿå°± 2MB åˆ° 10MBï¼‰ã€‚ç„¶è€Œåœ¨éƒ¨ç½²é˜¶æ®µï¼Œä½ å¯èƒ½éœ€è¦æŠŠ Adapter èåˆè¿›åŸæ¨¡å‹ï¼š\n",
    "1. **ç®€åŒ–çš„éƒ¨ç½²æµç¨‹**ï¼šä»…è½½å…¥ä¸€ä¸ªæ¨¡å‹å‚æ•°æ–‡ä»¶å³å¯ï¼Œæ— éœ€é¢å¤–è½½å…¥ adapter å‚æ•°æ–‡ä»¶\n",
    "2. **æ¨ç†é€Ÿåº¦æå‡**ï¼šadapter å¼•å…¥çš„è®¡ç®—å·²ç»èåˆè¿›äº†æ¨¡å‹ä¸­\n",
    "3. **æ¡†æ¶çš„å…¼å®¹æ€§**ï¼šæ›´èƒ½å’ŒæœåŠ¡æ¡†æ¶é€‚é…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\n",
    "    args.output_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yO6E9quouLl"
   },
   "source": [
    "## 3. æµ‹è¯•æ¨¡å‹ã€è¿›è¡Œæ¨ç†\n",
    "\n",
    "è®­ç»ƒç»“æŸåï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦æµ‹è¯•æ¨¡å‹ã€‚å¯ä»¥ä»æ•°æ®é›†æ‰¾ä¸€äº›æ ·æœ¬ï¼Œç„¶åçœ‹çœ‹æ¨¡å‹åœ¨è¿™äº›æ ·æœ¬ä¸Šçš„æ€§èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "I5B494OdouLl"
   },
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1UhohVdouLl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetune_name)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    finetune_name, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=merged_model, tokenizer=tokenizer, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99uFDAuuouLl"
   },
   "source": [
    "ç°åœ¨æˆ‘ä»¬æ‰¾äº›æ ·æœ¬æ¥æµ‹è¯•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-shSmUbvouLl",
    "outputId": "16d97c61-3b31-4040-c780-3c4de75c3824"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is the capital of Germany? Explain why thats the case and if it was different in the past?\",\n",
    "    \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\",\n",
    "    \"What is the difference between a fruit and a vegetable? Give examples of each.\",\n",
    "]\n",
    "\n",
    "\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][len(prompt) :].strip()\n",
    "\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>é¢å¤–ç»ƒä¹ ï¼šè½½å…¥è‡ªå·±è®­ç»ƒçš„ LoRA Adapter</h2>\n",
    "    <p>ç»“æŸæœ¬æ•™ç¨‹åï¼Œä½ å¯ä»¥ä½¿ç”¨å­¦åˆ°çš„æŠ€æœ¯ï¼Œè‡ªå·±è®­ç»ƒä¸€ä¸ª LoRAï¼Œç„¶åè½½å…¥ LoRA Adapter</p> \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
