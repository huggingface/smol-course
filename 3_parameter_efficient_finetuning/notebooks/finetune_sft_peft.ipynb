{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-6LLOPZouLg"
   },
   "source": [
    "# 在 TRL 框架下用 LoRA 微调大语言模型\n",
    "\n",
    "这个 notebook 展示如何用 LoRA 高效微调大语言模型。LoRA 是一种高效的参数微调方法，有如下优点：\n",
    "- 不更新预训练模型权重\n",
    "- 仅在注意力层添加少量低秩分解矩阵作为训练参数\n",
    "- 基本能减少 90% 训练参数\n",
    "- 能保留模型原有的能力\n",
    "\n",
    "本文涵盖这些步骤：\n",
    "1. 配置开发环境、设定 LoRA 相关配置\n",
    "2. 准备数据集\n",
    "3. 使用 `trl` 框架下的 `SFTTrainer` 进行 LoRA 微调\n",
    "4. 测试模型性能、学习加载 adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXqd9BXgouLi"
   },
   "source": [
    "## 1. 配置开发环境\n",
    "\n",
    "我们首先需要安装 PyTorch 和 Hugging Face 相关的库，这包括 `trl`、`transformers`、`datasets`。其中 `trl` 基于 `transformers` 和 `datasets`，用以微调模型、进行 RLHF、对齐 LLM 等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKvGVxImouLi"
   },
   "outputs": [],
   "source": [
    "# Install the requirements in Google Colab\n",
    "# !pip install transformers datasets trl huggingface_hub\n",
    "\n",
    "# Authenticate to Hugging Face\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()\n",
    "\n",
    "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHUzfwpKouLk"
   },
   "source": [
    "## 2. 载入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "z4p6Bvo7ouLk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 2260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: 你也可以用自己的数据集\n",
    "dataset = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TOhJdtsouLk"
   },
   "source": [
    "## 3. 在 `trl` 框架下用 `SFTTrainer` 实现大语言模型的 LoRA 微调\n",
    "\n",
    "在 `trl` 中，[SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) 通过 [PEFT](https://huggingface.co/docs/peft/en/index) 提供了 LoRA Adapter 的集成。这样的设定有以下几个好处：\n",
    "\n",
    "1. **高效利用内存**：\n",
    "   - 仅 Adapter 的参数会保存在 GPU 显存中\n",
    "   - 原模型参数被冻结，所以可以用低精度载入\n",
    "   - 这使得在消费级显卡上也可以微调大模型\n",
    "2. **训练层面**：\n",
    "   - 原生 PEFT/LoRA 集成，用户开发所需代码量少\n",
    "   - 支持 QLoRA（量化版 LoRA），可以进一步减少内存使用\n",
    "\n",
    "3. **Adapter 管理**：\n",
    "   - 可以方便地训练过程中保存 Adapter\n",
    "   - 可以方便地把 Adapter 融合进原模型\n",
    "\n",
    "本文将会进行 LoRA 微调，当然你也可以尝试 4-bit 量化来进一步减少内存使用。配置步骤包含以下几步：\n",
    "1. 定义好 LoRA 的相关参数（主要是 rank、alpha、dropout）\n",
    "2. 创建一个 SFTTrainer 的实例\n",
    "3. 训练模型、保存 adapter 的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-MyDataset\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbuVArTHouLk"
   },
   "source": [
    "由于 `SFTTrainer`  原生支持 `peft`，使用 LoRA 训练 LLM 就变得非常简单。我们需要配置的只有 `LoraConfig`。\n",
    "\n",
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>练习：为微调定义好 LoRA 相关参数</h2>\n",
    "    <p>从 Hugging Face hub 找一个合适的数据然后微调模型 </p> \n",
    "    <p><b>难度等级</b></p>\n",
    "    <p>🐢 使用默认的 LoRA 超参数直接微调</p>\n",
    "    <p>🐕 改变一些超参数，学习通过 weights & biases 平台查看</p>\n",
    "    <p>🦁 改变一些超参数，训练后查看这些改变是否影响了推理性能</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blDSs9swouLk"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# TODO: 配置 LoRA 参数\n",
    "# r: 低秩分解的秩，越小则需要训练的参数量越少\n",
    "rank_dimension = 6\n",
    "# lora_alpha: 将训练好的参数加到原模型上时的缩放倍数，越大则微调参数作用越明显\n",
    "lora_alpha = 8\n",
    "# lora_dropout: LoRA 相关层的 dropout，可以用来应对过拟合\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # 一般选择 4 到 32\n",
    "    lora_alpha=lora_alpha,  # 一般是 rank 的 2 倍\n",
    "    lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n",
    "    target_modules=\"all-linear\",  # 模型哪些层会添加 LoRA Adapter\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for model architecture\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5NUDPcaouLl"
   },
   "source": [
    "此外我们还需定义训练的超参数（`TrainingArguments`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqT28VZlouLl"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "# Hyperparameters based on QLoRA paper recommendations\n",
    "args = SFTConfig(\n",
    "    # Output settings\n",
    "    output_dir=finetune_name,  # Directory to save model checkpoints\n",
    "    # Training duration\n",
    "    num_train_epochs=1,  # Number of training epochs\n",
    "    # Batch size settings\n",
    "    per_device_train_batch_size=2,  # Batch size per GPU\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients for larger effective batch\n",
    "    # Memory optimization\n",
    "    gradient_checkpointing=True,  # Trade compute for memory savings\n",
    "    # Optimizer settings\n",
    "    optim=\"adamw_torch_fused\",  # Use fused AdamW for efficiency\n",
    "    learning_rate=2e-4,  # Learning rate (QLoRA paper)\n",
    "    max_grad_norm=0.3,  # Gradient clipping threshold\n",
    "    # Learning rate schedule\n",
    "    warmup_ratio=0.03,  # Portion of steps for warmup\n",
    "    lr_scheduler_type=\"constant\",  # Keep learning rate constant after warmup\n",
    "    # Logging and saving\n",
    "    logging_steps=10,  # Log metrics every N steps\n",
    "    save_strategy=\"epoch\",  # Save checkpoint every epoch\n",
    "    # Precision settings\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    # Integration settings\n",
    "    push_to_hub=False,  # Don't push to HuggingFace Hub\n",
    "    report_to=None,  # Disable external logging\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGhR7uFBouLl"
   },
   "source": [
    "配置完毕，我们可以创建 `SFTTrainer` 来训练模型了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M00Har2douLl"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 1512  # max sequence length for model and packing of the dataset\n",
    "\n",
    "# Create SFTTrainer with LoRA configuration\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,  # LoRA configuration\n",
    "    max_seq_length=max_seq_length,  # Maximum sequence length\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,  # Enable input packing for efficiency\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # Special tokens handled by template\n",
    "        \"append_concat_token\": False,  # No additional separator needed\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ_kRN24ouLl"
   },
   "source": [
    "通过启动 `train()` 函数，我们开始训练。本次训练包含 3 个 epoch。由于我们用了 PEFT，我们可以在训练过程中或结束后，只保存 adapter 的参数，无需保存原模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tq4nIYqKouLl"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300e5dfbb4b54750b77324345c7591f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=72, training_loss=1.6402628521124523, metrics={'train_runtime': 195.2398, 'train_samples_per_second': 1.485, 'train_steps_per_second': 0.369, 'total_flos': 282267289092096.0, 'train_loss': 1.6402628521124523, 'epoch': 0.993103448275862})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4HHSYYzouLl"
   },
   "source": [
    "我们模型使用了 Flash Attention 加速训练。在当前数据集（15k 的样本量）训练了 3 轮，在一个 `g5.2xlarge` 机器上用了 4 小时 14 分钟 36 秒。该机器报价 `1.21$/h`，所以我们总花费仅 `5.3$`。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C309KsXjouLl"
   },
   "source": [
    "### 将 LoRA Adapter 融入原模型\n",
    "\n",
    "训练 LoRA 时，我们只训练 adapter 里的参数，而不训练原模型。所以保存的参数也只有 adapter 里的参数（可能也就 2MB 到 10MB）。然而在部署阶段，你可能需要把 Adapter 融合进原模型：\n",
    "1. **简化的部署流程**：仅载入一个模型参数文件即可，无需额外载入 adapter 参数文件\n",
    "2. **推理速度提升**：adapter 引入的计算已经融合进了模型中\n",
    "3. **框架的兼容性**：更能和服务框架适配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=args.output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\n",
    "    args.output_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yO6E9quouLl"
   },
   "source": [
    "## 3. 测试模型、进行推理\n",
    "\n",
    "训练结束后，我们可能需要测试模型。可以从数据集找一些样本，然后看看模型在这些样本上的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "I5B494OdouLl"
   },
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1UhohVdouLl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetune_name)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    finetune_name, device_map=\"auto\", torch_dtype=torch.float16\n",
    ")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=merged_model, tokenizer=tokenizer, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99uFDAuuouLl"
   },
   "source": [
    "现在我们找些样本来测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-shSmUbvouLl",
    "outputId": "16d97c61-3b31-4040-c780-3c4de75c3824"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is the capital of Germany? Explain why thats the case and if it was different in the past?\",\n",
    "    \"Write a Python function to calculate the factorial of a number.\",\n",
    "    \"A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\",\n",
    "    \"What is the difference between a fruit and a vegetable? Give examples of each.\",\n",
    "]\n",
    "\n",
    "\n",
    "def test_inference(prompt):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][len(prompt) :].strip()\n",
    "\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(prompt)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
    "    <h2 style='margin: 0;color:blue'>额外练习：载入自己训练的 LoRA Adapter</h2>\n",
    "    <p>结束本教程后，你可以使用学到的技术，自己训练一个 LoRA，然后载入 LoRA Adapter</p> \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
