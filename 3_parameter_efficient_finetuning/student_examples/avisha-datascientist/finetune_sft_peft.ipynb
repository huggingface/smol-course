{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How to Fine-Tune LLMs with LoRA Adapters using Hugging Face TRL\n\nThis notebook demonstrates how to efficiently fine-tune large language models using LoRA (Low-Rank Adaptation) adapters. LoRA is a parameter-efficient fine-tuning technique that:\n- Freezes the pre-trained model weights\n- Adds small trainable rank decomposition matrices to attention layers\n- Typically reduces trainable parameters by ~90%\n- Maintains model performance while being memory efficient\n\nWe'll cover:\n1. Setup development environment and LoRA configuration\n2. Create and prepare the dataset for adapter training\n3. Fine-tune using `trl` and `SFTTrainer` with LoRA adapters\n4. Test the model and merge adapters (optional)\n","metadata":{"id":"z-6LLOPZouLg"}},{"cell_type":"markdown","source":"## 1. Setup development environment\n\nOur first step is to install Hugging Face Libraries and Pyroch, including trl, transformers and datasets. If you haven't heard of trl yet, don't worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs.\n","metadata":{"id":"fXqd9BXgouLi"}},{"cell_type":"code","source":"!pip install -U datasets huggingface_hub peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:30:23.109012Z","iopub.execute_input":"2024-12-25T15:30:23.109292Z","iopub.status.idle":"2024-12-25T15:30:29.234934Z","shell.execute_reply.started":"2024-12-25T15:30:23.109270Z","shell.execute_reply":"2024-12-25T15:30:29.234075Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\nCollecting huggingface_hub\n  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\nCollecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface_hub, peft\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.24.7\n    Uninstalling huggingface-hub-0.24.7:\n      Successfully uninstalled huggingface-hub-0.24.7\nSuccessfully installed huggingface_hub-0.27.0 peft-0.14.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install trl==0.12.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:30:40.361334Z","iopub.execute_input":"2024-12-25T15:30:40.361939Z","iopub.status.idle":"2024-12-25T15:30:54.491439Z","shell.execute_reply.started":"2024-12-25T15:30:40.361907Z","shell.execute_reply":"2024-12-25T15:30:54.490601Z"}},"outputs":[{"name":"stdout","text":"Collecting trl==0.12.0\n  Downloading trl-0.12.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.12.0) (0.34.2)\nRequirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from trl==0.12.0) (3.2.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl==0.12.0) (13.8.1)\nCollecting transformers>=4.46.0 (from trl==0.12.0)\n  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl==0.12.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl==0.12.0) (24.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl==0.12.0) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl==0.12.0) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl==0.12.0) (2.4.1+cu121)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl==0.12.0) (0.27.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl==0.12.0) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl==0.12.0) (3.16.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl==0.12.0) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl==0.12.0) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl==0.12.0) (2.1.4)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl==0.12.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl==0.12.0) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl==0.12.0) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl==0.12.0) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl==0.12.0) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl==0.12.0) (3.10.5)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl==0.12.0) (2024.9.11)\nCollecting tokenizers<0.22,>=0.21 (from transformers>=4.46.0->trl==0.12.0)\n  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl==0.12.0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl==0.12.0) (2.18.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl==0.12.0) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl==0.12.0) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl==0.12.0) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl==0.12.0) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl==0.12.0) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl==0.12.0) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl==0.12.0) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl==0.12.0) (4.12.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.12.0) (0.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl==0.12.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl==0.12.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl==0.12.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl==0.12.0) (2024.8.30)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl==0.12.0) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl==0.12.0) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl==0.12.0) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl==0.12.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl==0.12.0) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl==0.12.0) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl==0.12.0) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl==0.12.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate>=0.34.0->trl==0.12.0) (1.3.0)\nDownloading trl-0.12.0-py3-none-any.whl (310 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers, trl\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.2\n    Uninstalling transformers-4.44.2:\n      Successfully uninstalled transformers-4.44.2\nSuccessfully installed tokenizers-0.21.0 transformers-4.47.1 trl-0.12.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install transformers==4.46","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:30:57.866036Z","iopub.execute_input":"2024-12-25T15:30:57.866330Z","iopub.status.idle":"2024-12-25T15:31:08.663738Z","shell.execute_reply.started":"2024-12-25T15:30:57.866309Z","shell.execute_reply":"2024-12-25T15:31:08.662685Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.46\n  Downloading transformers-4.46.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.46) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.46) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46) (0.4.5)\nCollecting tokenizers<0.21,>=0.20 (from transformers==4.46)\n  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46) (4.66.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46) (2024.8.30)\n\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'transformers' candidate (version 4.46.0 at https://files.pythonhosted.org/packages/db/88/1ef8a624a33d7fe460a686b9e0194a7916320fc0d67d4e38e570beeac039/transformers-4.46.0-py3-none-any.whl (from https://pypi.org/simple/transformers/) (requires-python:>=3.8.0))\nReason for being yanked: This version unfortunately does not work with 3.8 but we did not drop the support yet\u001b[0m\u001b[33m\n\u001b[0mDownloading transformers-4.46.0-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.1\n    Uninstalling transformers-4.47.1:\n      Successfully uninstalled transformers-4.47.1\nSuccessfully installed tokenizers-0.20.3 transformers-4.46.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_API_KEY\"] = \"5be1f6132482bda0f2b4b5d7cc99eb376a809336\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:31:12.920991Z","iopub.execute_input":"2024-12-25T15:31:12.921305Z","iopub.status.idle":"2024-12-25T15:31:12.925309Z","shell.execute_reply.started":"2024-12-25T15:31:12.921278Z","shell.execute_reply":"2024-12-25T15:31:12.924327Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Install the requirements in Google Colab\n# !pip install transformers datasets trl huggingface_hub\n\n# Authenticate to Hugging Face\n\nfrom huggingface_hub import login\n\nlogin()\n\n# for convenience you can create an environment variable containing your hub token as HF_TOKEN","metadata":{"id":"tKvGVxImouLi","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:31:14.553866Z","iopub.execute_input":"2024-12-25T15:31:14.554176Z","iopub.status.idle":"2024-12-25T15:31:14.885311Z","shell.execute_reply.started":"2024-12-25T15:31:14.554149Z","shell.execute_reply":"2024-12-25T15:31:14.884487Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c713905634b4862a6d7bc58df2652c1"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## 2. Load the dataset","metadata":{"id":"XHUzfwpKouLk"}},{"cell_type":"code","source":"# Load a sample dataset\nfrom datasets import load_dataset\n\n# TODO: define your dataset and config using the path and name parameters\ndataset = load_dataset(path=\"cfilt/iitb-english-hindi\")\ndataset","metadata":{"id":"z4p6Bvo7ouLk","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:31:24.914716Z","iopub.execute_input":"2024-12-25T15:31:24.915009Z","iopub.status.idle":"2024-12-25T15:31:31.334861Z","shell.execute_reply.started":"2024-12-25T15:31:24.914989Z","shell.execute_reply":"2024-12-25T15:31:31.334027Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ead7047468c4469ab97b0cd4e868862"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11e24736102349cf933cdbfaba149fe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/190M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"383b31973cc14dd4abe25908f488b9ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/85.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b28d0085ef64e99830e753e4e385054"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77833cb532cf43e2b1ee336daa2c4d90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1659083 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fa5331418c34ff1beba6a38c20ce812"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/520 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e8c53702ddd4351a0dc2fe22824d819"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2507 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"176708400ad44235b9f08215de4689c5"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['translation'],\n        num_rows: 1659083\n    })\n    validation: Dataset({\n        features: ['translation'],\n        num_rows: 520\n    })\n    test: Dataset({\n        features: ['translation'],\n        num_rows: 2507\n    })\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def format_dataset(sample):\n    messages_formatted = [\n        {\"content\":\"Translate given sentence to Hindi: \"+sample[\"translation\"][\"en\"], \"role\":\"user\"},\n        {\"content\":sample[\"translation\"][\"hi\"], \"role\":\"assistant\"}\n    ]\n    sample[\"messages\"] = messages_formatted\n\n    return sample\n\ndataset = dataset.map(format_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:31:38.377361Z","iopub.execute_input":"2024-12-25T15:31:38.377892Z","iopub.status.idle":"2024-12-25T15:32:57.815642Z","shell.execute_reply.started":"2024-12-25T15:31:38.377845Z","shell.execute_reply":"2024-12-25T15:32:57.814701Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1659083 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14fbf9044ce24fbcbbf1dc43fd14a339"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/520 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd959fdc60654e68b66671e00bc5cf53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2507 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1238d5a7eefe4cbabf232535a1920d25"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"dataset['train'] = dataset['train'].shard(num_shards=30, index=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:33:05.990733Z","iopub.execute_input":"2024-12-25T15:33:05.991057Z","iopub.status.idle":"2024-12-25T15:33:06.001231Z","shell.execute_reply.started":"2024-12-25T15:33:05.991030Z","shell.execute_reply":"2024-12-25T15:33:06.000313Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T13:19:05.511860Z","iopub.execute_input":"2024-12-25T13:19:05.512279Z","iopub.status.idle":"2024-12-25T13:19:05.520056Z","shell.execute_reply.started":"2024-12-25T13:19:05.512244Z","shell.execute_reply":"2024-12-25T13:19:05.518794Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['translation', 'messages'],\n        num_rows: 16591\n    })\n    validation: Dataset({\n        features: ['translation', 'messages'],\n        num_rows: 520\n    })\n    test: Dataset({\n        features: ['translation', 'messages'],\n        num_rows: 2507\n    })\n})"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:33:09.644615Z","iopub.execute_input":"2024-12-25T15:33:09.644974Z","iopub.status.idle":"2024-12-25T15:33:09.651019Z","shell.execute_reply.started":"2024-12-25T15:33:09.644943Z","shell.execute_reply":"2024-12-25T15:33:09.650280Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'translation': {'en': 'Give your application an accessibility workout',\n  'hi': 'à¤…à¤ªà¤¨à¥‡ à¤…à¤¨à¥à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¥‹ à¤ªà¤¹à¥à¤‚à¤šà¤¨à¥€à¤¯à¤¤à¤¾ à¤µà¥à¤¯à¤¾à¤¯à¤¾à¤® à¤•à¤¾ à¤²à¤¾à¤­ à¤¦à¥‡à¤‚'},\n 'messages': [{'content': 'Translate given sentence to Hindi: Give your application an accessibility workout',\n   'role': 'user'},\n  {'content': 'à¤…à¤ªà¤¨à¥‡ à¤…à¤¨à¥à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¥‹ à¤ªà¤¹à¥à¤‚à¤šà¤¨à¥€à¤¯à¤¤à¤¾ à¤µà¥à¤¯à¤¾à¤¯à¤¾à¤® à¤•à¤¾ à¤²à¤¾à¤­ à¤¦à¥‡à¤‚',\n   'role': 'assistant'}]}"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## 3. Fine-tune LLM using `trl` and the `SFTTrainer` with LoRA\n\nThe [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` provides integration with LoRA adapters through the [PEFT](https://huggingface.co/docs/peft/en/index) library. Key advantages of this setup include:\n\n1. **Memory Efficiency**: \n   - Only adapter parameters are stored in GPU memory\n   - Base model weights remain frozen and can be loaded in lower precision\n   - Enables fine-tuning of large models on consumer GPUs\n\n2. **Training Features**:\n   - Native PEFT/LoRA integration with minimal setup\n   - Support for QLoRA (Quantized LoRA) for even better memory efficiency\n\n3. **Adapter Management**:\n   - Adapter weight saving during checkpoints\n   - Features to merge adapters back into base model\n\nWe'll use LoRA in our example, which combines LoRA with 4-bit quantization to further reduce memory usage without sacrificing performance. The setup requires just a few configuration steps:\n1. Define the LoRA configuration (rank, alpha, dropout)\n2. Create the SFTTrainer with PEFT config\n3. Train and save the adapter weights\n","metadata":{"id":"9TOhJdtsouLk"}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom trl import SFTConfig, SFTTrainer, setup_chat_format\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:33:17.875818Z","iopub.execute_input":"2024-12-25T15:33:17.876107Z","iopub.status.idle":"2024-12-25T15:33:33.607162Z","shell.execute_reply.started":"2024-12-25T15:33:17.876086Z","shell.execute_reply":"2024-12-25T15:33:33.606232Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e851a3981b4248cea36b24bf5b147f74"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"device = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n)\n\n# Load the model and tokenizer\nmodel_name = \"HuggingFaceTB/SmolLM2-135M\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    pretrained_model_name_or_path=model_name\n).to(device)\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n\n# Set up the chat format\nmodel, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n\n# Set our name for the finetune to be saved &/ uploaded to\nfinetune_name = \"SmolLM2-FT-EnHindiTranslation\"\nfinetune_tags = [\"smol-course\", \"LoRA\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:33:51.334072Z","iopub.execute_input":"2024-12-25T15:33:51.334396Z","iopub.status.idle":"2024-12-25T15:34:00.788678Z","shell.execute_reply.started":"2024-12-25T15:33:51.334371Z","shell.execute_reply":"2024-12-25T15:34:00.787759Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"580b23b9b97649cb98a881e1f66032bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d008e0d1b135478e8be23bf5b983e6e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fefdbe60c3bd45d998191f4ecd8fcb21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.66k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"128b6e3af4d241af80eff51b21713eed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18184ee441be4017ba906b80391524a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c8179a9b5ad49089cb4da23e9a14184"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"456226ed6cda4d0cad445ad4c694b139"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"272feb036a6d44aea67539ee5ed1a89c"}},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"TheÂ `SFTTrainer`Â  supports a native integration withÂ `peft`, which makes it super easy to efficiently tune LLMs using, e.g. LoRA. We only need to create ourÂ `LoraConfig`Â and provide it to the trainer.\n\n<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n    <h2 style='margin: 0;color:blue'>Exercise: Define LoRA parameters for finetuning</h2>\n    <p>Take a dataset from the Hugging Face hub and finetune a model on it. </p> \n    <p><b>Difficulty Levels</b></p>\n    <p>ğŸ¢ Use the general parameters for an abitrary finetune</p>\n    <p>ğŸ• Adjust the parameters and review in weights & biases.</p>\n    <p>ğŸ¦ Adjust the parameters and show change in inference results.</p>\n</div>","metadata":{"id":"ZbuVArTHouLk"}},{"cell_type":"code","source":"from peft import LoraConfig\n\n# TODO: Configure LoRA parameters\n# r: rank dimension for LoRA update matrices (smaller = more compression)\nrank_dimension = 8\n# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\nlora_alpha = 16\n# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\nlora_dropout = 0.05\n\npeft_config = LoraConfig(\n    r=rank_dimension,  # Rank dimension - typically between 4-32\n    lora_alpha=lora_alpha,  # LoRA scaling factor - typically 2x rank\n    lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n    bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n    target_modules=\"all-linear\",  # Which modules to apply LoRA to\n    task_type=\"CAUSAL_LM\",  # Task type for model architecture\n)","metadata":{"id":"blDSs9swouLk","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:37:20.732751Z","iopub.execute_input":"2024-12-25T15:37:20.733101Z","iopub.status.idle":"2024-12-25T15:37:20.737484Z","shell.execute_reply.started":"2024-12-25T15:37:20.733074Z","shell.execute_reply":"2024-12-25T15:37:20.736657Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Before we can start our training we need to define the hyperparameters (`TrainingArguments`) we want to use.","metadata":{"id":"l5NUDPcaouLl"}},{"cell_type":"code","source":"# Training configuration\n# Hyperparameters based on QLoRA paper recommendations\noutput_dir_name = finetune_name + \"_\" + str(rank_dimension) + \"_\" + str(lora_alpha)\nargs = SFTConfig(\n    # Output settings\n    output_dir=output_dir_name,  # Directory to save model checkpoints\n    # Training duration\n    num_train_epochs=1,  # Number of training epochs\n    # Batch size settings\n    per_device_train_batch_size=2,  # Batch size per GPU\n    gradient_accumulation_steps=2,  # Accumulate gradients for larger effective batch\n    # Memory optimization\n    gradient_checkpointing=True,  # Trade compute for memory savings\n    # Optimizer settings\n    optim=\"adamw_torch_fused\",  # Use fused AdamW for efficiency\n    learning_rate=2e-4,  # Learning rate (QLoRA paper)\n    max_grad_norm=0.3,  # Gradient clipping threshold\n    # Learning rate schedule\n    warmup_ratio=0.03,  # Portion of steps for warmup\n    lr_scheduler_type=\"constant\",  # Keep learning rate constant after warmup\n    # Logging and saving\n    logging_steps=10,  # Log metrics every N steps\n    save_strategy=\"epoch\",  # Save checkpoint every epoch\n    # Precision settings\n    bf16=True,  # Use bfloat16 precision\n    # Integration settings\n    push_to_hub=False,  # Don't push to HuggingFace Hub\n    report_to=\"wandb\",  # Disable external logging\n)","metadata":{"id":"NqT28VZlouLl","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:37:24.178927Z","iopub.execute_input":"2024-12-25T15:37:24.179223Z","iopub.status.idle":"2024-12-25T15:37:24.210586Z","shell.execute_reply.started":"2024-12-25T15:37:24.179202Z","shell.execute_reply":"2024-12-25T15:37:24.209907Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"We now have every building block we need to create ourÂ `SFTTrainer`Â to start then training our model.","metadata":{"id":"cGhR7uFBouLl"}},{"cell_type":"code","source":"max_seq_length = 1512  # max sequence length for model and packing of the dataset\n\n# Create SFTTrainer with LoRA configuration\ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=dataset[\"train\"],\n    peft_config=peft_config,  # LoRA configuration\n    max_seq_length=max_seq_length,  # Maximum sequence length\n    tokenizer=tokenizer,\n    packing=True,  # Enable input packing for efficiency\n    dataset_kwargs={\n        \"add_special_tokens\": False,  # Special tokens handled by template\n        \"append_concat_token\": False,  # No additional separator needed\n    },\n)","metadata":{"id":"M00Har2douLl","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:37:27.023153Z","iopub.execute_input":"2024-12-25T15:37:27.023436Z","iopub.status.idle":"2024-12-25T15:37:36.918854Z","shell.execute_reply.started":"2024-12-25T15:37:27.023416Z","shell.execute_reply":"2024-12-25T15:37:36.918050Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing, dataset_kwargs. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:334: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c887c6204dc14ab189c8fa6100edb602"}},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"Start training our model by calling the `train()` method on our `Trainer` instance. This will start the training loop and train our model for 3 epochs. Since we are using a PEFT method, we will only save the adapted model weights and not the full model.","metadata":{"id":"zQ_kRN24ouLl"}},{"cell_type":"code","source":"# start training, the model will be automatically saved to the hub and the output directory\ntrainer.train()\n\n# save model\ntrainer.save_model()","metadata":{"id":"Tq4nIYqKouLl","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T15:37:39.779989Z","iopub.execute_input":"2024-12-25T15:37:39.780312Z","iopub.status.idle":"2024-12-25T15:59:04.443933Z","shell.execute_reply.started":"2024-12-25T15:37:39.780286Z","shell.execute_reply":"2024-12-25T15:59:04.443169Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mavisha-bhiryani\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241225_153746-4fvb0ms2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/avisha-bhiryani/huggingface/runs/4fvb0ms2' target=\"_blank\">SmolLM2-FT-EnHindiTranslation_8_16</a></strong> to <a href='https://wandb.ai/avisha-bhiryani/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/avisha-bhiryani/huggingface' target=\"_blank\">https://wandb.ai/avisha-bhiryani/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/avisha-bhiryani/huggingface/runs/4fvb0ms2' target=\"_blank\">https://wandb.ai/avisha-bhiryani/huggingface/runs/4fvb0ms2</a>"},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [450/450 21:08, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.557400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.348500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.191200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.061700</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.964000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.883800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.828200</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.749500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.696700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.648700</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.611900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.588500</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.541900</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.522000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.501800</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.488200</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.448400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.442000</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.430900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.410300</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.395300</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.352500</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.387400</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.385100</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.336200</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.370300</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.344000</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.304700</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.316400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.340800</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.309100</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.276400</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.281000</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.282400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.256300</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.256000</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.254300</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.276100</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.236100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.244400</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.214100</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.247900</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>1.209400</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.224000</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.207700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"**For tuning the parameters I created 100 shards**\nfor 8,16 and for 10,18 the drop is not big -> 8, 16 final\n**Having tuned the parameters the number of shards reduced to 30**","metadata":{}},{"cell_type":"markdown","source":"The training with Flash Attention for 3 epochs with a dataset of 15k samples took 4:14:36 on a `g5.2xlarge`. The instance costs `1.21$/h` which brings us to a total cost of only ~`5.3$`.\n\n","metadata":{"id":"y4HHSYYzouLl"}},{"cell_type":"markdown","source":"### Merge LoRA Adapter into the Original Model\n\nWhen using LoRA, we only train adapter weights while keeping the base model frozen. During training, we save only these lightweight adapter weights (~2-10MB) rather than a full model copy. However, for deployment, you might want to merge the adapters back into the base model for:\n\n1. **Simplified Deployment**: Single model file instead of base model + adapters\n2. **Inference Speed**: No adapter computation overhead\n3. **Framework Compatibility**: Better compatibility with serving frameworks\n","metadata":{"id":"C309KsXjouLl"}},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\n\n\n# Load PEFT model on CPU\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    pretrained_model_name_or_path=args.output_dir,\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n)\n\n# Merge LoRA and base model and save\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(\n    args.output_dir, safe_serialization=True, max_shard_size=\"2GB\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:04:25.161822Z","iopub.execute_input":"2024-12-25T16:04:25.162150Z","iopub.status.idle":"2024-12-25T16:04:27.237128Z","shell.execute_reply.started":"2024-12-25T16:04:25.162126Z","shell.execute_reply":"2024-12-25T16:04:27.236410Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## 3. Test Model and run Inference\n\nAfter the training is done we want to test our model. We will load different samples from the original dataset and evaluate the model on those samples, using a simple loop and accuracy as our metric.\n\n","metadata":{"id":"-yO6E9quouLl"}},{"cell_type":"markdown","source":"<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n    <h2 style='margin: 0;color:blue'>Bonus Exercise: Load LoRA Adapter</h2>\n    <p>Use what you learnt from the ecample note book to load your trained LoRA adapter for inference.</p> \n</div>","metadata":{}},{"cell_type":"code","source":"# free the memory again\ndel model\ndel trainer\ntorch.cuda.empty_cache()","metadata":{"id":"I5B494OdouLl","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:11:52.592954Z","iopub.execute_input":"2024-12-25T16:11:52.593252Z","iopub.status.idle":"2024-12-25T16:11:52.695366Z","shell.execute_reply.started":"2024-12-25T16:11:52.593231Z","shell.execute_reply":"2024-12-25T16:11:52.694576Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline\n\n# Load Model with PEFT adapter\ntokenizer = AutoTokenizer.from_pretrained(output_dir_name)\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    output_dir_name, device_map=\"auto\", torch_dtype=torch.float16,\n)\npipe = pipeline(\n    \"translation\", model=merged_model, tokenizer=tokenizer, device=device,\n)","metadata":{"id":"P1UhohVdouLl","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:54:56.788986Z","iopub.execute_input":"2024-12-25T16:54:56.789323Z","iopub.status.idle":"2024-12-25T16:54:58.482444Z","shell.execute_reply.started":"2024-12-25T16:54:56.789282Z","shell.execute_reply":"2024-12-25T16:54:58.481583Z"}},"outputs":[{"name":"stderr","text":"The model 'LlamaForCausalLM' is not supported for translation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n","output_type":"stream"}],"execution_count":76},{"cell_type":"markdown","source":"Lets test some prompt samples and see how the model performs.","metadata":{"id":"99uFDAuuouLl"}},{"cell_type":"code","source":"prompts = [\n    \"Translate to Hindi: Select\",\n]\n\n\ndef test_inference(prompt):\n    prompt = pipe.tokenizer.apply_chat_template(\n        [{\"role\": \"user\", \"content\": prompt}],\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n    outputs = pipe(\n        prompt,\n    )\n    print(outputs)\n    return outputs[0][\"translation_text\"][len(prompt):]\n\n\nfor prompt in prompts:\n    print(f\"    prompt:\\n{prompt}\")\n    print(f\"    response:\\n{test_inference(prompt)}\")\n    print(\"-\" * 50)","metadata":{"id":"-shSmUbvouLl","outputId":"16d97c61-3b31-4040-c780-3c4de75c3824","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T16:58:48.465318Z","iopub.execute_input":"2024-12-25T16:58:48.465663Z","iopub.status.idle":"2024-12-25T16:58:48.654028Z","shell.execute_reply.started":"2024-12-25T16:58:48.465635Z","shell.execute_reply":"2024-12-25T16:58:48.653076Z"}},"outputs":[{"name":"stdout","text":"    prompt:\nTranslate to Hindi: Select\n[{'translation_text': 'user\\nTranslate to Hindi: Select\\nassistant\\nà¤šà¥à¤¨à¥‡'}]\n    response:\n\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}