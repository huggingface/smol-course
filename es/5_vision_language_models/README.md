# Modelos de Lenguaje Visual (VLM)

## 1. Uso de VLM

Los Modelos de Lenguaje Visuales (VLMs, por sus siglas en ingl茅s) procesan entradas de im谩genes junto con texto para habilitar tareas como la descripci贸n de im谩genes, respuestas a preguntas visuales y razonamiento multimodal.

Una arquitectura t铆pica de VLM consta de un codificador de im谩genes para extraer caracter铆sticas visuales, una capa de proyecci贸n para alinear representaciones visuales y textuales, y un modelo de lenguaje para procesar o generar texto. Esto permite que el modelo establezca conexiones entre elementos visuales y conceptos ling眉铆sticos.

Los VLMs pueden usarse en diferentes configuraciones dependiendo del caso de uso. Los modelos base manejan tareas generales de visi贸n y lenguaje, mientras que las variantes optimizadas para chat soportan interacciones conversacionales. Algunos modelos incluyen componentes adicionales para anclar las predicciones en evidencia visual o especializarse en tareas espec铆ficas como la detecci贸n de objetos.

Para m谩s detalles sobre la parte t茅cnica y el uso de VLMs, consulta la p谩gina de [Uso de VLM](./vlm_usage.md).

## 2. Fine-Tuning de VLM

El fine-tuning de un VLM implica adaptar un modelo preentrenado para realizar tareas espec铆ficas o para operar eficazmente en un conjunto de datos particular. El proceso puede seguir metodolog铆as como el fine-tuning supervisado, optimizaci贸n por preferencias o un enfoque h铆brido que combine ambos, como se introdujo en los M贸dulos 1 y 2.

Aunque las herramientas y t茅cnicas fundamentales son similares a las utilizadas para los LLMs, el fine-tuning de VLMs requiere un enfoque adicional en la representaci贸n y preparaci贸n de datos para im谩genes. Esto asegura que el modelo integre y procese de manera efectiva tanto los datos visuales como textuales para un rendimiento 贸ptimo. Dado que el modelo de demostraci贸n, SmolVLM, es significativamente m谩s grande que el modelo de lenguaje utilizado en el m贸dulo anterior, es esencial explorar m茅todos para un fine-tuning eficiente. T茅cnicas como la cuantizaci贸n y PEFT pueden ayudar a hacer el proceso m谩s accesible y rentable, permitiendo que m谩s usuarios experimenten con el modelo.

Para obtener una gu铆a detallada sobre el fine-tuning de VLMs, visita la p谩gina de [Fine-Tuning de VLM](./vlm_finetuning.md).

## Cuadernos de Ejercicios

| T铆tulo | Descripci贸n | Ejercicio | Enlace | Colab |
|--------|-------------|-----------|--------|-------|
| Uso de VLM | Aprende c贸mo cargar y usar un VLM preentrenado para diversas tareas |  Procesar una imagen<br> Procesar m煤ltiples im谩genes con manejo de lotes <br> Procesar un video completo | [Cuaderno](./notebooks/vlm_usage_sample.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/5_vision_language_models/notebooks/vlm_usage_sample.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Abrir en Colab"/></a> |
| Fine-Tuning de VLM | Aprende c贸mo realizar el fine-tuning de un VLM preentrenado para conjuntos de datos espec铆ficos |  Usar un conjunto de datos b谩sico para el fine-tuning<br> Probar un nuevo conjunto de datos<br> Experimentar con m茅todos alternativos de fine-tuning | [Cuaderno](./notebooks/vlm_sft_sample.ipynb)| <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/5_vision_language_models/notebooks/vlm_sft_sample.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Abrir en Colab"/></a> |

## Referencias  
- [Hugging Face Learn: Fine-Tuning Supervisado de VLMs](https://huggingface.co/learn/cookbook/fine_tuning_vlm_trl)
- [Hugging Face Learn: Fine-Tuning Supervisado de SmolVLM](https://huggingface.co/learn/cookbook/fine_tuning_smol_vlm_sft_trl)  
- [Hugging Face Learn: Fine-Tuning por Optimizaci贸n de Preferencias SmolVLM](https://huggingface.co/learn/cookbook/fine_tuning_vlm_dpo_smolvlm_instruct)  
- [Hugging Face Blog: Optimizaci贸n de Preferencias para VLMs](https://huggingface.co/blog/dpo_vlm)
- [Hugging Face Blog: Modelos de Lenguaje Visuales](https://huggingface.co/blog/vlms)
- [Hugging Face Blog: SmolVLM](https://huggingface.co/blog/smolvlm)  
- [Hugging Face Model: SmolVLM-Instruct](https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct)
- [CLIP: Aprendiendo Modelos Visual Transferibles desde Supervisi贸n en Lenguaje Natural](https://arxiv.org/abs/2103.00020)  
- [Align Before Fuse: Aprendizaje de Representaciones de Visi贸n y Lenguaje con Destilaci贸n por Momentum](https://arxiv.org/abs/2107.07651)
