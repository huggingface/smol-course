# Modelos de Visi√≥n-Lenguaje

## 1. Uso de VLM

Los Modelos de Visi√≥n-Lenguaje (VLMs por sus siglas en ingl√©s) procesan im√°genes y texto de manera simult√°nea para realizar tareas como la generaci√≥n de descripciones de im√°genes, la respuesta a preguntas visuales y el razonamiento multimodal.

Una arquitectura t√≠pica de VLM incluye un codificador de im√°genes para extraer caracter√≠sticas visuales, una capa de proyecci√≥n para alinear las representaciones visuales y textuales, y un modelo de lenguaje para procesar o generar texto. Esto permite al modelo establecer conexiones entre elementos visuales y conceptos ling√º√≠sticos.

Los VLMs pueden configurarse seg√∫n el caso de uso. Los modelos base manejan tareas generales de visi√≥n-lenguaje, mientras que las variantes optimizadas para chat permiten interacciones conversacionales. Algunos modelos incluyen componentes adicionales para fundamentar predicciones en evidencia visual o especializarse en tareas espec√≠ficas, como la detecci√≥n de objetos.

Para m√°s detalles sobre el uso y las t√©cnicas de VLMs, consulta la p√°gina [Uso de VLM](./vlm_usage.md).

## 2. Fine-tuning de VLM

El fine-tuning de un VLM consiste en adaptar un modelo previamente entrenado para realizar tareas espec√≠ficas o mejorar su desempe√±o en un conjunto de datos determinado. Este proceso puede seguir metodolog√≠as como el fine-tuning supervisado, la optimizaci√≥n por preferencias o un enfoque h√≠brido que combine ambos, como se introduce en los M√≥dulos 1 y 2.

Aunque las herramientas y t√©cnicas principales son similares a las utilizadas en los LLMs, el fine-tuning de VLMs requiere un enfoque adicional en la representaci√≥n y preparaci√≥n de datos de im√°genes. Esto garantiza que el modelo integre y procese de manera efectiva tanto los datos visuales como los textuales para un rendimiento √≥ptimo. Dado que el modelo de demostraci√≥n, SmolVLM, es significativamente m√°s grande que el modelo de lenguaje utilizado en el m√≥dulo anterior, es fundamental explorar m√©todos para un fine-tuning eficiente. T√©cnicas como la cuantizaci√≥n y PEFT pueden hacer que el proceso sea m√°s accesible y rentable, permitiendo que m√°s usuarios experimenten con el modelo.

Para obtener una gu√≠a detallada sobre el fine-tuning de VLMs, visita la p√°gina [fine-tuning de VLM](./vlm_finetuning.md).

## Cuadernos de ejercicios

| T√≠tulo | Descripci√≥n | Ejercicio | Enlace | Colab |
|--------|-------------|-----------|--------|-------|
| Uso de VLM | Aprende a cargar y usar un VLM preentrenado para diversas tareas | üê¢ Procesar una imagen<br>üêï Procesar m√∫ltiples im√°genes con manejo por lotes<br>ü¶Å Procesar un video completo | [Notebook](./notebooks/vlm_usage_sample.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/5_vision_language_models/notebooks/vlm_usage_sample.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Abrir en Colab"/></a> |
| fine-tuning de VLM | Aprende a ajustar un VLM preentrenado para conjuntos de datos espec√≠ficos | üê¢ Usar un conjunto de datos b√°sico para fine-tuning<br>üêï Probar un nuevo conjunto de datos<br>ü¶Å Experimentar con m√©todos alternativos de fine-tuning | [Notebook](./notebooks/vlm_sft_sample.ipynb) | <a target="_blank" href="https://colab.research.google.com/github/huggingface/smol-course/blob/main/5_vision_language_models/notebooks/vlm_sft_sample.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Abrir en Colab"/></a> |

## Referencias

- [Hugging Face Learn: fine-tuning supervisado de VLMs](https://huggingface.co/learn/cookbook/fine_tuning_vlm_trl)
- [Hugging Face Learn: fine-tuning supervisado de SmolVLM](https://huggingface.co/learn/cookbook/fine_tuning_smol_vlm_sft_trl)
- [Hugging Face Learn: Optimizaci√≥n de preferencias para el fine-tuning de SmolVLM](https://huggingface.co/learn/cookbook/fine_tuning_vlm_dpo_smolvlm_instruct)
- [Hugging Face Blog: Optimizaci√≥n de preferencias para VLMs](https://huggingface.co/blog/dpo_vlm)
- [Hugging Face Blog: Modelos de Visi√≥n y Lenguaje](https://huggingface.co/blog/vlms)
- [Hugging Face Blog: SmolVLM](https://huggingface.co/blog/smolvlm)
- [Hugging Face Model: SmolVLM-Instruct](https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct)
- [CLIP: Learning Transferable Visual Models from Natural Language Supervision](https://arxiv.org/abs/2103.00020)
- [Align Before Fuse: Vision and Language Representation Learning with Momentum Distillation](https://arxiv.org/abs/2107.07651)
