{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/branley1/smol-course/blob/main/1_instruction_tuning/notebooks/sft_finetuning_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2heHFnG4fr7",
        "outputId": "de8fe52e-cdb5-4924-dd66-36da669cc17f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.26.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (1.1.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (2.5.1+cu121)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n"
          ]
        }
      ],
      "source": [
        "# Install the requirements in Google Colab\n",
        "!pip install transformers datasets trl huggingface_hub\n",
        "\n",
        "# Authenticate to Hugging Face\n",
        "from huggingface_hub import login\n",
        "!huggingface-cli login\n",
        "\n",
        "login()\n",
        "\n",
        "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w06u0bno4fr7"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
        "import torch\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Load the SmolLM2 model and tokenizer\n",
        "model_smol = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\").to(device)\n",
        "tokenizer_smol = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
        "\n",
        "# Set up the chat format for SmolLM2\n",
        "model_smol, tokenizer_smol = setup_chat_format(model=model_smol, tokenizer=tokenizer_smol)\n",
        "\n",
        "\n",
        "# Load the GPT-2 tokenizer for ds_code_parrot\n",
        "tokenizer_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "if tokenizer_gpt2.pad_token is None:\n",
        "    tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token\n",
        "\n",
        "# Set our name for the finetune to be saved &/ uploaded to\n",
        "finetune_name = \"SmolLM2-FT-MyDataset\"\n",
        "finetune_tags = [\"smol-course\", \"module_1\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wefSajxR4fr8"
      },
      "source": [
        "# Generate with the base model\n",
        "\n",
        "Here we will try out the base model which does not have a chat template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8zY1Cn84fr8"
      },
      "outputs": [],
      "source": [
        "# Let's test the base model before training\n",
        "prompt = \"Write a haiku about programming\"\n",
        "\n",
        "# Format with template\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "formatted_prompt = tokenizer_smol.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "# Generate response\n",
        "inputs = tokenizer_smol(formatted_prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = model_smol.generate(**inputs, max_new_tokens=100)\n",
        "display(\"Before training:\")\n",
        "display(tokenizer_smol.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ModojJHX4fr8"
      },
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "We will load a sample dataset and format it for training. The dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model.\n",
        "\n",
        "**TRL will format input messages based on the model's chat templates.** They need to be represented as a list of dictionaries with the keys: `role` and `content`,."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD6-MAn_4fr8"
      },
      "outputs": [],
      "source": [
        "# Load a sample dataset\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Tokenizer setup\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define datasets\n",
        "# Smoltalk dataset\n",
        "ds_smoltalk = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")\n",
        "\n",
        "# Process Smoltalk dataset for TRL chat templates\n",
        "def process_smoltalk(sample):\n",
        "    # Convert Smoltalk conversation to chat format\n",
        "    messages = [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in sample[\"messages\"]]\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "# Apply the processing\n",
        "ds_smoltalk = ds_smoltalk.map(process_smoltalk, remove_columns=ds_smoltalk[\"train\"].column_names)\n",
        "\n",
        "# BigCode dataset\n",
        "ds_big_code = load_dataset(path=\"bigcode/the-stack-smol-xl\", data_dir=\"data/python\")\n",
        "\n",
        "# Process BigCode dataset for TRL chat templates\n",
        "def process_big_code(sample):\n",
        "    # For BigCode, structure the code snippet into a chat format\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a programming assistant specializing in Python.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write or explain the following Python code:\"},\n",
        "        {\"role\": \"assistant\", \"content\": sample[\"content\"]},\n",
        "    ]\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "# Convert the generator to a list before creating the Dataset\n",
        "processed_big_code = [process_big_code(sample) for sample in ds_big_code[\"train\"]]\n",
        "ds_big_code_train = Dataset.from_list(processed_big_code)  # Create a new Dataset object\n",
        "ds_big_code[\"train\"] = ds_big_code_train  # Replace the original 'train' split with the processed one\n",
        "\n",
        "# CodeParrot dataset\n",
        "languages_to_keep = [\"Python\", \"C++\", \"C\", \"HTML\", \"CSS\", \"SQL\", \"TypeScript\", \"TeX\"]\n",
        "ds_code_parrot = load_dataset(\"codeparrot/github-code\", streaming=True, split=\"train\").filter(\n",
        "    lambda x: x[\"language\"] in languages_to_keep)\n",
        "\n",
        "# Convert processed CodeParrot datatset into a generator\n",
        "processed_code_parrot = (process_code_parrot(sample) for sample in ds_code_parrot)\n",
        "\n",
        "# Process CodeParrot dataset for TRL chat templates\n",
        "def process_code_parrot(sample):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": f\"You are a programming assistant specializing in {sample['language']}.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write or explain the following code snippet:\"},\n",
        "        {\"role\": \"assistant\", \"content\": sample[\"code\"]},\n",
        "    ]\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "# Map the processing directly on the streaming dataset\n",
        "processed_code_parrot = ds_code_parrot.map(process_code_parrot)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# General tokenization function\n",
        "def tokenize_messages_dataset(dataset,tokenizer):\n",
        "  \"\"\"\n",
        "  Tokenizes a dataset using the specified tokenizer.\n",
        "  Args:\n",
        "      dataset: The dataset to tokenize.\n",
        "      tokenizer: The tokenizer to use.\n",
        "  Returns:\n",
        "      Tokenized dataset.\n",
        "  \"\"\"\n",
        "  def tokenize_function(sample):\n",
        "      chat_str = \"\"\n",
        "      for msg in sample.get(\"messages\", []):\n",
        "        if isinstance(msg, dict): # Check if msg is in a dictionary\n",
        "          role, content = msg.get(\"role\", \"\").capitalize(), msg.get(\"content\", \"\")\n",
        "          chat_str += f\"{role}: {content}\\n\"\n",
        "\n",
        "      # Tokenize and pad\n",
        "      return tokenizer(\n",
        "          chat_str,\n",
        "          truncation=True,\n",
        "          padding=\"max_length\",\n",
        "          max_length=512,\n",
        "          return_attention_mask=True,\n",
        "          )\n",
        "\n",
        "  return dataset.map(tokenize_function, batched=False, remove_columns=[\"messages\"])\n",
        "\n",
        "# Tokenization function for datasets with 'code'\n",
        "def tokenize_code_dataset(dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Tokenizes datasets that have a 'code' field.\n",
        "    \"\"\"\n",
        "    def tokenize_function(sample):\n",
        "        code_str = sample.get(\"code\", \"\")\n",
        "        return tokenizer(\n",
        "            code_str,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=512,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "    return dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=False, # Process one sample at a time\n",
        "        remove_columns=[\"code\", \"repo_name\", \"path\", \"language\", \"license\", \"size\"]\n",
        "    )\n",
        "\n",
        "def add_labels_to_dataset(dataset):\n",
        "    \"\"\"\n",
        "    Adds a 'labels' field to the dataset, which is a copy of 'input_ids'.\n",
        "    \"\"\"\n",
        "    def add_labels(batch):\n",
        "      # Ensure 'input_ids' is a list before copying\n",
        "        if isinstance(batch[\"input_ids\"], list):\n",
        "          batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
        "        else:\n",
        "          raise KeyError(\"'input_ids' should be a list of integers.\")\n",
        "        return batch\n",
        "\n",
        "    return dataset.map(add_labels)\n",
        "\n",
        "# Tokenize Smoltalk and BigCode datasets and save to disk\n",
        "tokenized_smoltalk = tokenize_messages_dataset(ds_smoltalk, tokenizer_smol)\n",
        "tokenized_smoltalk.save_to_disk(\"tokenized_smoltalk\")\n",
        "\n",
        "tokenized_big_code = tokenize_messages_dataset(ds_big_code[\"train\"], tokenizer_smol)\n",
        "tokenized_big_code.save_to_disk(\"tokenized_big_code\")\n",
        "\n",
        "# Tokenize CodeParrot dataset\n",
        "# Since 'ds_code_parrot' is an IterableDataset, take a subset and convert it to a regular Dataset\n",
        "tokenized_code_parrot_iterable = tokenize_code_dataset(ds_code_parrot, tokenizer_gpt2).take(512)\n",
        "# Convert the IterableDataset to a list to materialize it\n",
        "tokenized_code_parrot_list = list(tokenized_code_parrot_iterable)\n",
        "# Create an in-memory Dataset from the list\n",
        "tokenized_code_parrot_dataset = Dataset.from_list(tokenized_code_parrot_list)\n",
        "tokenized_code_parrot_dataset.save_to_disk(\"tokenized_code_parrot\")\n",
        "\n",
        "# Convert the IterableDataset to a list\n",
        "tokenized_code_parrot_list = list(tokenized_code_parrot_iterable)\n",
        "\n",
        "# Create an in-memory Dataset from the list\n",
        "tokenized_code_parrot_dataset = Dataset.from_list(tokenized_code_parrot_list)\n",
        "\n",
        "# Add labels to all tokenized datasets\n",
        "tokenized_smoltalk = add_labels_to_dataset(tokenized_smoltalk)\n",
        "tokenized_big_code = add_labels_to_dataset(tokenized_big_code)\n",
        "tokenized_code_parrot = add_labels_to_dataset(tokenized_code_parrot_dataset)\n",
        "\n",
        "# Inspect the first sample of each tokenized dataset\n",
        "# print(\"Smoltalk Sample:\", tokenized_smoltalk[\"train\"][0])\n",
        "# print(\"BigCode Sample:\", tokenized_big_code[0])\n",
        "# print(\"CodeParrot Sample:\", tokenized_code_parrot_dataset[0])"
      ],
      "metadata": {
        "id": "zohM-NKXTjx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Smoltalk and BigCode (using SmolLM2)\n",
        "model_name_smol = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "model_smol = AutoModelForCausalLM.from_pretrained(model_name_smol).to(device)\n",
        "tokenizer_smol = AutoTokenizer.from_pretrained(model_name_smol)\n",
        "model_smol, tokenizer_smol = setup_chat_format(model=model_smol, tokenizer=tokenizer_smol)\n",
        "\n",
        "# For CodeParrot (using gpt2)\n",
        "model_name_gpt2 = \"gpt2\"  # Or a specific gpt2 variant like \"gpt2-medium\"\n",
        "model_gpt2 = AutoModelForCausalLM.from_pretrained(model_name_gpt2).to(device)\n",
        "tokenizer_gpt2 = AutoTokenizer.from_pretrained(model_name_gpt2)\n",
        "model_gpt2, tokenizer_gpt2 = setup_chat_format(model=model_gpt2, tokenizer=tokenizer_gpt2)\n"
      ],
      "metadata": {
        "id": "QxP9VCvOcHkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K8Lgm3k4fr9"
      },
      "source": [
        "## Configuring the SFTTrainer\n",
        "\n",
        "The `SFTTrainer` is configured with various parameters that control the training process. These include the number of training steps, batch size, learning rate, and evaluation strategy. Adjust these parameters based on your specific requirements and computational resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmY_ZyD84fr9"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_from_disk\n",
        "from torch.utils.data import DataLoader\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# 1. Initialize Accelerator for mixed precision and optimized training\n",
        "accelerator = Accelerator(mixed_precision='fp16')\n",
        "\n",
        "# 2. Load Pre-trained Model and Tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').to(accelerator.device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# 3. Load Pre-tokenized Datasets\n",
        "tokenized_smoltalk = load_from_disk(\"tokenized_smoltalk\")\n",
        "tokenized_big_code = load_from_disk(\"tokenized_big_code\")\n",
        "tokenized_code_parrot = load_from_disk(\"tokenized_code_parrot\").select(range(512))\n",
        "\n",
        "# Split CodeParrot dataset into training and Evaluation Sets (90% train, 10% eval)\n",
        "split_dataset = tokenized_code_parrot_dataset.train_test_split(test_size=0.1)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset = split_dataset[\"test\"]\n",
        "\n",
        "# SFTConfig for SmolLM2\n",
        "sft_config_smol = SFTConfig(\n",
        "    output_dir=\"./sft_output\",\n",
        "    max_steps=1000,  # Adjust based on dataset size and desired training duration\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,  # Increase from 4 to 8\n",
        "    gradient_accumulation_steps=2, # Accumulates gradients over 2 steps\n",
        "    learning_rate=5e-5,  # Default LR for fine-tuning\n",
        "    max_seq_length=256,\n",
        "    logging_steps=100,  # Reduced logging steps\n",
        "    save_steps=1000,  # Reduced checkpoint frequency\n",
        "    evaluation_strategy=\"steps\",  # Evaluate the model at regular intervals\n",
        "    eval_steps=500,  # Evaluate every 100 steps\n",
        "    fp16=True, # Enable mixed precision\n",
        "    use_mps_device=(\n",
        "        True if device == \"mps\" else False\n",
        "    ),  # Use MPS for mixed precision training\n",
        "    hub_model_id=\"finetuned_smol_smolLM2\",\n",
        ")\n",
        "\n",
        "# SFTTrainer for Smoltalk (SmolLM2)\n",
        "trainer_smol_smoltalk = SFTTrainer(\n",
        "    model=model_smol,\n",
        "    args=sft_config_smol,\n",
        "    train_dataset=tokenized_smoltalk[\"train\"],\n",
        "    tokenizer=tokenizer_smol,\n",
        "    eval_dataset=tokenized_smoltalk.get(\"test\"),\n",
        ")\n",
        "\n",
        "# SFTTrainer for BigCode (SmolLM2)\n",
        "trainer_smol_bigcode = SFTTrainer(\n",
        "    model=model_smol,\n",
        "    args=sft_config_smol,\n",
        "    train_dataset=tokenized_big_code,\n",
        "    tokenizer=tokenizer_smol,\n",
        "    eval_dataset=tokenized_big_code,\n",
        ")\n",
        "\n",
        "# Configure the SFTTrainer for GPT-2\n",
        "sft_config_gpt2 = SFTConfig(\n",
        "    output_dir=\"./sft_output_codeparrot\",\n",
        "    max_steps=3000, # Increase for larger datasets\n",
        "    per_device_train_batch_size=8, # Adjust for GPU memory\n",
        "    gradient_accumulation_steps=2, # Accumulate gradients over 2 steps\n",
        "    learning_rate=5e-5,\n",
        "    max_seq_length=256,\n",
        "    fp16=True,\n",
        "    logging_steps=50, # Log every 50 steps\n",
        "    save_steps=500, # Save checkpoints every 500 steps\n",
        "    evaluation_strategy=\"steps\", # Evaluate at regular intervals\n",
        "    eval_steps=100, # Evaluate every 100 steps\n",
        "    hub_model_id=\"finetuned_gpt2_codeparrot\",\n",
        ")\n",
        "\n",
        "\n",
        "# Apply tokenization to the streaming dataset. First, convert it to an in-memory\n",
        "# dataset using take(N) to fetch N elements, toList(N) converts it to a regular Python list\n",
        "# Instead of creating a new Dataset object, map the tokenization function directly on the streaming dataset\n",
        "# and then take a subset of the processed data.\n",
        "tokenized_code_parrot = list(ds_code_parrot.map(\n",
        "    lambda x: tokenizer_gpt2(\n",
        "        x[\"code\"],  # Access the 'code' from the sample\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        return_attention_mask=True\n",
        "    ),\n",
        "    batched=True,\n",
        "    remove_columns=[\"repo_name\", \"path\", \"language\", \"license\", \"size\"] # Remove all original columns\n",
        ").take(512)) # Convert to list using list()\n",
        "\n",
        "tokenized_code_parrot_dataset = Dataset.from_list(tokenized_code_parrot)\n",
        "\n",
        "# SFTTrainer for CodeParrot (GPT-2)\n",
        "trainer_gpt2_codeparrot = SFTTrainer(\n",
        "    model=model_gpt2,  # Using GPT-2 for CodeParrot\n",
        "    args=sft_config_gpt2,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer_gpt2,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "# 7. Prepare with Accelerator\n",
        "trainer_smol_smoltalk_acc = accelerator.prepare(trainer_smol_smoltalk)\n",
        "trainer_smol_bigcode_acc = accelerator.prepare(trainer_smol_bigcode)\n",
        "trainer_gpt2_codeparrot_acc = accelerator.prepare(trainer_gpt2_codeparrot)\n",
        "\n",
        "# Initialize DataLoader with Optimized Settings\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_smoltalk,\n",
        "    batch_size=sft_config_smol.per_device_train_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=8,            # Based on your CPU cores\n",
        "    pin_memory=True,          # Speeds up data transfer to GPU\n",
        "    prefetch_factor=4,        # Based on memory\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_big_code,\n",
        "    batch_size=sft_config_smol.per_device_train_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=2,\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_code_parrot,\n",
        "    batch_size=sft_config_gpt2.per_device_train_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSitwzId4fr9"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "With the trainer configured, we can now proceed to train the model. The training process will involve iterating over the dataset, computing the loss, and updating the model's parameters to minimize this loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc9MwFSN4fr9"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "# 1. Log in to your WandB account\n",
        "wandb.login(key=\"4d10e9f814a4d0d285b06e84a8ceed2156ec6ed6\")\n",
        "\n",
        "# wandb.init(project=\"your_project_name\", entity=\"smol-course\")\n",
        "\n",
        "# Train the models\n",
        "trainer_smol_smoltalk.train()\n",
        "trainer_smol_big_code.train()\n",
        "trainer_gpt2_code_parrot.train()\n",
        "\n",
        "# Save the models\n",
        "trainer_smol_smoltalk.save_model(f\"./finetuned_smol_smoltalk\")\n",
        "trainer_smol_smoltalk.push_to_hub(\n",
        "    repo_id=\"bmmasi1/finetuned_smol_smoltalk\",\n",
        "    tags=[\"smol-course\", \"smoltalk\", \"conversation\", \"fine-tuned\"]\n",
        ")\n",
        "trainer_smol_big_code.save_model(f\"./finetuned_smol_bigcode\")\n",
        "trainer_smol_bigcode.push_to_hub(\n",
        "    repo_id=\"bmmasi1/finetuned_smol_bigcode\",\n",
        "    tags=[\"smol-course\", \"bigcode\", \"code\", \"fine-tuned\"]\n",
        ")\n",
        "trainer_gpt2_code_parrot.save_model(f\"./finetuned_gpt2_codeparrot\")\n",
        "trainer_gpt2_code_parrot.push_to_hub(\n",
        "    repo_id=\"bmmasi1/finetuned_gpt2_codeparrot\",\n",
        "    tags=[\"smol-course\", \"codeparrot\", \"code\", \"fine-tuned\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ70HU714fr9"
      },
      "outputs": [],
      "source": [
        "# trainer.push_to_hub(tags=finetune_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWJZz3oa4fr-"
      },
      "source": [
        "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
        "    <h2 style='margin: 0;color:blue'>Bonus Exercise: Generate with fine-tuned model</h2>\n",
        "    <p>üêï Use the fine-tuned to model generate a response, just like with the base example..</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjtJVBMF4fr-"
      },
      "outputs": [],
      "source": [
        "# Test the fine-tuned model on the same prompt\n",
        "\n",
        "# Let's test the base model before training\n",
        "prompt = \"Write a haiku about programming\"\n",
        "\n",
        "# Format with template\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "# Generate response\n",
        "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# TODO: use the fine-tuned to model generate a response, just like with the base example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxeI_Pw14fr-"
      },
      "source": [
        "## üíê You're done!\n",
        "\n",
        "This notebook provided a step-by-step guide to fine-tuning the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer`. By following these steps, you can adapt the model to perform specific tasks more effectively. If you want to carry on working on this course, here are steps you could try out:\n",
        "\n",
        "- Try this notebook on a harder difficulty\n",
        "- Review a colleagues PR\n",
        "- Improve the course material via an Issue or PR."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}