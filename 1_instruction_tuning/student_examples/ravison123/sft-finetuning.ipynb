{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"cb3ae606e7ea4fba9c819b0a63241a45":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c73a57cfb3364978b722de9623646b49","IPY_MODEL_83beef8e6b394f17a828908b9c2746a5","IPY_MODEL_219318bfca6c4733bb76dc7f0db860d0"],"layout":"IPY_MODEL_174030ef07894812ab52511454646de9"}},"c73a57cfb3364978b722de9623646b49":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d36df8327ede4ddfbc0a94f483d61187","placeholder":"​","style":"IPY_MODEL_1323c5e44a0348d1b1445cf086513d3c","value":"Map: 100%"}},"83beef8e6b394f17a828908b9c2746a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5848f45818db4b73ad1333ecc7ee7411","max":119,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b0d8578612474f3ba535a907f0e1e8e0","value":119}},"219318bfca6c4733bb76dc7f0db860d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bce439b1a1c40b8bf15128533be33bb","placeholder":"​","style":"IPY_MODEL_473b677cde704b568b9e5811ad699449","value":" 119/119 [00:00&lt;00:00, 584.87 examples/s]"}},"174030ef07894812ab52511454646de9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d36df8327ede4ddfbc0a94f483d61187":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1323c5e44a0348d1b1445cf086513d3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5848f45818db4b73ad1333ecc7ee7411":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0d8578612474f3ba535a907f0e1e8e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8bce439b1a1c40b8bf15128533be33bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"473b677cde704b568b9e5811ad699449":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e5d999d64c64599ab4e850df6e2c143":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":[],"layout":"IPY_MODEL_43d807aa55ff4fdca30db91aa0d6e408"}},"37cb3b5238d34976a9cf54cfe9e5e155":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e2540726d274cfa987ff2c325a06b1b","placeholder":"​","style":"IPY_MODEL_6bfa5aa43cd44bb194c012390e2b39b7","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"1aeef4b49a3849649d6623dc7118f593":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_adf6ed1a253c455096de9a90fe2ea4cb","placeholder":"​","style":"IPY_MODEL_c65df193ec3940b5b78f11b38cfbea01","value":""}},"108bac6597a8425998c87b765377ad1c":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_3baa7b2334dc4e84a7590f913af3696a","style":"IPY_MODEL_fff496b6ebcd4852a19d3e19e406ba7d","value":true}},"c4ea5302fef54382a2a98394ad99a3d1":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_42137fbc6fea4a499189f90d787fecc8","style":"IPY_MODEL_a4aa33b5b6794f78ad433891eface8fd","tooltip":""}},"9832740263864329bde1c7b71cc5b447":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0707a38cdd2479684b30ddaf315b020","placeholder":"​","style":"IPY_MODEL_c7243fe8e1d8440ca34e547e60b43279","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"43d807aa55ff4fdca30db91aa0d6e408":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"7e2540726d274cfa987ff2c325a06b1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bfa5aa43cd44bb194c012390e2b39b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"adf6ed1a253c455096de9a90fe2ea4cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c65df193ec3940b5b78f11b38cfbea01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3baa7b2334dc4e84a7590f913af3696a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fff496b6ebcd4852a19d3e19e406ba7d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42137fbc6fea4a499189f90d787fecc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4aa33b5b6794f78ad433891eface8fd":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"b0707a38cdd2479684b30ddaf315b020":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7243fe8e1d8440ca34e547e60b43279":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0287d16a58134f45ad3746b7757abca4":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52cbec6bfdce40238b409d314b61b7ea","placeholder":"​","style":"IPY_MODEL_1a4b6b33f63f4b90b9a17411699f29ee","value":"Connecting..."}},"52cbec6bfdce40238b409d314b61b7ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a4b6b33f63f4b90b9a17411699f29ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afc7b2ba2b4644139adbe3c00050c05b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0dff223876314eb98cc04f90c4f9bbb0","IPY_MODEL_71dc975c3f3743a9856a424e9bd26fff","IPY_MODEL_1a227192764a430d821ef2aea4427552"],"layout":"IPY_MODEL_792d58bd830e4a398f0702dc6d67aa65"}},"0dff223876314eb98cc04f90c4f9bbb0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2521c8a04994eff9ab35b3862f363fd","placeholder":"​","style":"IPY_MODEL_3e4eec6dcfdc410b8fb2a92bfcd96aec","value":"Resolving data files: 100%"}},"71dc975c3f3743a9856a424e9bd26fff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b8c931df0e04faca4d1d1c5993250b6","max":30,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8d34a7b4795a4185816c804965f55aaf","value":30}},"1a227192764a430d821ef2aea4427552":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f20fac0b41f4677afdd2893fb258d99","placeholder":"​","style":"IPY_MODEL_b63fe154eda74dceb126e1e2a0bbc4dd","value":" 30/30 [00:00&lt;00:00, 1287.62it/s]"}},"792d58bd830e4a398f0702dc6d67aa65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2521c8a04994eff9ab35b3862f363fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e4eec6dcfdc410b8fb2a92bfcd96aec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b8c931df0e04faca4d1d1c5993250b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d34a7b4795a4185816c804965f55aaf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f20fac0b41f4677afdd2893fb258d99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b63fe154eda74dceb126e1e2a0bbc4dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Supervised Fine-Tuning with SFTTrainer\n\nThis notebook demonstrates how to fine-tune the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer` from the `trl` library. The notebook cells run and will finetune the model. You can select your difficulty by trying out different datasets.\n\n<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n    <h2 style='margin: 0;color:blue'>Exercise: Fine-Tuning SmolLM2 with SFTTrainer</h2>\n    <p>Take a dataset from the Hugging Face hub and finetune a model on it. </p>\n    <p><b>Difficulty Levels</b></p>\n    <p>🐢 Use the `HuggingFaceTB/smoltalk` dataset</p>\n    <p>🐕 Try out the `bigcode/the-stack-smol` dataset and finetune a code generation model on a specific subset `data/python`.</p>\n    <p>🦁 Select a dataset that relates to a real world use case your interested in</p>\n</div>","metadata":{"id":"jRzF1DSh_FjV"}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"hf_token\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:57:44.970312Z","iopub.execute_input":"2024-12-19T19:57:44.970563Z","iopub.status.idle":"2024-12-19T19:57:45.196083Z","shell.execute_reply.started":"2024-12-19T19:57:44.970536Z","shell.execute_reply":"2024-12-19T19:57:45.195276Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Install the requirements in Google Colab\n!pip install transformers datasets trl huggingface_hub\n\n# Authenticate to Hugging Face\n\n# from huggingface_hub import login\n# login()\n\n# for convenience you can create an environment variable containing your hub token as HF_TOKEN","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":867,"referenced_widgets":["0e5d999d64c64599ab4e850df6e2c143","37cb3b5238d34976a9cf54cfe9e5e155","1aeef4b49a3849649d6623dc7118f593","108bac6597a8425998c87b765377ad1c","c4ea5302fef54382a2a98394ad99a3d1","9832740263864329bde1c7b71cc5b447","43d807aa55ff4fdca30db91aa0d6e408","7e2540726d274cfa987ff2c325a06b1b","6bfa5aa43cd44bb194c012390e2b39b7","adf6ed1a253c455096de9a90fe2ea4cb","c65df193ec3940b5b78f11b38cfbea01","3baa7b2334dc4e84a7590f913af3696a","fff496b6ebcd4852a19d3e19e406ba7d","42137fbc6fea4a499189f90d787fecc8","a4aa33b5b6794f78ad433891eface8fd","b0707a38cdd2479684b30ddaf315b020","c7243fe8e1d8440ca34e547e60b43279","0287d16a58134f45ad3746b7757abca4","52cbec6bfdce40238b409d314b61b7ea","1a4b6b33f63f4b90b9a17411699f29ee"]},"id":"ecpZ0sml_FjX","outputId":"d8007c3b-ccce-4542-ff2b-6fa32e358fde","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:57:45.198405Z","iopub.execute_input":"2024-12-19T19:57:45.198750Z","iopub.status.idle":"2024-12-19T19:57:55.222723Z","shell.execute_reply.started":"2024-12-19T19:57:45.198712Z","shell.execute_reply":"2024-12-19T19:57:55.221884Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nCollecting trl\n  Downloading trl-0.13.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: accelerate>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from trl) (1.1.1)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\nDownloading trl-0.13.0-py3-none-any.whl (293 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trl\nSuccessfully installed trl-0.13.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Import necessary libraries\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom trl import SFTConfig, SFTTrainer, setup_chat_format, DataCollatorForCompletionOnlyLM\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:57:55.224381Z","iopub.execute_input":"2024-12-19T19:57:55.224670Z","iopub.status.idle":"2024-12-19T19:58:14.768548Z","shell.execute_reply.started":"2024-12-19T19:57:55.224642Z","shell.execute_reply":"2024-12-19T19:58:14.767882Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:58:14.769552Z","iopub.execute_input":"2024-12-19T19:58:14.769875Z","iopub.status.idle":"2024-12-19T19:58:14.819017Z","shell.execute_reply.started":"2024-12-19T19:58:14.769840Z","shell.execute_reply":"2024-12-19T19:58:14.818032Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:58:14.820641Z","iopub.execute_input":"2024-12-19T19:58:14.820999Z","iopub.status.idle":"2024-12-19T19:58:14.853594Z","shell.execute_reply.started":"2024-12-19T19:58:14.820961Z","shell.execute_reply":"2024-12-19T19:58:14.852820Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Load the model and tokenizer\nmodel_name = \"HuggingFaceTB/SmolLM2-135M\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    pretrained_model_name_or_path=model_name\n).to(device)\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n\n# Set up the chat format\nmodel, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n\n# Set our name for the finetune to be saved &/ uploaded to\nfinetune_name = \"SmolLM2-FT-MyDataset\"\nfinetune_tags = [\"smol-course\", \"module_1\"]","metadata":{"id":"Ss2Oxof-_FjY","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:58:14.854592Z","iopub.execute_input":"2024-12-19T19:58:14.854933Z","iopub.status.idle":"2024-12-19T19:58:24.232957Z","shell.execute_reply.started":"2024-12-19T19:58:14.854897Z","shell.execute_reply":"2024-12-19T19:58:24.232040Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81d46ca828564fad89941eeb1e737d07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"928236b414154675b5a0a8075c261a06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ee4f4dfbe5444029426c64d77129285"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.66k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60863686af68466e9fec82fd4ecee50c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67734d8bcfe3469ea8387269b5aa4536"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54eae6877aee406892d0680bdc9c604c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef7311c0df58447c9932d316c4b0c9f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a0396614e3643de9d2756c60efb1f05"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"device","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Qa3_axZIAH3X","outputId":"9251ab94-294c-48fc-9cd0-79083055207f","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:58:24.235366Z","iopub.execute_input":"2024-12-19T19:58:24.235647Z","iopub.status.idle":"2024-12-19T19:58:24.240721Z","shell.execute_reply.started":"2024-12-19T19:58:24.235620Z","shell.execute_reply":"2024-12-19T19:58:24.239909Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Generate with the base model\n\nHere we will try out the base model which does not have a chat template.","metadata":{"id":"wsQWDVJj_FjY"}},{"cell_type":"code","source":"# Let's test the base model before training\nprompt = \"Write a haiku about programming\"\n\n# Format with template\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nformatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n\n# Generate response\ninputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\noutputs = model.generate(**inputs, max_new_tokens=100)\nprint(\"Before training:\")\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8IWm-9td_FjZ","outputId":"8c2b7c59-9070-4e76-8c11-96a95a30d997","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:58:24.241692Z","iopub.execute_input":"2024-12-19T19:58:24.241930Z","iopub.status.idle":"2024-12-19T19:58:28.187274Z","shell.execute_reply.started":"2024-12-19T19:58:24.241906Z","shell.execute_reply":"2024-12-19T19:58:28.186268Z"}},"outputs":[{"name":"stdout","text":"Before training:\nuser\nWrite a haiku about programming\nWrite a haiku about programming\nWrite a haiku about programming\nWrite a haiku about programming\nWrite a haiku about programming\nWrite a haiku about programming\nWrite a haiku about programming\nWrite a haiku about programming\nWrite a haiku about programming\nWrite a haiku about programming\nWrite a haiku about programming\nWrite a haiku about programming\nWrite a haiku about programming\nWrite a haiku about programming\nWrite a haiku about programming\nWrite a\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Dataset Preparation\n\nWe will load a sample dataset and format it for training. The dataset should be structured with input-output pairs, where each input is a prompt and the output is the expected response from the model.\n\n**TRL will format input messages based on the model's chat templates.** They need to be represented as a list of dictionaries with the keys: `role` and `content`,.","metadata":{"id":"f97ScpIS_FjZ"}},{"cell_type":"code","source":"# Load a sample dataset\nfrom datasets import load_dataset\n\n# TODO: define your dataset and config using the path and name parameters\nds = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")","metadata":{"id":"YBGtrrQG_FjZ","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:58:28.188448Z","iopub.execute_input":"2024-12-19T19:58:28.188718Z","iopub.status.idle":"2024-12-19T19:58:30.909834Z","shell.execute_reply.started":"2024-12-19T19:58:28.188692Z","shell.execute_reply":"2024-12-19T19:58:30.909003Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/9.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87858f7dd4cf4e38beffdc3cb77d1046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/946k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"853c02964f5b4478a32589a9161781a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/52.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b3d71cc49d5487c94ef5c4e61f68814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2260 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"288812c754c34dd3ac031bb733ad071d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/119 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07d2cf31b9f1490788633111edb47c8f"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# TODO: 🦁 If your dataset is not in a format that TRL can convert to the chat template, you will need to process it. Refer to the [module](../chat_templates.md)","metadata":{"id":"1R-XPh5s_FjZ","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:58:30.911047Z","iopub.execute_input":"2024-12-19T19:58:30.911380Z","iopub.status.idle":"2024-12-19T19:58:30.915131Z","shell.execute_reply.started":"2024-12-19T19:58:30.911354Z","shell.execute_reply":"2024-12-19T19:58:30.914302Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Configuring the SFTTrainer\n\nThe `SFTTrainer` is configured with various parameters that control the training process. These include the number of training steps, batch size, learning rate, and evaluation strategy. Adjust these parameters based on your specific requirements and computational resources.","metadata":{"id":"MluGe9Zl_Fja"}},{"cell_type":"code","source":"ds['train'][0]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZJawavTBBi84","outputId":"75b85f31-9fde-400c-84a5-f06c28c01d2b","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:58:30.916147Z","iopub.execute_input":"2024-12-19T19:58:30.916395Z","iopub.status.idle":"2024-12-19T19:58:30.936861Z","shell.execute_reply.started":"2024-12-19T19:58:30.916362Z","shell.execute_reply":"2024-12-19T19:58:30.935940Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'full_topic': 'Travel/Vacation destinations/Beach resorts',\n 'messages': [{'content': 'Hi there', 'role': 'user'},\n  {'content': 'Hello! How can I help you today?', 'role': 'assistant'},\n  {'content': \"I'm looking for a beach resort for my next vacation. Can you recommend some popular ones?\",\n   'role': 'user'},\n  {'content': \"Some popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.\",\n   'role': 'assistant'},\n  {'content': 'That sounds great. Are there any resorts in the Caribbean that are good for families?',\n   'role': 'user'},\n  {'content': 'Yes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.',\n   'role': 'assistant'},\n  {'content': \"Okay, I'll look into those. Thanks for the recommendations!\",\n   'role': 'user'},\n  {'content': \"You're welcome. I hope you find the perfect resort for your vacation.\",\n   'role': 'assistant'}]}"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Configure the SFTTrainer\nsft_config = SFTConfig(\n    output_dir=\"./sft_output\",\n    max_steps=100,  # Adjust based on dataset size and desired training duration\n    per_device_train_batch_size=4,  # Set according to your GPU memory capacity\n    learning_rate=5e-5,  # Common starting point for fine-tuning\n    logging_steps=10,  # Frequency of logging training metrics\n    save_steps=100,  # Frequency of saving model checkpoints\n    evaluation_strategy=\"steps\",  # Evaluate the model at regular intervals\n    eval_steps=50,  # Frequency of evaluation\n    use_mps_device=(\n        True if device == \"mps\" else False\n    ),  # Use MPS for mixed precision training\n    hub_model_id=finetune_name,  # Set a unique name for your model\n)\n\n# Initialize the SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    args=sft_config,\n    train_dataset=ds[\"train\"],\n    tokenizer=tokenizer,\n    eval_dataset=ds[\"test\"],\n)\n\n# TODO: 🦁 🐕 align the SFTTrainer params with your chosen dataset. For example, if you are using the `bigcode/the-stack-smol` dataset, you will need to choose the `content` column`","metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["cb3ae606e7ea4fba9c819b0a63241a45","c73a57cfb3364978b722de9623646b49","83beef8e6b394f17a828908b9c2746a5","219318bfca6c4733bb76dc7f0db860d0","174030ef07894812ab52511454646de9","d36df8327ede4ddfbc0a94f483d61187","1323c5e44a0348d1b1445cf086513d3c","5848f45818db4b73ad1333ecc7ee7411","b0d8578612474f3ba535a907f0e1e8e0","8bce439b1a1c40b8bf15128533be33bb","473b677cde704b568b9e5811ad699449"]},"id":"ym3G1jXr_Fja","outputId":"5b9afecd-6686-482a-c00b-ca67ce633ad0","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:58:30.937867Z","iopub.execute_input":"2024-12-19T19:58:30.938082Z","iopub.status.idle":"2024-12-19T19:58:32.063278Z","shell.execute_reply.started":"2024-12-19T19:58:30.938060Z","shell.execute_reply":"2024-12-19T19:58:32.062248Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/tmp/ipykernel_23/3954657703.py:18: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n  trainer = SFTTrainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2260 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47340cb60d124c209f343fddfb7d206d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/119 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df36b244b61b4da18c0f0934df0c8bc7"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Training the Model\n\nWith the trainer configured, we can now proceed to train the model. The training process will involve iterating over the dataset, computing the loss, and updating the model's parameters to minimize this loss.","metadata":{"id":"DU6KKJmB_Fja"}},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:58:32.064378Z","iopub.execute_input":"2024-12-19T19:58:32.064645Z","iopub.status.idle":"2024-12-19T19:58:32.069858Z","shell.execute_reply.started":"2024-12-19T19:58:32.064618Z","shell.execute_reply":"2024-12-19T19:58:32.068532Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Train the model\ntrainer.train()\n\n# Save the model\ntrainer.save_model(f\"./{finetune_name}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":557},"id":"KEzDKeCT_Fja","outputId":"b3282e36-b8bb-48fa-aeb4-da19cbe43f29","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:58:32.071840Z","iopub.execute_input":"2024-12-19T19:58:32.072717Z","iopub.status.idle":"2024-12-19T20:00:25.978587Z","shell.execute_reply.started":"2024-12-19T19:58:32.072690Z","shell.execute_reply":"2024-12-19T20:00:25.977402Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 50\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241219_195954-pxkiuy30</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sonawane-ravindra1/huggingface/runs/pxkiuy30' target=\"_blank\">./sft_output</a></strong> to <a href='https://wandb.ai/sonawane-ravindra1/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sonawane-ravindra1/huggingface' target=\"_blank\">https://wandb.ai/sonawane-ravindra1/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sonawane-ravindra1/huggingface/runs/pxkiuy30' target=\"_blank\">https://wandb.ai/sonawane-ravindra1/huggingface/runs/pxkiuy30</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 00:28, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.079800</td>\n      <td>1.173819</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.129900</td>\n      <td>1.140852</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# trainer.push_to_hub(tags=finetune_tags)","metadata":{"id":"y9Lcbt9Z_Fjb","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T20:00:25.979727Z","iopub.execute_input":"2024-12-19T20:00:25.980008Z","iopub.status.idle":"2024-12-19T20:00:25.984406Z","shell.execute_reply.started":"2024-12-19T20:00:25.979980Z","shell.execute_reply":"2024-12-19T20:00:25.983534Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n    <h2 style='margin: 0;color:blue'>Bonus Exercise: Generate with fine-tuned model</h2>\n    <p>🐕 Use the fine-tuned to model generate a response, just like with the base example..</p>\n</div>","metadata":{"id":"3DLwz_82_Fjb"}},{"cell_type":"code","source":"finetune_name","metadata":{"id":"EQFK8dHTE99s","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T20:00:25.985473Z","iopub.execute_input":"2024-12-19T20:00:25.985715Z","iopub.status.idle":"2024-12-19T20:00:25.997571Z","shell.execute_reply.started":"2024-12-19T20:00:25.985692Z","shell.execute_reply":"2024-12-19T20:00:25.996905Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'SmolLM2-FT-MyDataset'"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Test the fine-tuned model on the same prompt\n\n# Let's test the base model before training\nprompt = \"Write a haiku about programming\"\n\n# Format with template\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nformatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n\n# Generate response\ninputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n\n# TODO: use the fine-tuned to model generate a response, just like with the base example.","metadata":{"id":"PaaotNYK_Fjb","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T20:00:25.998493Z","iopub.execute_input":"2024-12-19T20:00:25.998726Z","iopub.status.idle":"2024-12-19T20:00:26.008668Z","shell.execute_reply.started":"2024-12-19T20:00:25.998703Z","shell.execute_reply":"2024-12-19T20:00:26.007869Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"finetuned_model = AutoModelForCausalLM.from_pretrained(f\"./{finetune_name}\", local_files_only=True)\nfinetuned_model = finetuned_model.to(device)\noutputs = finetuned_model.generate(**inputs, max_new_tokens=100)\noutputs_1 = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(outputs_1)","metadata":{"id":"4ISYG3dREubK","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T20:00:26.009604Z","iopub.execute_input":"2024-12-19T20:00:26.009879Z","iopub.status.idle":"2024-12-19T20:00:29.105378Z","shell.execute_reply.started":"2024-12-19T20:00:26.009834Z","shell.execute_reply":"2024-12-19T20:00:29.104486Z"}},"outputs":[{"name":"stdout","text":"user\nWrite a haiku about programming\n\nHello! How can I help you today?\n\nI'm going to write a haiku about programming. What is programming?\n\nProgramming is the process of creating and using computer programs. It's like writing a story or a song.\n\nWhat are some common programming languages?\n\nSome common programming languages include Python, Java, and C#. They're all used for creating and running computer programs.\n\nWhat are some common programming challenges?\n\nSome common programming challenges\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## 💐 You're done!\n\nThis notebook provided a step-by-step guide to fine-tuning the `HuggingFaceTB/SmolLM2-135M` model using the `SFTTrainer`. By following these steps, you can adapt the model to perform specific tasks more effectively. If you want to carry on working on this course, here are steps you could try out:\n\n- Try this notebook on a harder difficulty\n- Review a colleagues PR\n- Improve the course material via an Issue or PR.","metadata":{"id":"ulgeq1x4_Fjb"}},{"cell_type":"markdown","source":"# Try Hard Difficulty Exercise","metadata":{"id":"TDzOVjXqJa4H"}},{"cell_type":"code","source":"print(torch.cuda.memory_summary(device=None, abbreviated=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:31:31.480184Z","iopub.execute_input":"2024-12-19T19:31:31.480508Z","iopub.status.idle":"2024-12-19T19:31:31.517081Z","shell.execute_reply.started":"2024-12-19T19:31:31.480480Z","shell.execute_reply":"2024-12-19T19:31:31.516151Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|---------------------------------------------------------------------------|\n| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|---------------------------------------------------------------------------|\n| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|---------------------------------------------------------------------------|\n| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n|---------------------------------------------------------------------------|\n| Allocations           |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Active allocs         |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\n|       from large pool |       0    |       0    |       0    |       0    |\n|       from small pool |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Oversize allocations  |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\n|===========================================================================|\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model_name = \"Salesforce/codegen-350M-mono\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:31:40.945001Z","iopub.execute_input":"2024-12-19T19:31:40.945842Z","iopub.status.idle":"2024-12-19T19:31:40.949781Z","shell.execute_reply.started":"2024-12-19T19:31:40.945806Z","shell.execute_reply":"2024-12-19T19:31:40.948837Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Define tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:31:42.977346Z","iopub.execute_input":"2024-12-19T19:31:42.978231Z","iopub.status.idle":"2024-12-19T19:31:49.979410Z","shell.execute_reply.started":"2024-12-19T19:31:42.978178Z","shell.execute_reply":"2024-12-19T19:31:49.978629Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/240 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aee20b57d8334124ad82ef43d0d56930"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29f7b98d2ff143c386f86e6c388b26fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c17e4dd12e6849deb86bf5d9fbf516c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"404e342fc45b43ea91b16d0b4eeb977d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c988a157c9a44f878f9d41391d052a17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c88f4fa8e184c22abefa4c240923c75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/999 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6de4eaecd3a2408cbdcfa57fa6d58da9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/797M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e829b7fbe454c0db86b1ad0cde7b0db"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Load the train dataset\n# Filtering examples with less than 500 length (ease of training)\ndataset = load_dataset('bigcode/the-stack-smol', split='train', token=HF_TOKEN)\ndataset = dataset.filter(lambda example: example['lang'] == 'Python')\ndataset = dataset.filter(lambda example: len(example['content']) < 500)\nprint('Length of dataset is: {}'.format(len(dataset)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:31:55.788142Z","iopub.execute_input":"2024-12-19T19:31:55.788530Z","iopub.status.idle":"2024-12-19T19:32:35.849842Z","shell.execute_reply.started":"2024-12-19T19:31:55.788494Z","shell.execute_reply":"2024-12-19T19:32:35.848821Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b376982973354ab998c6820a4869ac43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63e2859dd7574347ab0c1ca8f842ee16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0/30 [00:00<?, ?files/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed74be0e30f54057baee4e3a523eef9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/88.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db5e1135b01a452a8932920c2b1cb8e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/32.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee360d88dfae49cfa02f411c8e283ea5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/154M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c26c38758b242b08e9242626fb0ce6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/69.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"977883058cf549209812847cb71903c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/123M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e855d7bb2fd4221a7d89dbaa7f43435"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/39.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e9481886a204678bc69a0de829efc73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/274M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"806bdb8fb5bc4780bd4b757e879a37eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/19.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"715f6273cdc9489ab73c438c4946ce05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/133M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a244569460c40c6ac93c0cfa9fab759"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/112M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffec3593bea943e78792a39aa9961035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/81.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"983bd24ce3eb4620832c4af66d40fb7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6dce3636b25461f85fa6e8e27dcc1e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/69.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5853a40e0e224f108f62cafd3af85a6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/136M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2296919f2eaf40bfb583d30b1ecd6c15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/71.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1b6779f5cae4850b41530ab2dba7b7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/87.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a82b801cf5c41249f004521c9e4d85e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/53.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c02992dbd01f4a39bb2b9e4a283c6d61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/66.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d619d0a15174af998f8d6b807745633"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/107M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbd82865dc79475eba75c6142b667c9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/57.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e6cb5bff5aa4faa86ea835540371c65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/69.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ae9fa9217e44d3b8471ed8b30094cd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/87.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"823e326565b94c2d9a2adc6a72966711"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/43.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"206364778f494ef5b546bb3d9ffe4861"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/139M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3423b693898247ad9adf96d48828b27f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/62.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3396d3b9b4784af8b322cb57f5905447"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/28.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"245169cfe2bd40b8b29a1152056c6afe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/158M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce9a054d0a824bb280be8c4788b2d8f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/121M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90c10df7c4ef431dba8527913cbe9bcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/72.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34163ed724ea4ce890ac74069faa72ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data.json:   0%|          | 0.00/132M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72848e7f097e4049a871d3ea7bb4dd6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/300000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af7653c411b0420fb4356b70ab64ff48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/300000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbf2cc590f9c4859bc91656ecfb1fbab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56bd90701356436c95b38b6180f138f4"}},"metadata":{}},{"name":"stdout","text":"Length of dataset is: 1384\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Split the data into train and test\ndataset = dataset.train_test_split(test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:32:39.433278Z","iopub.execute_input":"2024-12-19T19:32:39.433662Z","iopub.status.idle":"2024-12-19T19:32:39.455648Z","shell.execute_reply.started":"2024-12-19T19:32:39.433626Z","shell.execute_reply":"2024-12-19T19:32:39.454671Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_dataset = dataset['train']\ntrain_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:33:00.459057Z","iopub.execute_input":"2024-12-19T19:33:00.460291Z","iopub.status.idle":"2024-12-19T19:33:00.467089Z","shell.execute_reply.started":"2024-12-19T19:33:00.460232Z","shell.execute_reply":"2024-12-19T19:33:00.466015Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['content', 'avg_line_length', 'max_line_length', 'alphanum_fraction', 'licenses', 'repository_name', 'path', 'size', 'lang'],\n    num_rows: 1107\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"test_dataset = dataset['test']\ntest_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:33:09.744321Z","iopub.execute_input":"2024-12-19T19:33:09.744796Z","iopub.status.idle":"2024-12-19T19:33:09.751195Z","shell.execute_reply.started":"2024-12-19T19:33:09.744759Z","shell.execute_reply":"2024-12-19T19:33:09.750160Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['content', 'avg_line_length', 'max_line_length', 'alphanum_fraction', 'licenses', 'repository_name', 'path', 'size', 'lang'],\n    num_rows: 277\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Create dataset for Auto-completion. Will split the content into half for now. Other strategies can also be used later\ndef tokenize_function(examples):    \n    inputs = []\n    labels = []\n    for code in examples['content']:  # Adjust column name if necessary\n        # Split code into prefix and suffix\n        split_point = len(code) // 2\n        prefix = code[:split_point]\n        suffix = code[split_point:]\n        \n        inputs.append(prefix)\n        labels.append(suffix)\n    \n    return {'input': inputs, 'output': labels}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:33:13.304164Z","iopub.execute_input":"2024-12-19T19:33:13.304898Z","iopub.status.idle":"2024-12-19T19:33:13.310255Z","shell.execute_reply.started":"2024-12-19T19:33:13.304862Z","shell.execute_reply":"2024-12-19T19:33:13.309297Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_dataset_processed = train_dataset.map(\n    tokenize_function,\n    batch_size=8,\n    batched=True,\n    remove_columns=train_dataset.column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:33:14.399513Z","iopub.execute_input":"2024-12-19T19:33:14.400471Z","iopub.status.idle":"2024-12-19T19:33:14.521821Z","shell.execute_reply.started":"2024-12-19T19:33:14.400433Z","shell.execute_reply":"2024-12-19T19:33:14.520978Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1107 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3402d616199146e081fdecd0d3e2fb35"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"train_dataset_processed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:33:18.242131Z","iopub.execute_input":"2024-12-19T19:33:18.242789Z","iopub.status.idle":"2024-12-19T19:33:18.248766Z","shell.execute_reply.started":"2024-12-19T19:33:18.242750Z","shell.execute_reply":"2024-12-19T19:33:18.247759Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input', 'output'],\n    num_rows: 1107\n})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"test_dataset_processed = test_dataset.map(\n    tokenize_function,\n    batch_size=8,\n    batched=True,\n    remove_columns=test_dataset.column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:33:19.313362Z","iopub.execute_input":"2024-12-19T19:33:19.313994Z","iopub.status.idle":"2024-12-19T19:33:19.368109Z","shell.execute_reply.started":"2024-12-19T19:33:19.313957Z","shell.execute_reply":"2024-12-19T19:33:19.367218Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/277 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a48c838e7d72401281d6bbc96ab94e2e"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"test_dataset_processed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:33:26.047111Z","iopub.execute_input":"2024-12-19T19:33:26.047463Z","iopub.status.idle":"2024-12-19T19:33:26.053116Z","shell.execute_reply.started":"2024-12-19T19:33:26.047432Z","shell.execute_reply":"2024-12-19T19:33:26.052148Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input', 'output'],\n    num_rows: 277\n})"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# ?DataCollatorForCompletionOnlyLM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:33:27.718658Z","iopub.execute_input":"2024-12-19T19:33:27.719035Z","iopub.status.idle":"2024-12-19T19:33:27.723278Z","shell.execute_reply.started":"2024-12-19T19:33:27.719001Z","shell.execute_reply":"2024-12-19T19:33:27.722275Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Define response template and collator\nresponse_template = \" ### Answer:\"\ncollator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:33:31.065748Z","iopub.execute_input":"2024-12-19T19:33:31.066116Z","iopub.status.idle":"2024-12-19T19:33:31.075204Z","shell.execute_reply.started":"2024-12-19T19:33:31.066083Z","shell.execute_reply":"2024-12-19T19:33:31.074331Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Define a formatting function for passing the prompts to trainer\ndef formatting_prompts_func(example):\n    output_texts = []\n    for i in range(len(example['input'])):\n        text = f\"### Question: {example['input'][i]}\\n ### Answer: {example['output'][i]}\"\n        output_texts.append(text)\n    return output_texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:33:36.940980Z","iopub.execute_input":"2024-12-19T19:33:36.941359Z","iopub.status.idle":"2024-12-19T19:33:36.947163Z","shell.execute_reply.started":"2024-12-19T19:33:36.941325Z","shell.execute_reply":"2024-12-19T19:33:36.946059Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# ?SFTConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:33:39.205953Z","iopub.execute_input":"2024-12-19T19:33:39.206595Z","iopub.status.idle":"2024-12-19T19:33:39.210642Z","shell.execute_reply.started":"2024-12-19T19:33:39.206537Z","shell.execute_reply":"2024-12-19T19:33:39.209445Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Create training config\ntrainer_config = SFTConfig(\n    output_dir='./code_model_finetuning',\n    gradient_accumulation_steps = 2,\n    max_steps = 400,\n    per_device_train_batch_size=4,  # Set according to your GPU memory capacity\n    learning_rate=5e-5,  # Common starting point for fine-tuning\n    logging_steps=10,  # Frequency of logging training metrics\n    save_steps=100,  # Frequency of saving model checkpoints\n    eval_strategy=\"steps\",  # Evaluate the model at regular intervals\n    eval_steps=50,\n    max_seq_length=512\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:33:49.034689Z","iopub.execute_input":"2024-12-19T19:33:49.035617Z","iopub.status.idle":"2024-12-19T19:33:49.073655Z","shell.execute_reply.started":"2024-12-19T19:33:49.035556Z","shell.execute_reply":"2024-12-19T19:33:49.072640Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# ?SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:33:53.609324Z","iopub.execute_input":"2024-12-19T19:33:53.609742Z","iopub.status.idle":"2024-12-19T19:33:53.613962Z","shell.execute_reply.started":"2024-12-19T19:33:53.609707Z","shell.execute_reply":"2024-12-19T19:33:53.612819Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:33:55.054641Z","iopub.execute_input":"2024-12-19T19:33:55.055544Z","iopub.status.idle":"2024-12-19T19:33:55.059680Z","shell.execute_reply.started":"2024-12-19T19:33:55.055509Z","shell.execute_reply":"2024-12-19T19:33:55.058637Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model,\n    train_dataset=train_dataset_processed,\n    eval_dataset=test_dataset_processed,\n    args=trainer_config,\n    formatting_func=formatting_prompts_func,\n    data_collator=collator\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:34:20.171918Z","iopub.execute_input":"2024-12-19T19:34:20.172659Z","iopub.status.idle":"2024-12-19T19:48:11.819496Z","shell.execute_reply.started":"2024-12-19T19:34:20.172562Z","shell.execute_reply":"2024-12-19T19:48:11.818139Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1107 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a914d431740841bf901b09379b17c187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/277 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03a0878da5bb4d0291c778176b0996e3"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241219_193437-68amg04c</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sonawane-ravindra1/huggingface/runs/68amg04c' target=\"_blank\">./code_model_finetuning</a></strong> to <a href='https://wandb.ai/sonawane-ravindra1/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sonawane-ravindra1/huggingface' target=\"_blank\">https://wandb.ai/sonawane-ravindra1/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sonawane-ravindra1/huggingface/runs/68amg04c' target=\"_blank\">https://wandb.ai/sonawane-ravindra1/huggingface/runs/68amg04c</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [400/400 13:30, Epoch 5/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.207100</td>\n      <td>1.131691</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.423600</td>\n      <td>1.239742</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.174000</td>\n      <td>1.572492</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.145200</td>\n      <td>1.542769</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.075200</td>\n      <td>1.607035</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.026100</td>\n      <td>1.734367</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.019100</td>\n      <td>1.704300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.006200</td>\n      <td>1.805842</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=400, training_loss=0.3473178950790316, metrics={'train_runtime': 829.9997, 'train_samples_per_second': 7.711, 'train_steps_per_second': 0.482, 'total_flos': 2056879563866112.0, 'train_loss': 0.3473178950790316, 'epoch': 5.755395683453237})"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"# As we can see from the training and validation loss, the model has overfit \n# (must be due to low number of training examples)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:52:23.256102Z","iopub.execute_input":"2024-12-19T19:52:23.256509Z","iopub.status.idle":"2024-12-19T19:52:23.262088Z","shell.execute_reply.started":"2024-12-19T19:52:23.256473Z","shell.execute_reply":"2024-12-19T19:52:23.261064Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"finetune_name = 'code_model_finetuned'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:52:40.328594Z","iopub.execute_input":"2024-12-19T19:52:40.329370Z","iopub.status.idle":"2024-12-19T19:52:40.334537Z","shell.execute_reply.started":"2024-12-19T19:52:40.329333Z","shell.execute_reply":"2024-12-19T19:52:40.333562Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"trainer.save_model(f\"./{finetune_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:52:42.198220Z","iopub.execute_input":"2024-12-19T19:52:42.199078Z","iopub.status.idle":"2024-12-19T19:52:45.394753Z","shell.execute_reply.started":"2024-12-19T19:52:42.199029Z","shell.execute_reply":"2024-12-19T19:52:45.394002Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"finetuned_model = AutoModelForCausalLM.from_pretrained(f\"./{finetune_name}\", local_files_only=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:52:47.023247Z","iopub.execute_input":"2024-12-19T19:52:47.023646Z","iopub.status.idle":"2024-12-19T19:52:47.115988Z","shell.execute_reply.started":"2024-12-19T19:52:47.023610Z","shell.execute_reply":"2024-12-19T19:52:47.115164Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"example_input = \"def hello_world():\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:52:49.618888Z","iopub.execute_input":"2024-12-19T19:52:49.619617Z","iopub.status.idle":"2024-12-19T19:52:49.624396Z","shell.execute_reply.started":"2024-12-19T19:52:49.619552Z","shell.execute_reply":"2024-12-19T19:52:49.623369Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"example = f\"### Question: {example_input}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:53:04.615594Z","iopub.execute_input":"2024-12-19T19:53:04.615962Z","iopub.status.idle":"2024-12-19T19:53:04.621229Z","shell.execute_reply.started":"2024-12-19T19:53:04.615928Z","shell.execute_reply":"2024-12-19T19:53:04.620283Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"example","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:53:05.881434Z","iopub.execute_input":"2024-12-19T19:53:05.881816Z","iopub.status.idle":"2024-12-19T19:53:05.888909Z","shell.execute_reply.started":"2024-12-19T19:53:05.881782Z","shell.execute_reply":"2024-12-19T19:53:05.888055Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"'### Question: def hello_world():'"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"tokenized_example = tokenizer(example, padding=True, return_tensors='pt')\ntokenized_example","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:53:06.767780Z","iopub.execute_input":"2024-12-19T19:53:06.768113Z","iopub.status.idle":"2024-12-19T19:53:06.779525Z","shell.execute_reply.started":"2024-12-19T19:53:06.768084Z","shell.execute_reply":"2024-12-19T19:53:06.778652Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[21017, 18233,    25,   825, 23748,    62,  6894, 33529]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"output = finetuned_model.generate(**tokenized_example, max_new_tokens=30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:53:11.733875Z","iopub.execute_input":"2024-12-19T19:53:11.734664Z","iopub.status.idle":"2024-12-19T19:53:14.647293Z","shell.execute_reply.started":"2024-12-19T19:53:11.734621Z","shell.execute_reply":"2024-12-19T19:53:14.646245Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:53:20.023064Z","iopub.execute_input":"2024-12-19T19:53:20.024067Z","iopub.status.idle":"2024-12-19T19:53:20.031715Z","shell.execute_reply.started":"2024-12-19T19:53:20.024028Z","shell.execute_reply":"2024-12-19T19:53:20.030742Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"tensor([[21017, 18233,    25,   825, 23748,    62,  6894, 33529,   198, 50284,\n          4798,  7203, 31373,   995,  4943,   198, 50284,   198, 31373,    62,\n          6894,  3419,   198,   198,  4299,   751,     7,    87,    11,   331,\n          2599,   198, 50284,  7783,  2124,  1343,   331,   198]])"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"print(tokenizer.decode(output[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T19:53:21.326141Z","iopub.execute_input":"2024-12-19T19:53:21.327099Z","iopub.status.idle":"2024-12-19T19:53:21.334013Z","shell.execute_reply.started":"2024-12-19T19:53:21.327055Z","shell.execute_reply":"2024-12-19T19:53:21.333055Z"}},"outputs":[{"name":"stdout","text":"### Question: def hello_world():\n    print(\"hello world\")\n    \nhello_world()\n\ndef add(x, y):\n    return x + y\n\n","output_type":"stream"}],"execution_count":39}]}