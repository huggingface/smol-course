# Advanced RAG Strategies

Advanced Retrieval-Augmented Generation (RAG) techniques address the challenges faced by naive RAG. These strategies enhance the process of document retrieval and improve the quality of answers generated by large language models (LLMs). There are multiple possible optimizations in each step of the pipeline for an advanced RAG, but particularly retrieval stage is the focus.

The strategies are thus divided into pre-retrieval, retrieval, and post-retrieval to address the following challenges.

- **How to achieve accurate semantic representations of documents and queries?**
- **What methods align the semantic spaces of queries and documents (chunks)?**
- **How to align the retriever’s output with the preferences of the LLM?**


## Pre-Retrieval Strategies

Efficient data indexing is essential for improving the retrieval performance in a RAG system. Key pre-retrieval strategies include:

- **Improve Data Quality**: Remove irrelevant information, resolve ambiguity in entities and terms, confirm factual accuracy, maintain context, and update outdated information.
- **Optimize Index Structure**: Adjust chunk sizes to capture relevant context or incorporate graph structures to represent relationships between entities.
- **Add Metadata**: Enhance data filtering by adding relevant metadata such as dates, chapters, subsections, and purposes to document chunks.
- **Chunk Optimization**: Fine-tune chunk size to balance between too large or too small chunks to improve the embedding process.

### Key Pre-Retrieval Techniques:

- **Sliding Window**: Chunking method with overlap between text blocks.
- **Auto-Merging Retrieval**: Starts with small text blocks and later provides larger, related text blocks for the LLM.
- **Abstract Embedding**: Focuses on Top-K retrieval based on document abstracts for a comprehensive document context.
- **Metadata Filtering**: Leverages document metadata for enhanced filtering.
- **Graph Indexing**: Converts entities and relationships into nodes and connections to improve relevance.

## Retrieval Strategies

During the retrieval phase, the goal is to identify the most relevant document chunks to the query. This requires optimizing the embedding models used to represent both the query and chunks.

- **Domain Knowledge Fine-Tuning**: Fine-tune embedding models using domain-specific datasets to capture the unique aspects of the RAG system.
- **Similarity Metrics**: Select an appropriate metric to measure the similarity between the query and chunk embeddings. Common metrics include:
  - Cosine Similarity
  - Euclidean Distance (L2)
  - Dot Product
  - L2 Squared Distance
  - Manhattan Distance

Several vector databases support multiple similarity metrics, allowing further customization/optimization.

## Post-Retrieval Strategies

After retrieving context data (chunks) from a vector database, the next step is to process this information and pass it to the LLM. However, some retrieved chunks may be irrelevant, noisy, or repeated, impacting the LLM’s ability to generate accurate answers.

### Strategies to Address Post-Retrieval Issues:

- **Reranking**: Prioritize the most relevant chunks by reranking the retrieved results. This ensures LLMs are given the top-K most pertinent context, reducing performance issues caused by excessive context. Available reranking techniques are offered by libraries like LlamaIndex, LangChain, and HayStack.
- **Prompt Compression**: Filter out irrelevant context and shorten the prompt before inputting it to the LLM. Techniques such as mutual information or perplexity estimation, along with summarization, help in reducing context length and noise.

⏩ Try the [Improved RAG Tutorial](./notebooks/improved_rag_haystack_example.ipynb) to implement improved RAG pipelines.

## **Resources**