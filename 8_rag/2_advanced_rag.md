# Advanced RAG Strategies

Advanced Retrieval-Augmented Generation (RAG) techniques address the challenges faced by naive RAG. These strategies enhance the process of document retrieval and improve the quality of answers generated by large language models (LLMs). There are multiple possible optimizations in each step of the pipeline for an advanced RAG, but particularly retrieval stage is the focus.

The strategies are thus divided into pre-retrieval, retrieval, and post-retrieval to address the following challenges.

- **How to achieve accurate semantic representations of documents and queries?**
- **What methods align the semantic spaces of queries and documents (chunks)?**
- **How to align the retriever’s output with the preferences of the LLM?**


## Pre-Retrieval Strategies

Efficient data indexing is essential for improving the retrieval performance in a RAG system. Key pre-retrieval strategies include:

- **Improve Data Quality**: Remove irrelevant information, resolve ambiguity in entities and terms, confirm factual accuracy, maintain context, and update outdated information.
- **Optimize Index Structure**: Adjust chunk sizes to capture relevant context or incorporate graph structures to represent relationships between entities.
- **Add Metadata**: Enhance data filtering by adding relevant metadata such as dates, chapters, subsections, and purposes to document chunks.
- **Chunk Optimization**: Fine-tune chunk size to balance between too large or too small chunks to improve the embedding process.

### Key Pre-Retrieval Techniques:

- **Sliding Window**: Chunking method with overlap between text blocks.
- **Auto-Merging Retrieval**: Starts with small text blocks and later provides larger, related text blocks for the LLM.
- **Abstract Embedding**: Focuses on Top-K retrieval based on document abstracts for a comprehensive document context.
- **Metadata Filtering**: Leverages document metadata for enhanced filtering.
- **Graph Indexing**: Converts entities and relationships into nodes and connections to improve relevance.

## Retrieval Strategies

During the retrieval phase, the goal is to identify the most relevant document chunks to the query. This requires optimizing the embedding models used to represent both the query and chunks.

- **Domain Knowledge Fine-Tuning**: Fine-tune embedding models using domain-specific datasets to capture the unique aspects of the RAG system.
- **Similarity Metrics**: Select an appropriate metric to measure the similarity between the query and chunk embeddings. Common metrics include:
  - Cosine Similarity
  - Euclidean Distance (L2)
  - Dot Product
  - L2 Squared Distance
  - Manhattan Distance

Several vector databases support multiple similarity metrics, allowing further customization/optimization.

## Post-Retrieval Strategies

After retrieving context data (chunks) from a vector database, the next step is to process this information and pass it to the LLM. However, some retrieved chunks may be irrelevant, noisy, or repeated, impacting the LLM’s ability to generate accurate answers.

### Strategies to Address Post-Retrieval Issues:

- **Reranking**: Re-rank the retrieved chunks to prioritize the most relevant ones. This is especially useful as LLMs may struggle when excessive context is introduced. Reranking identifies the top-K most relevant chunks to use as context for the LLM. Libraries such as LlamaIndex, Langchain, and HayStack offer various reranking techniques.
- **Prompt Compression**: Compress retrieved information by filtering out irrelevant context and reducing the length of the prompt before feeding it to the LLM. Use small language models to calculate mutual information or perplexity to estimate the importance of each context element. Summarization techniques can also help compress long contexts.

⏩ Try the [Improved RAG Tutorial](./notebooks/improved_rag_haystack_example.ipynb) to implement improved RAG pipelines.

## **Resources**