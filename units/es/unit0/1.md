# Bienvenido al ü§ó smol-course

![Fine-Tuning Course thumbnail](https://github.com/huggingface/smol-course/blob/main/banner.png?raw=true)

¬°Bienvenido al curso completo (y el m√°s peque√±o) sobre **el Fine-tuning de Modelos de Lenguaje**!

Este curso gratuito te llevar√° por una aventura, desde principiante hasta experto, en la comprensi√≥n, implementaci√≥n y optimizaci√≥n de t√©cnicas de fine-tuning para modelos extensos de lenguaje

Esta primera unidad te ayudar√° a incorporarte:

- Descubre el **programa del curso**.
- **Obt√©n m√°s informaci√≥n sobre el proceso de certificaci√≥n y el calendario**.
- Conoce al equipo que est√° detr√°s del curso.
- Crea tu **cuenta**.
- **Reg√≠strate en nuestro servidor de Discord** y conoce a tus compa√±eros de clase y a nosotros.

¬°Empecemos!

<Tip>

¬°Este curso es peque√±o pero intenso! Est√° dirigido a desarrolladores de software e ingenieros que desean acelerar el perfeccionamiento de sus habilidades en fine-tuning de modelos de lenguaje grande (LLM). Si no es tu caso, echa un vistazo al [Curso LLM](https://huggingface.co/learn/llm-course/).

</Tip>

## ¬øQu√© puedes esperar de este curso?

En este curso, podr√°s:

- üìñ Estudiar el ajuste por instrucciones, fine-tuning supervisado, y la alineaci√≥n de preferencias en la teor√≠a y en la pr√°ctica.
- üßë‚Äçüíª Aprender a usar frameworks y herramientas como TRL y Transformers.
- üíæ Compartir tus proyectos y explorar aplicaciones de fine-tuning creadas por la comunidad.
- üèÜ Participar en retos en los que evaluar√°s tus modelos ajustados frente a los de otros estudiantes.
- üéì Conseguir un certificado de finalizaci√≥n al completar las tareas.

Al terminar este curso, comprender√°s **c√≥mo ajustar los modelos de lenguaje de manera eficaz y crear aplicaciones de IA especializadas utilizando las √∫ltimas t√©cnicas de fine-tuning**.

¬°No olvides [**inscribirte al curso!**](https://huggingface.co/smol-course)

## ¬øC√≥mo es el curso?

El curso se compone de:

- _Unidades fundamentales_: donde aprender√°s **conceptos te√≥ricos** sobre el fine-tuning.

* _Pr√°cticas_: donde aprender√°s **a utilizar frameworks de fine-tuning establecidos** para adaptar tus modelos. Estas secciones pr√°cticas contar√°n con entornos preconfigurados.
* _Tareas de casos pr√°cticos_: donde aplicar√°s los conceptos que has aprendido para resolver un problema del mundo real que t√∫ mismo elegir√°s.
* _Colaboraciones_: colaboramos con los socios de Hugging Face para ofrecerte las √∫ltimas implementaciones y herramientas de fine-tuning.

Este **curso es un proyecto vivo, que evoluciona con tus comentarios y contribuciones.** No dudes en abrir issues y Pull Requests en GitHub, y participa en los debates en nuestro servidor de Discord.

## Cual es el plan de estudio?

Aqu√≠ est√° el **plan de estudio general del curso**. Se publicar√° una lista m√°s detallada de temas con cada unidad.

| #   | Tema                       | Descripci√≥n                                                                      | Fecha de publicaci√≥n |
| --- | -------------------------- | -------------------------------------------------------------------------------- | -------------------- |
| 1   | Ajuste por Instrucciones   | Fine-tuning supervisado, plantillas de chat, seguimiento de instrucciones        | ‚úÖ                   |
| 2   | Evaluaci√≥n                 | Puntos de referencia y evaluaci√≥n de dominios personalizados                     | Septiembre           |
| 3   | Alineaci√≥n de Preferencias | Alineaci√≥n de modelos con las preferencias humanas mediante algoritmos como DPO. | Octubre              |
| 4   | Aprendizaje por Refuerzo   | Optimizaci√≥n de modelos basados en pol√≠ticas de refuerzo.                        | Octubre              |
| 5   | Modelos de Lenguaje Visual | Adaptaci√≥n y uso de modelos multimodales                                         | Noviembre            |
| 6   | Datos Sint√©ticos           | Generaci√≥n de conjuntos de datos sint√©ticos para dominios personalizados         | Noviembre            |
| 7   | Ceremonia de Premiaci√≥n    | Presentaci√≥n de proyectos y celebraci√≥n                                          | Diciembre            |

## ¬øCu√°les son los requisitos previos?

Para poder seguir este curso, debes tener:

- Conocimientos b√°sicos sobre los conceptos de IA y LLM.
- Familiaridad con la programaci√≥n en Python y los fundamentos del machine learning.
- Experiencia con PyTorch o frameworks de aprendizaje profundo similares.
- Conocimientos b√°sicos sobre la arquitectura de los transformadores.

Si no cumples ninguno de estos requisitos, no te preocupes. Echa un vistazo al [Curso LLM](https://huggingface.co/learn/llm-course/) para empezar.

<Tip>

Los cursos anteriores no son requisitos previos en s√≠ mismos, por lo que si comprendes los conceptos de modelos extensos de lenguaje y transformadores, ¬°puedes comenzar el curso ahora mismo!

</Tip>

## ¬øQu√© herramientas necesito?

Solo necesitas dos cosas:

- _Un ordenador_ con conexi√≥n a Internet y, preferiblemente, acceso a un GPU ([Hugging Face Pro](https://huggingface.co/pro) funciona muy bien).
- Una _cuenta_: para acceder a los recursos del curso y crear proyectos. Si a√∫n no tienes una cuenta, puedes crear una [aqu√≠](https://huggingface.co/join) (es gratis).

## El proceso de certificaci√≥n

Puedes elegir entre seguir este curso _de oyente_ o realizar las actividades y _obtener uno de los dos certificados que emitimos_. Si eliges completar de oyente el curso, puedes participar en todos los retos y realizar las tareas si lo deseas, y **no es necesario que nos lo notifiques**.

El proceso de certificaci√≥n es **totalmente gratuito**:

- _Para obtener una certificaci√≥n de fundamentos_: debes completar la Unidad 1 del curso. Est√° dirigida a estudiantes que desean comprender los conceptos b√°sicos del ajuste de instrucciones sin crear aplicaciones avanzadas.
- _Para obtener un certificado de completaci√≥n_: debes completar todas las unidades del curso y enviar un proyecto final. Est√° dirigida a estudiantes que desean demostrar su dominio de las t√©cnicas de fine-tuning.

## ¬øCu√°l es el ritmo recomendado?

Cada cap√≠tulo de este curso est√° dise√±ado **para completarse en una semana, con aproximadamente 3-4 horas de trabajo semanales**.

Dado que hay una fecha l√≠mite, te proporcionamos un ritmo recomendado:

![Miniatura del curso de ajuste fino](https://github.com/huggingface/smol-course/blob/rerelease-chapter-1/schedule.png?raw=true)

## ¬øC√≥mo aprovechar al m√°ximo al curso?

Para aprovechar al m√°ximo al curso, te ofrecemos algunos consejos:

1. [√önete a grupos de estudio en Discord](https://discord.gg/UrrTSsSyjb): Estudiar en grupo siempre es m√°s f√°cil. Para ello, debes unirte a nuestro servidor de Discord y verificar tu cuenta.
2. **Realiza los cuestionarios y las tareas**: La mejor manera de aprender es mediante la pr√°ctica y la autoevaluaci√≥n.
3. **Establece un horario para mantenerte al d√≠a**: Puedes utilizar nuestro calendario recomendado a continuaci√≥n o crear el tuyo propio.

![Consejos para el curso](https://huggingface.co/datasets/mcp-course/images/resolve/main/unit0/3.png)

## Qui√©nes somos

Acerca de los autores:

### Ben Burtenshaw

Ben es ingeniero de Machine Learning en Hugging Face y se dedica a crear aplicaciones de modelos extensos de lenguaje, con enfoques de post-entrenamiento y agenciales. [Sigue a Ben en el Hub](https://huggingface.co/burtenshaw) para ver sus √∫ltimos proyectos.

## Agradecimientos

Nos gustar√≠a expresar nuestro agradecimiento a las siguientes personas y socios por sus valiosas contribuciones y su apoyo:

- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [TRL (Transformer Reinforcement Learning)](https://huggingface.co/docs/trl)
- [PEFT (Parameter-Efficient Fine-Tuning)](https://huggingface.co/docs/peft)

## He encontrado un error o quiero mejorar el curso

Las contribuciones son **bienvenidas** ü§ó

- Si _has encontrado un error üêõ en un notebook_, por favor [abre un issue](https://github.com/huggingface/smol-course/issues/new) y **describe el issue**.
- Si _quieres mejorar el curso_, puedes [abrir un Pull Request](https://github.com/huggingface/smol-course/pulls) .
- Si _quieres a√±adir una secci√≥n completa o una nueva unidad_, lo mejor es [abrir un ticket](https://github.com/huggingface/smol-course/issues/new) y **describir qu√© contenido quieres a√±adir antes de empezar a escribirlo para que podamos orientarte**.

## Todav√≠a tengo preguntas

Por favor, haz tu pregunta en nuestro servidor de Discord #fine-tuning-course-questions.

Ahora que tienes toda la informaci√≥n, ¬°vamos a ponernos en marcha! ‚õµ
