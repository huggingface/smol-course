# Hands-on: Direct Preference Optimization with SmolLM3

In this hands-on notebook, you'll learn to align SmolLM3 with human preferences using Direct Preference Optimization (DPO). We'll use an instruction-tuned SmolLM3 model as our starting point and apply DPO to improve its alignment with human values and preferences.

<Callout variant="tip">

**Prerequisites**: This notebook assumes you have completed Unit 1 (Instruction Tuning) or are familiar with instruction-tuned models. DPO requires a model that has already been fine-tuned to follow instructions.

</Callout>

## Exercise Overview

<Callout variant="exercise" emoji="üéØ">

**Your Mission**: Train SmolLM3 using DPO to create a more aligned language model

**Difficulty Levels**:
- üê¢ **Beginner**: Use the `Anthropic/hh-rlhf` dataset with SmolLM3-3B-Instruct
- üêï **Intermediate**: Experiment with the `argilla/ultrafeedback-binarized-preferences` dataset and different beta values  
- ü¶Å **Advanced**: Use your own preference dataset or combine multiple datasets for multi-domain alignment

</Callout>

## Setup and Installation

Let's start by installing the required libraries and authenticating with Hugging Face:

```python
# Install required packages
!pip install transformers datasets trl torch accelerate bitsandbytes

# Authenticate with Hugging Face
from huggingface_hub import login
login()
```

<Callout variant="info">

Make sure you have a Hugging Face account and access token. You can create one at [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).

</Callout>

## Import Libraries

```python
import torch
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from trl import DPOTrainer, DPOConfig
import numpy as np
from pprint import pprint

# Set device
device = (
    "cuda" if torch.cuda.is_available() 
    else "mps" if torch.backends.mps.is_available() 
    else "cpu"
)
print(f"Using device: {device}")
```

## Load and Explore the Dataset

Let's start with the Anthropic HH-RLHF dataset, which contains human preference data for helpfulness and harmlessness:

```python
# üê¢ Load the Anthropic HH-RLHF dataset
# üêï Try "argilla/ultrafeedback-binarized-preferences" 
# ü¶Å Use your own preference dataset
dataset = load_dataset("Anthropic/hh-rlhf", split="train")

print(f"Dataset size: {len(dataset)}")
print("Dataset features:", dataset.features.keys())

# Examine a sample
sample = dataset[0]
pprint({
    "prompt": sample["prompt"][:200] + "..." if len(sample["prompt"]) > 200 else sample["prompt"],
    "chosen": sample["chosen"][:200] + "..." if len(sample["chosen"]) > 200 else sample["chosen"],
    "rejected": sample["rejected"][:200] + "..." if len(sample["rejected"]) > 200 else sample["rejected"]
})
```

<Callout variant="info">

**Dataset Structure**: DPO datasets contain three key fields:
- `prompt`: The input question or instruction
- `chosen`: The preferred response (higher quality)
- `rejected`: The non-preferred response (lower quality)

</Callout>

### Dataset Preprocessing (if needed)

Some datasets may require preprocessing to match the expected format:

```python
def preprocess_dataset(example):
    """
    Convert dataset to the format expected by DPOTrainer
    Modify this function based on your dataset structure
    """
    # üêï Implement custom preprocessing for different datasets
    return {
        "prompt": example["prompt"],
        "chosen": example["chosen"], 
        "rejected": example["rejected"]
    }

# Apply preprocessing if needed
# dataset = dataset.map(preprocess_dataset)

# Take a subset for faster training (optional)
dataset = dataset.select(range(min(5000, len(dataset))))
print(f"Using {len(dataset)} examples for training")
```

## Load SmolLM3 Model and Tokenizer

We'll use the instruction-tuned version of SmolLM3 as our base model:

```python
# üê¢ Use SmolLM3-3B-Instruct for beginners
# üêï Try SmolLM3-1.7B-Instruct for faster training
# ü¶Å Use your own fine-tuned model from Unit 1
model_name = "HuggingFaceTB/SmolLM3-3B-Instruct"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
    device_map="auto" if device == "cuda" else None,
    trust_remote_code=True
)

# Disable caching for training
model.config.use_cache = False

print(f"Model loaded: {model_name}")
print(f"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
```

<Callout variant="tip">

**Model Choice**: SmolLM3-3B-Instruct is perfect for learning DPO as it:
- Already follows instructions well (good starting point)
- Fits on consumer GPUs
- Demonstrates clear improvement with DPO training

</Callout>

## Configure DPO Training

Now let's set up the DPO training configuration:

```python
# Configure training arguments
training_args = DPOConfig(
    # Core DPO parameters
    beta=0.1,                           # üêï Try different values: 0.05, 0.2, 0.5
    max_prompt_length=512,              # Maximum prompt length
    max_length=1024,                    # Maximum total sequence length
    
    # Training parameters
    learning_rate=5e-7,                 # Lower than standard fine-tuning
    per_device_train_batch_size=2,      # Adjust based on GPU memory
    gradient_accumulation_steps=8,       # Effective batch size = 2 * 8 = 16
    max_steps=500,                      # üêï Try 1000-2000 for better results
    
    # Optimization settings
    warmup_steps=50,
    lr_scheduler_type="cosine",
    gradient_checkpointing=True,        # Saves memory
    
    # Precision and performance
    bf16=True if device == "cuda" else False,
    dataloader_drop_last=True,
    
    # Logging and saving
    logging_steps=10,
    save_steps=250,
    save_strategy="steps",
    output_dir="./smollm3_dpo_results",
    
    # Evaluation
    evaluation_strategy="steps",
    eval_steps=250,
    
    # Disable external logging
    report_to="none",
    
    # Hub settings (for sharing your model)
    push_to_hub=False,                  # Set to True if you want to share
    hub_model_id="your-username/smollm3-dpo",  # ü¶Å Customize your model name
)

print("Training configuration:")
print(f"Beta: {training_args.beta}")
print(f"Learning rate: {training_args.learning_rate}")
print(f"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
print(f"Max steps: {training_args.max_steps}")
```

<Callout variant="info">

**Key Hyperparameters**:
- **Beta (Œ≤)**: Controls preference optimization strength. Start with 0.1, increase for stronger alignment
- **Learning rate**: Much lower than SFT (typically 5e-7 to 5e-6)
- **Max steps**: 500-1000 steps often sufficient for noticeable improvement

</Callout>

## Initialize DPO Trainer

```python
# Create evaluation dataset (optional)
eval_dataset = None
if len(dataset) > 1000:
    # Split dataset for evaluation
    split_dataset = dataset.train_test_split(test_size=0.1, seed=42)
    train_dataset = split_dataset["train"]
    eval_dataset = split_dataset["test"]
else:
    train_dataset = dataset

# Initialize the DPO trainer
trainer = DPOTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
)

print(f"Training dataset size: {len(train_dataset)}")
if eval_dataset:
    print(f"Evaluation dataset size: {len(eval_dataset)}")
```

## Training Loop

Time to train! This will take some time depending on your hardware:

```python
# Start training
print("Starting DPO training...")
trainer.train()

print("Training completed!")

# Save the final model
final_model_path = "./smollm3_dpo_final"
trainer.save_model(final_model_path)
tokenizer.save_pretrained(final_model_path)

print(f"Model saved to: {final_model_path}")
```

<Callout variant="tip">

**Training Tips**:
- Monitor the training loss - it should decrease smoothly
- If loss explodes, reduce learning rate or beta
- Training typically takes 30-60 minutes on a modern GPU

</Callout>

## Evaluate Your Aligned Model

Let's compare the original and DPO-trained models:

```python
# Load the original model for comparison
original_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
    device_map="auto" if device == "cuda" else None,
)

# Load your DPO-trained model
dpo_model = AutoModelForCausalLM.from_pretrained(
    final_model_path,
    torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
    device_map="auto" if device == "cuda" else None,
)

def generate_response(model, tokenizer, prompt, max_length=200):
    """Generate response from a model"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=inputs.input_ids.shape[1] + max_length,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(prompt):].strip()

# Test prompts
test_prompts = [
    "How can I be more productive at work?",
    "What should I do if someone is being rude to me?",
    "Explain quantum computing in simple terms.",
    "Write a short story about a robot learning to paint.",
]

print("üî• Model Comparison Results:\n")

for i, prompt in enumerate(test_prompts, 1):
    print(f"{'='*60}")
    print(f"Test {i}: {prompt}")
    print(f"{'='*60}")
    
    print("üìù Original Model:")
    original_response = generate_response(original_model, tokenizer, prompt)
    print(original_response)
    
    print("\n‚ú® DPO-Aligned Model:")
    dpo_response = generate_response(dpo_model, tokenizer, prompt)
    print(dpo_response)
    print("\n")
```

## Analysis and Next Steps

```python
# üêï Advanced: Quantitative evaluation
def compute_preference_score(model, tokenizer, prompt, chosen, rejected):
    """
    Compute preference score: higher means model prefers chosen over rejected
    """
    # Tokenize inputs
    chosen_inputs = tokenizer(prompt + chosen, return_tensors="pt").to(model.device)
    rejected_inputs = tokenizer(prompt + rejected, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        chosen_logits = model(**chosen_inputs).logits
        rejected_logits = model(**rejected_inputs).logits
    
    # Compute log probabilities (simplified)
    chosen_logprobs = torch.log_softmax(chosen_logits, dim=-1)
    rejected_logprobs = torch.log_softmax(rejected_logits, dim=-1)
    
    # Return difference (positive means chosen is preferred)
    return (chosen_logprobs.mean() - rejected_logprobs.mean()).item()

# Test on a few examples from your dataset
print("üéØ Preference Alignment Analysis:")
print("(Positive scores = model prefers chosen response)")

for i in range(min(3, len(eval_dataset or train_dataset))):
    example = (eval_dataset or train_dataset)[i]
    
    original_score = compute_preference_score(
        original_model, tokenizer, 
        example["prompt"], example["chosen"], example["rejected"]
    )
    
    dpo_score = compute_preference_score(
        dpo_model, tokenizer,
        example["prompt"], example["chosen"], example["rejected"] 
    )
    
    print(f"\nExample {i+1}:")
    print(f"  Original model score: {original_score:.3f}")
    print(f"  DPO model score: {dpo_score:.3f}")
    print(f"  Improvement: {dpo_score - original_score:.3f}")
```

<Callout variant="success">

**Success Indicators**:
- DPO model should show higher preference scores
- Responses should be more helpful, honest, and harmless
- Model should maintain general language capabilities

</Callout>

## üéâ Congratulations!

You've successfully aligned SmolLM3 with human preferences using DPO! Here's what you've accomplished:

1. ‚úÖ **Loaded and preprocessed** a preference dataset
2. ‚úÖ **Configured DPO training** with appropriate hyperparameters
3. ‚úÖ **Trained SmolLM3** to better align with human preferences
4. ‚úÖ **Evaluated** the improvement in alignment quality

## Next Challenges

Ready to take your preference alignment skills further?

<Callout variant="challenge" emoji="üöÄ">

**üêï Intermediate Challenges**:
- Experiment with different beta values (0.05, 0.2, 0.5)
- Try the `argilla/ultrafeedback-binarized-preferences` dataset
- Compare training on different dataset sizes

**ü¶Å Advanced Challenges**:
- Create your own preference dataset for a specific domain
- Implement multi-objective DPO (safety + helpfulness)
- Combine multiple preference datasets
- Deploy your model and collect human feedback

</Callout>

## üîÑ Share Your Results

If you've trained a great model, consider sharing it with the community:

```python
# ü¶Å Share your model (optional)
if training_args.push_to_hub:
    trainer.push_to_hub(
        commit_message="DPO-aligned SmolLM3",
        tags=["smol-course", "dpo", "alignment", "smollm3"]
    )
```

## Resources for Further Learning

- **[TRL Documentation](https://huggingface.co/docs/trl)** - Complete guide to preference alignment
- **[DPO Paper](https://arxiv.org/abs/2305.18290)** - Original research paper
- **[Anthropic HH Paper](https://arxiv.org/abs/2204.05862)** - Human feedback methodology
- **[Alignment Handbook](https://github.com/huggingface/alignment-handbook)** - Advanced alignment techniques

Ready to explore ORPO next? Head to the [ORPO notebook](5) to learn about unified instruction tuning and preference alignment!