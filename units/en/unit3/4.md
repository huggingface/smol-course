# Submit your final project!

It's time to submit your DPO-aligned model! This unit uses the same leaderboard-based submission system as Unit 1. Here's the plan:

1. Read the written guide for the chapter ‚úÖ
2. Train a model using what you learned in the chapter.
3. Push the model to the Hugging Face Hub.
4. Evaluate the model using `hf jobs`.
5. Open a pull request on the leaderboard.

On this page we will go through each step.

## 1. Read the written guide for the chapter and 2. Train a model using what you learned in the chapter.

For Unit 3's submission, you should read all the materials in the unit and train a preference-aligned model using DPO. The training code is provided in:

- [DPO Training Exercise](./4) - Complete DPO training guide with SmolLM3

You'll need to combine this with [Training with Hugging Face Jobs](../unit1/5) techniques from Unit 1.

## 3. Push the model to the Hugging Face Hub

Once you've trained your DPO-aligned model, you'll need to push it to a repo on the Hugging Face Hub. TRL will take care of this for you if you add the `--push_to_hub` flag to your training command.

### DPO Training with Hub Upload:
```bash
hf jobs uv run \
    --flavor a100-large \
    --secrets HF_TOKEN \
    "https://raw.githubusercontent.com/huggingface/trl/main/trl/scripts/dpo.py" \
    --model_name_or_path HuggingFaceTB/SmolLM3-3B-Instruct \
    --dataset_name Anthropic/hh-rlhf \
    --learning_rate 5e-7 \
    --beta 0.1 \
    --max_steps 1000 \
    --push_to_hub \
    --hub_model_id your-username/smollm3-dpo-aligned \
    --report_to trackio
```

Your trained model will be available at `your-username/your-model-name`. For detailed documentation, check out the [checkpoints documentation](https://huggingface.co/docs/transformers/trainer#checkpoints) from `transformers`.

## 4. Evaluate the model using `hf jobs`

Now, we will evaluate your DPO-aligned model. We will use `hf jobs` to evaluate the model and combine it with `lighteval`. We will push the evaluation results to a dataset on the hub.

<Tip>

For DPO evaluation, we use tasks that test both helpfulness and safety aspects of alignment, including `truthfulqa`, `gsm8k`, and other alignment-focused benchmarks.

</Tip>

```bash
hf jobs uv run \
    --flavor a10g-large \
    --with "lighteval[vllm]" \
    --secrets HF_TOKEN \
    lighteval vllm "model_name=<your-username>/<your-model-name>" \
    "lighteval|truthfulqa:mc2|0|0,lighteval|gsm8k|0|0,lighteval|mmlu:abstract_algebra|5|0" \
    --push-to-hub --results-org <your-username>
```

This command will evaluate the model using `lighteval` and `vllm` and save the results to the Hugging Face Hub in a dataset repo that you define.

### Alternative Evaluation Tasks for DPO

You can also evaluate on additional alignment-specific benchmarks:

```bash
# Comprehensive DPO alignment evaluation
hf jobs uv run \
    --flavor a10g-large \
    --with "lighteval[vllm]" \
    --secrets HF_TOKEN \
    lighteval vllm "model_name=<your-username>/<your-model-name>" \
    "lighteval|truthfulqa:mc2|0|0,lighteval|hellaswag|0|0,lighteval|arc:challenge|0|0" \
    --push-to-hub --results-org <your-username>
```

<Tip>

We focus on alignment evaluation in Unit 3, but in Unit 2 we explore evaluation in more detail. The key benchmarks for DPO evaluation include:
- **TruthfulQA**: Tests for truthful and honest responses
- **GSM8K**: Mathematical reasoning and helpfulness
- **MMLU**: Broad knowledge and helpfulness across domains

</Tip>

## 5. Open a pull request on the leaderboard space

You are now ready to submit your DPO-aligned model to the leaderboard! You need to do two things:

1. Add your model's results to `submissions.json`
2. Share your training and evaluation commands in the PR text.

### Add your model's results to `submissions.json`

Open a pull request on the [leaderboard space](https://huggingface.co/spaces/smol-course/leaderboard/edit/main/submissions.json) to submit your model. You just need to add your model info and reference to the dataset you created in the previous step. We will pull the results and display them on the leaderboard.

```json
{
    "submissions": [

        ... // existing submissions
        
        {
            "username": "<your-username>",
            "model_name": "<your-model-name>", 
            "chapter": "3",
            "method": "DPO",
            "submission_date": "<your-submission-date>",
            "results-dataset": "<your-results-dataset>",
            "base_model": "HuggingFaceTB/SmolLM3-3B-Instruct",
            "preference_dataset": "Anthropic/hh-rlhf"
        }
    ]
}
```

### Share your training and evaluation commands in the PR text.

Within the PR text, share both your training and evaluation commands. For example:

```markdown
## Unit 3 Submission: DPO Preference Alignment with SmolLM3

### Training Command:
```bash
hf jobs uv run \
    --flavor a100-large \
    --secrets HF_TOKEN \
    "https://raw.githubusercontent.com/huggingface/trl/main/trl/scripts/dpo.py" \
    --model_name_or_path HuggingFaceTB/SmolLM3-3B-Instruct \
    --dataset_name Anthropic/hh-rlhf \
    --learning_rate 5e-7 \
    --beta 0.1 \
    --max_steps 1000 \
    --push_to_hub \
    --hub_model_id username/smollm3-dpo-aligned
```

### Evaluation Command:
```bash
hf jobs uv run \
    --flavor a10g-large \
    --with "lighteval[vllm]" \
    --secrets HF_TOKEN \
    lighteval vllm "model_name=username/smollm3-dpo-aligned" \
    "lighteval|truthfulqa:mc2|0|0,lighteval|gsm8k|0|0" \
    --push-to-hub --results-org username
```

### Method Used:
- **Approach**: Direct Preference Optimization (DPO)
- **Base Model**: SmolLM3-3B-Instruct
- **Preference Dataset**: Anthropic/hh-rlhf
- **Key Hyperparameters**: Œ≤=0.1, LR=5e-7, 1000 steps
```

This will help us reproduce your model training and evaluation before we add it to the leaderboard.

## What Makes a Good Unit 3 Submission?

For Unit 3, we're specifically looking at DPO preference alignment quality. Great submissions typically demonstrate:

### Technical Excellence
- **Proper hyperparameter choice**: Appropriate Œ≤ values, learning rates, and training steps
- **Dataset selection**: Thoughtful choice of preference datasets for your use case
- **Training stability**: Smooth loss curves and successful completion
- **Clear improvement**: DPO model shows better alignment than base instruction-tuned model

### Alignment Quality  
- **Safety**: Model avoids harmful, biased, or inappropriate responses
- **Helpfulness**: Responses are useful, relevant, and informative
- **Honesty**: Model acknowledges uncertainty and avoids hallucinations
- **Preference accuracy**: Model correctly distinguishes between preferred and rejected responses

### DPO Implementation
- **Proper reference model**: Using the correct instruction-tuned base model
- **Effective beta tuning**: Balancing preference optimization strength
- **Dataset quality**: Using high-quality preference pairs for training
- **Evaluation rigor**: Comprehensive testing on alignment benchmarks

## Evaluation Metrics

Your submission will be evaluated on:

1. **Core Benchmarks**: TruthfulQA, GSM8K, MMLU performance
2. **Alignment Quality**: Safety and helpfulness demonstrated through benchmark performance  
3. **Training Efficiency**: Resource usage and training stability
4. **DPO Effectiveness**: Clear improvement over non-aligned baseline models

## Tips for Success

üéØ **DPO Training Tips**:
- Start with Œ≤=0.1 and adjust based on results
- Use lower learning rates (5e-7) for stability  
- Monitor preference accuracy during training
- Quality of preference data matters more than quantity
- Ensure your reference model matches your training model

üéØ **Hyperparameter Guidelines**:
- **Beta (Œ≤)**: 0.05-0.2 range, start with 0.1
- **Learning Rate**: 5e-7 to 5e-6, lower than SFT
- **Max Steps**: 500-1500 steps usually sufficient
- **Batch Size**: Effective batch size of 16-32 works well

üéØ **General Tips**:
- Test your model locally on diverse prompts before submission
- Document your hyperparameter choices and reasoning
- Compare your results with baseline instruction-tuned models
- Consider the trade-offs between safety and helpfulness
- Monitor both training loss and preference accuracy

## Common Pitfalls to Avoid

‚ùå **Training Issues**:
- Using base models instead of instruction-tuned models for DPO
- Setting Œ≤ too high (causes over-alignment)
- Using learning rates that are too high
- Training for too many steps (causes overfitting)

‚ùå **Dataset Issues**:
- Low-quality preference pairs with unclear distinctions
- Insufficient dataset diversity
- Imbalanced preference annotations
- Using datasets not suited for your target domain

### Wait for the PR to be merged

Once the PR is merged, your DPO-aligned model will be added to the leaderboard! You can check the leaderboard [here](https://huggingface.co/spaces/smol-course/leaderboard).

## Resources and Further Reading

- [Unit 3 DPO Exercise](./4) - Complete DPO training guide
- [DPO Paper](https://arxiv.org/abs/2305.18290) - Original research paper
- [TRL DPO Documentation](https://huggingface.co/docs/trl/dpo_trainer) - Implementation details
- [Anthropic HH-RLHF Paper](https://arxiv.org/abs/2204.05862) - Human feedback methodology
- [Alignment Handbook](https://github.com/huggingface/alignment-handbook) - Advanced alignment techniques

Good luck with your DPO preference alignment submission! üöÄ