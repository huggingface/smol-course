# Hands-on: Unified Training with ORPO and SmolLM3

In this hands-on notebook, you'll experience the power of ORPO (Odds Ratio Preference Optimization) by training SmolLM3 from a base model to a fully aligned assistant in a single training stage. This unified approach combines instruction tuning and preference alignment, making it more efficient than traditional multi-stage methods.

<Callout variant="tip">

**Key Difference**: Unlike DPO, ORPO starts with a base (non-instruct) model and simultaneously learns instruction following AND preference alignment. This makes it perfect for end-to-end training pipelines.

</Callout>

## Exercise Overview

<Callout variant="exercise" emoji="üéØ">

**Your Mission**: Train SmolLM3-Base using ORPO to create an instruction-following, preference-aligned model in one go

**Difficulty Levels**:
- üê¢ **Beginner**: Use `trl-lib/ultrafeedback_binarized` with SmolLM3-3B-Base and default settings
- üêï **Intermediate**: Experiment with `mlabonne/orpo-dpo-mix-40k` and different alpha/beta values
- ü¶Å **Advanced**: Create a multi-domain preference dataset and optimize for specific use cases

</Callout>

## Setup and Installation

```python
# Install required packages
!pip install transformers datasets trl torch accelerate bitsandbytes

# Authenticate with Hugging Face
from huggingface_hub import login
login()
```

<Callout variant="info">

ORPO is computationally efficient, but still benefits from GPU acceleration. If you're on CPU, consider using the 1.7B model or reducing batch sizes.

</Callout>

## Import Libraries

```python
import torch
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from trl import ORPOTrainer, ORPOConfig, setup_chat_format
import numpy as np
from pprint import pprint

# Set device
device = (
    "cuda" if torch.cuda.is_available() 
    else "mps" if torch.backends.mps.is_available() 
    else "cpu"
)
print(f"Using device: {device}")
```

## Load and Explore Preference Dataset

```python
# üê¢ Start with ultrafeedback_binarized
# üêï Try mlabonne/orpo-dpo-mix-40k for diverse preferences
# ü¶Å Create or combine multiple preference datasets
dataset = load_dataset("trl-lib/ultrafeedback_binarized")

print(f"Dataset splits: {list(dataset.keys())}")
print(f"Train size: {len(dataset['train'])}")
print("Dataset features:", dataset["train"].features.keys())

# Examine the structure
sample = dataset["train"][0]
print("\nSample structure:")
pprint({
    "prompt": sample["prompt"][:150] + "..." if len(sample["prompt"]) > 150 else sample["prompt"],
    "chosen": sample["chosen"][:150] + "..." if len(sample["chosen"]) > 150 else sample["chosen"], 
    "rejected": sample["rejected"][:150] + "..." if len(sample["rejected"]) > 150 else sample["rejected"]
})
```

### Prepare Dataset for Training

```python
# Take a manageable subset for training
train_dataset = dataset["train"].select(range(min(10000, len(dataset["train"]))))
eval_dataset = dataset["test"].select(range(min(1000, len(dataset["test"]))))

print(f"Training with {len(train_dataset)} examples")
print(f"Evaluating on {len(eval_dataset)} examples")

# üêï Advanced: Dataset preprocessing for specific domains
def preprocess_for_domain(example):
    \"\"\"
    Custom preprocessing for domain-specific training
    Modify based on your use case
    \"\"\"
    # Add domain-specific system prompts or formatting
    return example

# Apply preprocessing if needed
# train_dataset = train_dataset.map(preprocess_for_domain)
```

<Callout variant="info">

**ORPO Dataset Requirements**: ORPO uses the same preference format as DPO but leverages it for both instruction tuning and preference alignment simultaneously.

</Callout>

## Load SmolLM3 Base Model

The key difference with ORPO is that we start with the base model, not an instruction-tuned version:

```python
# üê¢ SmolLM3-3B-Base for beginners
# üêï SmolLM3-1.7B-Base for faster training  
# ü¶Å SmolLM3-8B-Base if you have sufficient resources
model_name = "HuggingFaceTB/SmolLM3-3B-Base"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load base model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
    device_map="auto" if device == "cuda" else None,
    trust_remote_code=True
)

# Setup chat format for conversation handling
model, tokenizer = setup_chat_format(model, tokenizer)

# Disable caching for training
model.config.use_cache = False

print(f"Model loaded: {model_name}")
print(f"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
print(f"Chat template configured: {tokenizer.chat_template is not None}")
```

<Callout variant="tip">

**Why Base Models?**: ORPO works best with base models because it can simultaneously teach:
1. How to follow instructions (SFT component)
2. Which responses to prefer (preference component)

This unified approach often leads to better-aligned models than sequential training.

</Callout>

## Configure ORPO Training

```python
# Configure ORPO training arguments
training_args = ORPOConfig(
    # Core ORPO parameters  
    beta=0.1,                           # Temperature for odds ratio loss
    max_length=1024,                    # Maximum sequence length
    max_prompt_length=512,              # Maximum prompt length
    
    # Learning parameters
    learning_rate=8e-6,                 # Slightly higher than DPO
    lr_scheduler_type="linear",         # Linear decay works well for ORPO
    num_train_epochs=1,                 # Often sufficient for good alignment
    
    # Batch size and memory management
    per_device_train_batch_size=1,      # Adjust based on GPU memory
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=16,     # Effective batch size = 1 * 16 = 16
    gradient_checkpointing=True,        # Save memory
    
    # Optimization settings
    optim="paged_adamw_8bit" if device == "cuda" else "adamw_torch",
    warmup_steps=100,
    dataloader_drop_last=True,
    
    # Precision
    bf16=True if device == "cuda" else False,
    
    # Evaluation and logging
    evaluation_strategy="steps",
    eval_steps=500,
    logging_steps=25,
    
    # Saving
    save_strategy="steps", 
    save_steps=500,
    output_dir="./smollm3_orpo_results",
    
    # Disable external logging
    report_to="none",
    
    # Hub settings
    push_to_hub=False,                  # Set to True to share your model
    hub_model_id="your-username/smollm3-orpo",  # ü¶Å Customize
)

print("ORPO Configuration:")
print(f"Beta (OR loss weight): {training_args.beta}")
print(f"Learning rate: {training_args.learning_rate}")
print(f"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
print(f"Max length: {training_args.max_length}")
```

<Callout variant="info">

**Key ORPO Hyperparameters**:
- **Beta**: Controls odds ratio loss strength (0.1-0.3 typical range)
- **Learning rate**: Often higher than DPO since we're training from base model
- **Epochs**: Usually 1 epoch is sufficient for good results

</Callout>

## Initialize ORPO Trainer

```python
# Initialize the ORPO trainer
trainer = ORPOTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
)

print("ORPO Trainer initialized")
print(f"Training on {len(train_dataset)} examples")
print(f"Evaluating on {len(eval_dataset)} examples")

# Preview what the trainer will do
print(f"\nTraining will perform:")
print(f"- Supervised fine-tuning to learn instruction following")
print(f"- Preference alignment using odds ratio optimization")
print(f"- Both objectives optimized simultaneously!")
```

## Training with ORPO

Now for the exciting part - unified training!

```python
# Start ORPO training
print("üöÄ Starting ORPO training (unified SFT + preference alignment)...")

# This single training run does both instruction tuning AND preference alignment
trainer.train()

print("‚úÖ ORPO training completed!")

# Save the trained model
final_model_path = "./smollm3_orpo_final"
trainer.save_model(final_model_path)
tokenizer.save_pretrained(final_model_path)

print(f"üìÅ Model saved to: {final_model_path}")
```

<Callout variant="success">

**What Just Happened?**: In a single training run, ORPO has:
1. ‚úÖ Taught your base model to follow instructions
2. ‚úÖ Aligned it with human preferences
3. ‚úÖ Done both more efficiently than traditional methods!

</Callout>

## Evaluate Your ORPO Model

Let's see how well our unified training worked:

```python
# Load models for comparison
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
    device_map="auto" if device == "cuda" else None,
)

orpo_model = AutoModelForCausalLM.from_pretrained(
    final_model_path,
    torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
    device_map="auto" if device == "cuda" else None,
)

# Load tokenizer for generation
gen_tokenizer = AutoTokenizer.from_pretrained(final_model_path)

def generate_response(model, tokenizer, prompt, max_new_tokens=200):
    \"\"\"Generate response using chat format\"\"\"
    messages = [{"role": "user", "content": prompt}]
    chat_prompt = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    inputs = tokenizer(chat_prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(chat_prompt):].strip()

# Test prompts covering different capabilities
test_prompts = [
    "Explain the concept of machine learning to a 10-year-old.",
    "What's the most ethical way to handle a disagreement with a friend?",
    "Write a Python function to calculate the factorial of a number.",
    "How can I be more environmentally conscious in my daily life?",
    "What are the pros and cons of remote work?",
]

print("üîç Comparing Base Model vs ORPO-trained Model:\\n")

for i, prompt in enumerate(test_prompts, 1):
    print(f"{'='*70}")
    print(f"Test {i}: {prompt}")
    print(f"{'='*70}")
    
    print("ü§ñ Base Model (no instruction tuning):")
    try:
        base_response = generate_response(base_model, gen_tokenizer, prompt)
        print(base_response[:300] + "..." if len(base_response) > 300 else base_response)
    except Exception as e:
        print(f"Error: {e}")
    
    print("\\n‚ú® ORPO Model (instruction-tuned + preference-aligned):")
    orpo_response = generate_response(orpo_model, gen_tokenizer, prompt)
    print(orpo_response[:300] + "..." if len(orpo_response) > 300 else orpo_response)
    print("\\n")
```

## Advanced Evaluation Metrics

```python
# üêï Advanced: Quantitative evaluation
def evaluate_preferences(model, tokenizer, examples, num_examples=5):
    \"\"\"Evaluate how well model distinguishes preferred responses\"\"\"
    preference_scores = []
    
    for example in examples[:num_examples]:
        prompt = example["prompt"]
        chosen = example["chosen"]
        rejected = example["rejected"]
        
        # Create full conversations
        chosen_messages = [{"role": "user", "content": prompt}, {"role": "assistant", "content": chosen}]
        rejected_messages = [{"role": "user", "content": prompt}, {"role": "assistant", "content": rejected}]
        
        chosen_text = tokenizer.apply_chat_template(chosen_messages, tokenize=False)
        rejected_text = tokenizer.apply_chat_template(rejected_messages, tokenize=False)
        
        # Compute log probabilities
        chosen_inputs = tokenizer(chosen_text, return_tensors="pt").to(model.device)
        rejected_inputs = tokenizer(rejected_text, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            chosen_logits = model(**chosen_inputs).logits
            rejected_logits = model(**rejected_inputs).logits
        
        # Simple preference score based on average log probability
        chosen_logprobs = torch.log_softmax(chosen_logits, dim=-1).mean()
        rejected_logprobs = torch.log_softmax(rejected_logits, dim=-1).mean()
        
        preference_score = (chosen_logprobs - rejected_logprobs).item()
        preference_scores.append(preference_score)
    
    return np.mean(preference_scores)

# Test preference alignment
print("üéØ Preference Alignment Analysis:")
print("(Higher scores = better preference alignment)\\n")

base_preference_score = evaluate_preferences(base_model, gen_tokenizer, eval_dataset)
orpo_preference_score = evaluate_preferences(orpo_model, gen_tokenizer, eval_dataset)

print(f"Base model preference score: {base_preference_score:.4f}")
print(f"ORPO model preference score: {orpo_preference_score:.4f}")
print(f"Improvement: {orpo_preference_score - base_preference_score:.4f}")

# ü¶Å Domain-specific evaluation
print(f"\\nüéØ Instruction Following Assessment:")

instruction_prompts = [
    "List three benefits of exercise.",
    "Summarize the main causes of climate change in 2 sentences.",
    "Convert this to JSON format: Name: John, Age: 25, City: NYC",
]

print("Testing instruction-following capabilities...")
for prompt in instruction_prompts:
    print(f"\\nPrompt: {prompt}")
    response = generate_response(orpo_model, gen_tokenizer, prompt, max_new_tokens=100)
    print(f"Response: {response}")
```

<Callout variant="tip">

**Evaluation Insights**: Look for:
- ‚úÖ Coherent responses to instructions (vs base model rambling)
- ‚úÖ Helpful and honest answers
- ‚úÖ Preference for higher-quality response patterns
- ‚úÖ Maintained general knowledge and reasoning

</Callout>

## Hyperparameter Exploration

```python
# üêï Experiment with different ORPO settings
orpo_experiments = {
    "conservative": {"beta": 0.05, "learning_rate": 5e-6},
    "balanced": {"beta": 0.1, "learning_rate": 8e-6},  # What we used
    "aggressive": {"beta": 0.3, "learning_rate": 1e-5},
}

print("üß™ ORPO Hyperparameter Guidelines:")
print("=" * 50)

for name, params in orpo_experiments.items():
    print(f"{name.capitalize()} Setting:")
    print(f"  Beta: {params['beta']} - {'Lower' if params['beta'] < 0.1 else 'Higher'} preference alignment")
    print(f"  LR: {params['learning_rate']} - {'Safer' if params['learning_rate'] < 8e-6 else 'Faster'} training")
    print()

print("üí° Tuning Tips:")
print("- Start with balanced settings (beta=0.1, lr=8e-6)")
print("- Increase beta if preference alignment is weak")  
print("- Decrease beta if model becomes repetitive")
print("- Adjust learning rate based on training stability")
```

## üéâ Congratulations!

You've successfully implemented ORPO and created a fully-aligned language model in a single training stage! Here's what you achieved:

1. ‚úÖ **Unified Training**: Combined instruction tuning and preference alignment
2. ‚úÖ **Efficiency**: Achieved alignment with ~50% less compute than DPO+SFT
3. ‚úÖ **Performance**: Created a model that both follows instructions and aligns with preferences
4. ‚úÖ **Practical Skills**: Mastered state-of-the-art preference alignment techniques

## Comparison: ORPO vs Traditional Methods

| Aspect | Traditional (SFT‚ÜíDPO) | ORPO |
|--------|---------------------|------|
| Training Stages | 2 separate stages | 1 unified stage |
| Reference Model | Required | Not required |  
| Memory Usage | Higher | Lower |
| Training Time | ~2x longer | ~50% faster |
| Performance | Excellent | Excellent |
| Complexity | Higher | Lower |

## Next Challenges

<Callout variant="challenge" emoji="üöÄ">

**üêï Intermediate Experiments**:
- Try different beta values (0.05, 0.2, 0.3) and compare results
- Test on `mlabonne/orpo-dpo-mix-40k` dataset
- Compare 1-epoch vs multi-epoch training

**ü¶Å Advanced Projects**:
- Create domain-specific preference datasets (coding, creative writing, etc.)
- Implement multi-objective ORPO with weighted preference types
- Build evaluation pipeline for safety and alignment assessment
- Deploy ORPO model as a chat interface

</Callout>

## Share Your Success

```python
# ü¶Å Share your aligned model with the community
if training_args.push_to_hub:
    trainer.push_to_hub(
        commit_message="ORPO-aligned SmolLM3 - unified instruction tuning + preference alignment",
        tags=["smol-course", "orpo", "alignment", "smollm3", "unified-training"]
    )
    print("üåü Model shared on Hugging Face Hub!")
```

## Key Takeaways

üéØ **ORPO Advantages**:
- **Efficiency**: Single-stage training saves time and resources
- **Simplicity**: No need to manage reference models or complex pipelines  
- **Performance**: Matches or exceeds traditional multi-stage methods
- **Flexibility**: Easy to adapt for different domains and use cases

üéØ **When to Use ORPO**:
- Starting from base models
- Resource-constrained environments
- Rapid prototyping of aligned models
- Domain-specific alignment needs

## Resources and Further Reading

- **[ORPO Paper](https://arxiv.org/abs/2403.07691)** - Original research paper
- **[TRL ORPO Documentation](https://huggingface.co/docs/trl/orpo_trainer)** - Implementation details
- **[Alignment Handbook](https://github.com/huggingface/alignment-handbook)** - Advanced techniques
- **[SmolLM3 Blog](https://hf.co/blog/smollm3)** - Model architecture and capabilities

üéä **You've completed Unit 3!** You now have practical experience with both DPO and ORPO - two of the most powerful preference alignment techniques available. These skills will serve you well as you build more sophisticated and better-aligned language models!