# Introduction to Preference Alignment with SmolLM3

Welcome to Unit 3 of the smollest course on fine-tuning! This module will guide you through preference alignment using **SmolLM3**, building on the instruction tuning foundation from Unit 1. You'll learn how to align language models with human preferences using Direct Preference Optimization (DPO) to create more helpful, harmless, and honest AI assistants.

<Tip>

By the end of this unit you will be aligning an LLM with human preferences using Direct Preference Optimization (DPO). This course is smol but fast! If you're looking for a smoother gradient, check out the [The LLM Course](https://huggingface.co/learn/llm-course/chapter1/1).

</Tip>

## What is Preference Alignment?

While supervised fine-tuning (SFT) teaches models to follow instructions and engage in conversations, preference alignment takes this further by training models to generate responses that match human preferences. It's the process of making AI systems more aligned with what humans actually want, rather than just following instructions literally. In simple terms, it makes language models better for applications in the real world.

Preference alignment addresses several key challenges:

- Safety - Ensuring models don't generate harmful, biased, or inappropriate content
- Helpfulness - Making responses more useful and relevant to human needs
- Honesty - Encouraging truthful responses and reducing hallucinations
- Value alignment - Ensuring model outputs reflect human values and ethics
- Response quality - Improving coherence, relevance, and overall response quality

<Tip>

For a deeper dive into alignment techniques, check out the [Direct Preference Optimization paper](https://huggingface.co/papers/2305.18290) which is the original paper that introduced DPO.

</Tip>

## Direct Preference Optimization (DPO)

DPO revolutionizes preference alignment by eliminating the need for separate reward models and complex reinforcement learning. In this unit, we'll explore Direct Preference Optimization (DPO), the leading technique for aligning language models with human preferences.  You'll learn:

- How DPO directly optimizes model weights using preference data
- The mathematical foundations that make DPO more stable than RLHF
- Practical implementation with the TRL library
- Best practices for preparing and using preference datasets
- Evaluation techniques to measure alignment quality
- Advanced techniques for multi-objective alignment

The DPO alignment pipeline is much simpler than the RLHF alignment pipeline.

1. **Supervised Fine-Tuning**: Adapt the base model to follow instructions
2. **Direct Preference Optimization**: Directly optimize the model using preference data

This means we can train a model on preference data without the need for a reward model or reinforcement learning. This eliminates the reward model and reinforcement learning complexity while achieving comparable or better results.

For exercises in this unit, we will use [SmolLM3](https://hf.co/blog/smollm3) for preference alignment once again. We could use either the instruction tuned model or the result of the unit 1 exercise.

## What You'll Build

Throughout this unit, you'll:

- Train SmolLM3 using DPO on preference datasets
- Master DPO hyperparameter configuration and tuning
- Compare DPO results with baseline instruction-tuned models
- Evaluate model safety and alignment quality using standard benchmarks
- Submit your aligned model to the course leaderboard
- Deploy aligned models for practical applications

Ready to make your models more aligned with human preferences using DPO? Let's begin!