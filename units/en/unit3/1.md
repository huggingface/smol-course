# Introduction to Preference Alignment with SmolLM3

Welcome to Unit 3 of the smollest course of fine-tuning! This module will guide you through preference alignment techniques using **SmolLM3**, building on the instruction tuning foundation from Unit 1. You'll learn how to align language models with human preferences to create more helpful, harmless, and honest AI assistants.

<Tip>

By the end of this unit you will be aligning an LLM with human preferences using DPO and ORPO. This course is smol but fast! If you're looking for a smoother gradient, check out the [The LLM Course](https://huggingface.co/learn/llm-course/chapter1/1).

</Tip>

## What is Preference Alignment?

While supervised fine-tuning (SFT) teaches models to follow instructions and engage in conversations, preference alignment takes this further by training models to generate responses that match human values and preferences. It's the process of making AI systems more aligned with what humans actually want, rather than just following instructions literally.

Preference alignment addresses several key challenges:

- **Safety**: Ensuring models don't generate harmful, biased, or inappropriate content
- **Helpfulness**: Making responses more useful and relevant to human needs
- **Honesty**: Encouraging truthful responses and reducing hallucinations
- **Value alignment**: Ensuring model outputs reflect human values and ethics
- **Response quality**: Improving coherence, relevance, and overall response quality

<Tip>

For a deeper dive into alignment techniques, check out the [Argilla RLHF Guide](https://argilla.io/blog/mantisnlp-rlhf-part-8/) which covers the evolution from RLHF to modern preference alignment methods.

</Tip>

## Why SmolLM3 for Preference Alignment?

[SmolLM3](https://hf.co/blog/smollm3) is ideal for learning preference alignment because it:

- Has already been instruction-tuned, providing a solid foundation for preference alignment
- Fits comfortably on consumer GPUs for cost-effective experimentation
- Demonstrates state-of-the-art performance for its size class
- Supports extended context lengths up to 8k tokens (128k for some variants)
- Features advanced reasoning capabilities including chain-of-thought
- Comes with comprehensive training documentation and examples

## Unit Overview

In this unit, we'll explore two powerful preference alignment techniques:

### Direct Preference Optimization (DPO)

DPO revolutionizes preference alignment by eliminating the need for separate reward models and complex reinforcement learning. You'll learn:

- How DPO directly optimizes model weights using preference data
- The mathematical foundations that make DPO more stable than RLHF
- Practical implementation with the TRL library
- Best practices for preparing and using preference datasets
- Evaluation techniques to measure alignment quality

### Odds Ratio Preference Optimization (ORPO)

ORPO takes efficiency further by combining instruction tuning and preference alignment into a single unified process. You'll discover:

- How ORPO integrates SFT and preference learning in one training stage
- The odds ratio mechanism for contrasting preferred vs rejected responses
- Memory and computational advantages over multi-stage approaches
- Implementation strategies and hyperparameter tuning
- Performance comparisons with traditional alignment methods

## The Alignment Pipeline

Traditional alignment follows a two-stage process:
1. **Supervised Fine-Tuning**: Adapt the base model to follow instructions
2. **Preference Alignment**: Align outputs with human preferences using RLHF or DPO

Modern approaches like ORPO streamline this into a single stage, making alignment more accessible and efficient.

## Prerequisites

Before starting this unit, ensure you have:

- Completed Unit 1 (Instruction Tuning) or equivalent experience with SFT
- A solid understanding of transformer architectures
- Familiarity with Hugging Face transformers and datasets libraries
- Access to compute resources (GPU recommended but not required)
- Basic understanding of machine learning evaluation metrics

## What You'll Build

Throughout this unit, you'll:

- Train SmolLM3 using DPO on preference datasets
- Implement ORPO for unified instruction tuning and alignment
- Compare different preference alignment approaches
- Evaluate model safety and alignment quality
- Deploy aligned models for practical applications

Ready to make your models more aligned with human preferences? Let's begin!