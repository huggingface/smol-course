# Introduction to Preference Alignment with SmolLM3

Welcome to Unit 3 of the smollest course on fine-tuning! This module will guide you through preference alignment using **SmolLM3**, building on the instruction tuning foundation from Unit 1. You'll learn how to align language models with human preferences using Direct Preference Optimization (DPO) to create more helpful, harmless, and honest AI assistants.

<Tip>

By the end of this unit you will be aligning an LLM with human preferences using Direct Preference Optimization (DPO). This course is smol but fast! If you're looking for a smoother gradient, check out the [The LLM Course](https://huggingface.co/learn/llm-course/chapter1/1).

</Tip>

## What is Preference Alignment?

While supervised fine-tuning (SFT) teaches models to follow instructions and engage in conversations, preference alignment takes this further by training models to generate responses that match human preferences. It's the process of making AI systems more aligned with what humans actually want, rather than just following instructions literally. In simple terms, it makes language models better for applications in the real world.

Preference alignment addresses several key challenges:

- Safety - Ensuring models don't generate harmful, biased, or inappropriate content
- Helpfulness - Making responses more useful and relevant to human needs
- Honesty - Encouraging truthful responses and reducing hallucinations
- Value alignment - Ensuring model outputs reflect human values and ethics
- Response quality - Improving coherence, relevance, and overall response quality

<Tip>

For a deeper dive into alignment techniques, check out the [Direct Preference Optimization paper](https://huggingface.co/papers/2305.18290) which is the original paper that introduced DPO.

</Tip>

## Why SmolLM3 for Preference Alignment?

[SmolLM3](https://hf.co/blog/smollm3) is ideal for learning preference alignment because it:

- Has already been instruction-tuned, providing a solid foundation for preference alignment
- Fits comfortably on consumer GPUs for cost-effective experimentation
- Demonstrates state-of-the-art performance for its size class
- Supports extended context lengths up to 8k tokens (128k for some variants)
- Features advanced reasoning capabilities including chain-of-thought
- Comes with comprehensive training documentation and examples

## Direct Preference Optimization (DPO)

DPO revolutionizes preference alignment by eliminating the need for separate reward models and complex reinforcement learning. In this unit, we'll explore Direct Preference Optimization (DPO), the leading technique for aligning language models with human preferences.  You'll learn:

- How DPO directly optimizes model weights using preference data
- The mathematical foundations that make DPO more stable than RLHF
- Practical implementation with the TRL library
- Best practices for preparing and using preference datasets
- Evaluation techniques to measure alignment quality
- Advanced techniques for multi-objective alignment

## The DPO Alignment Pipeline

Traditional RLHF alignment is complex and requires multiple stages:
1. **Supervised Fine-Tuning**: Adapt the base model to follow instructions
2. **Reward Model Training**: Train a separate model to predict human preferences
3. **Reinforcement Learning**: Use PPO to optimize against the reward model

DPO simplifies this dramatically:
1. **Supervised Fine-Tuning**: Adapt the base model to follow instructions
2. **Direct Preference Optimization**: Directly optimize the model using preference data

This eliminates the reward model and reinforcement learning complexity while achieving comparable or better results.

## Prerequisites

Before starting this unit, ensure you have:

- Completed Unit 1 (Instruction Tuning) or equivalent experience with SFT
- A solid understanding of transformer architectures
- Familiarity with Hugging Face transformers and datasets libraries
- Access to compute resources (GPU recommended but not required)
- Basic understanding of machine learning evaluation metrics

## What You'll Build

Throughout this unit, you'll:

- Train SmolLM3 using DPO on preference datasets
- Master DPO hyperparameter configuration and tuning
- Compare DPO results with baseline instruction-tuned models
- Evaluate model safety and alignment quality using standard benchmarks
- Submit your aligned model to the course leaderboard
- Deploy aligned models for practical applications

Ready to make your models more aligned with human preferences using DPO? Let's begin!