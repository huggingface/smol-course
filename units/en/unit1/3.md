# Supervised Fine-Tuning with SmolLM3

Supervised Fine-Tuning (SFT) is the cornerstone of instruction tuning - it's how we transform a base language model into an instruction-following assistant. In this section, you'll learn to fine-tune **SmolLM3** using real-world datasets and production-ready tools.

## What is Supervised Fine-Tuning?

SFT is the process of continuing to train a pre-trained model on task-specific datasets with labeled examples. Think of it as specialized education:

- **Pre-training** teaches the model general language understanding (like learning to read)
- **Supervised fine-tuning** teaches specific skills and behaviors (like learning to be a helpful assistant)

### The SmolLM3 SFT Journey

SmolLM3's instruction-following capabilities come from a sophisticated SFT process:

1. **Base Model** (`SmolLM3-3B-Base`): Trained on 11T tokens of general text
2. **SFT Training**: Fine-tuned on curated instruction datasets including SmolTalk2
3. **Preference Alignment**: Further refined using techniques like APO (Anchored Preference Optimization)

This multi-stage approach creates a model that's both knowledgeable and helpful.

## Why SFT Works: The Science Behind It

SFT is effective because it leverages the rich representations learned during pre-training while adapting the model's behavior patterns. The process works by:

**Behavioral Adaptation**: Teaching the model to recognize instruction patterns and respond appropriately
**Task Specialization**: Focusing the model's vast knowledge on specific use cases  
**Safety Alignment**: Training the model to be helpful, harmless, and honest
**Conversation Skills**: Learning to maintain context and engage naturally

The key insight is that we're not teaching new knowledge from scratch - we're reshaping how existing knowledge is applied.

## When to Use Supervised Fine-Tuning

SFT is your go-to technique when you need to:

### Transform Behavior Patterns
- Convert a base model into a conversational assistant
- Teach specific response styles or formats
- Implement safety guidelines and ethical boundaries

### Domain Specialization
- **Customer Service**: Consistent, professional responses following company guidelines
- **Education**: Tutoring capabilities with step-by-step explanations
- **Technical Support**: Accurate troubleshooting with domain-specific knowledge
- **Creative Writing**: Specific genres, styles, or creative constraints

### Task-Specific Applications
- **Code Generation**: Programming in specific languages or frameworks
- **Data Analysis**: Structured output formats for business intelligence
- **Language Translation**: Improved accuracy for specific language pairs
- **Content Moderation**: Consistent policy enforcement

### Quality Improvements
- **Factual Accuracy**: Reducing hallucinations in specific domains
- **Response Length**: Teaching appropriate verbosity levels
- **Multilingual Support**: Enhancing performance in specific languages
- **Reasoning Skills**: Improving step-by-step problem solving

The key question is: "Does my use case require behavior that differs significantly from general-purpose conversation?" If yes, SFT is likely beneficial.

## The SFT Process: From Data to Deployment

The SFT process follows a systematic approach that ensures high-quality results:

### 1. Dataset Preparation and Selection

**Choose the Right Dataset**:
- **SmolTalk2**: The dataset used to train SmolLM3, containing high-quality instruction-response pairs
- **Domain-specific datasets**: For specialized applications (medical, legal, technical)
- **Custom datasets**: Your own curated examples for specific use cases

**Data Quality Checklist**:
- ✅ Diverse examples covering your use case scenarios
- ✅ High-quality responses that match your desired output style
- ✅ Proper chat template formatting
- ✅ Balanced representation of different task types
- ✅ Clean, well-structured data without errors

### 2. Environment Setup and Configuration

**Hardware Requirements**:
- **Minimum**: 8GB GPU memory (RTX 3080/4070) for basic fine-tuning
- **Recommended**: 16GB+ GPU memory (RTX 4080/4090, A100) for optimal performance
- **Alternative**: Google Colab Pro with A100 access

**Software Stack**:
```bash
pip install transformers>=4.36.0
pip install trl>=0.7.0
pip install datasets>=2.14.0
pip install accelerate>=0.24.0
pip install peft>=0.7.0  # For LoRA/QLoRA
pip install bitsandbytes>=0.41.0  # For quantization
```

### 3. Training Configuration

**Key Hyperparameters**:
- **Learning Rate**: 5e-5 to 1e-4 (start with 5e-5 for SmolLM3)
- **Batch Size**: 4-16 depending on GPU memory
- **Max Sequence Length**: 2048-4096 tokens
- **Training Steps**: 1000-5000 depending on dataset size
- **Warmup Steps**: 10% of total training steps

### 4. Monitoring and Evaluation

Throughout training, monitor:
- **Training Loss**: Should decrease steadily
- **Validation Loss**: Should decrease without overfitting
- **Sample Outputs**: Regular generation quality checks
- **Resource Usage**: GPU memory and training speed

**Experiment Tracking with Trackio**:
For comprehensive experiment tracking, we recommend **[Trackio](https://huggingface.co/docs/trackio)** - a lightweight, free experiment tracking library built on Hugging Face infrastructure. Trackio provides:

- **Drop-in Replacement**: API compatible with `wandb.init`, `wandb.log`, and `wandb.finish`
- **Local-first Design**: Dashboard runs locally by default, with optional Hugging Face Spaces hosting
- **Free Hosting**: Everything, including hosting on Hugging Face Spaces, is completely free
- **Lightweight**: <3,000 lines of Python code, easily extensible

```python
# Simple Trackio integration
import trackio as wandb

# Initialize tracking
wandb.init(project="smollm3-sft")

# Log metrics during training
wandb.log({"train_loss": 0.5, "learning_rate": 5e-5})

# Finish tracking
wandb.finish()
```

We'll dive deeper into evaluation techniques in [Module 4](../4_evaluation).

## SFT in the Broader Training Pipeline

SFT is the crucial first step in a multi-stage training process:

### Stage 1: Supervised Fine-Tuning (SFT)
- Transform base model into instruction-following assistant
- Learn basic conversation patterns and task understanding
- Establish safety guidelines and helpful behavior

### Stage 2: Preference Alignment
- **DPO (Direct Preference Optimization)**: Learn from preference data
- **ORPO (Odds Ratio Preference Optimization)**: Combine SFT and preference learning
- **APO (Anchored Preference Optimization)**: SmolLM3's approach for stable alignment

SmolLM3 uses APO for preference alignment, which provides more stable training and better performance than traditional RLHF approaches.

## Supervised Fine-Tuning with TRL (Transformer Reinforcement Learning)

**TRL** is the go-to toolkit for training language models, built specifically for instruction tuning and alignment. It's what we'll use throughout this course.

### Why TRL?

- **Production Ready**: Used by major organizations and research labs
- **Comprehensive**: Supports SFT, DPO, ORPO, PPO, and more advanced techniques
- **Efficient**: Optimized for memory usage and training speed
- **Flexible**: Works with any Hugging Face model
- **CLI Support**: Command-line tools for scalable training workflows

### Key Components

**SFTTrainer**: The core class for supervised fine-tuning
**SFTConfig**: Configuration management for training parameters
**CLI Tools**: Command-line interface for production workflows
**Integration**: Seamless integration with Hugging Face Hub, Trackio, Weights & Biases, and more

### TRL's Architecture

TRL is built on top of the Hugging Face ecosystem:
- **Transformers**: Model loading and inference
- **Datasets**: Data processing and management  
- **Accelerate**: Distributed training and optimization
- **PEFT**: Parameter-efficient fine-tuning (LoRA, QLoRA)

This integrated approach means you get all the benefits of the Hugging Face ecosystem while using state-of-the-art training techniques.

## Hands-On: Your First SmolLM3 Fine-Tune

Ready to put theory into practice? Here's a preview of what you'll build in the exercises. You can use either Python or CLI approach:

<frameworkcontent>
<pythontab>

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset
import trackio as wandb

# Initialize experiment tracking
wandb.init(project="smollm3-sft", name="my-first-sft-run")

# Load SmolLM3 base model
model = AutoModelForCausalLM.from_pretrained("HuggingFaceTB/SmolLM3-3B-Base")
tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM3-3B-Base")

# Load SmolTalk2 dataset
dataset = load_dataset("HuggingFaceTB/smoltalk2", "SFT")

# Configure training with Trackio integration
config = SFTConfig(
    output_dir="./smollm3-finetuned",
    per_device_train_batch_size=4,
    learning_rate=5e-5,
    max_steps=1000,
    report_to="trackio",  # Enable Trackio logging
)

# Train!
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    args=config,
)
trainer.train()

# Finish experiment tracking
wandb.finish()
```

</pythontab>
<clitab>

```bash
# Fine-tune SmolLM3 using TRL CLI with Trackio tracking
trl sft \
    --model_name_or_path HuggingFaceTB/SmolLM3-3B-Base \
    --dataset_name HuggingFaceTB/smoltalk2 \
    --dataset_config SFT \
    --output_dir ./smollm3-sft-model \
    --per_device_train_batch_size 4 \
    --learning_rate 5e-5 \
    --max_steps 1000 \
    --logging_steps 50 \
    --save_steps 200 \
    --report_to trackio \
    --push_to_hub \
    --hub_model_id your-username/smollm3-custom
```

</clitab>
</frameworkcontent>

## Cloud Training Options

While you can train models locally, cloud infrastructure offers significant advantages for SFT training. For users who want to skip the complexity of GPU setup and environment management, **Hugging Face Jobs** provides a seamless solution.

🚀 **[Training with Hugging Face Jobs](./5.md)** - Learn how to use fully managed cloud infrastructure for hassle-free SFT training with high-end GPUs, automatic scaling, and integrated monitoring.


## Key Takeaways

1. **SFT is Essential**: It's the bridge between base models and instruction-following assistants
2. **Data Quality Matters**: High-quality datasets lead to better fine-tuned models
3. **TRL Simplifies Everything**: From research to production, TRL provides the tools you need
4. **SmolLM3 is Perfect for Learning**: Powerful enough to be useful, small enough to be accessible
5. **Multiple Approaches**: Both programmatic and CLI workflows for different use cases

## Next Steps

Now that you understand the theory, choose your training approach:

⏭️ [Training with Hugging Face Jobs](./5.md) - Use cloud infrastructure for hassle-free training
⏭️ [Hands-On Exercises](./4.md) - Fine-tune your own SmolLM3 model locally or in the cloud

## Resources and Further Reading

- [Training with Hugging Face Jobs](./5.md) - Cloud-based training with managed infrastructure
- [Trackio Documentation](https://huggingface.co/docs/trackio) - Free, lightweight experiment tracking
- [TRL Documentation](https://huggingface.co/docs/trl) - Comprehensive guide to all TRL features
- [SFTTrainer API Reference](https://huggingface.co/docs/trl/sft_trainer) - Detailed parameter documentation
- [SmolTalk2 Dataset](https://huggingface.co/datasets/HuggingFaceTB/smoltalk2) - The dataset that trained SmolLM3
- [SmolLM3 Training Blog Post](https://huggingface.co/blog/smollm3) - Deep dive into SmolLM3's training process
- [TRL CLI Documentation](https://huggingface.co/docs/trl/clis) - Command-line interface guide
