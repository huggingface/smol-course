# Supervised Fine-Tuning with SmolLM3

Supervised Fine-Tuning (SFT) is the cornerstone of instruction tuning - it's how we transform a base language model into an instruction-following assistant. In this section, you'll learn to fine-tune **SmolLM3** using real-world datasets and production-ready tools.

## What is Supervised Fine-Tuning?

SFT is the process of continuing to train a pre-trained model on task-specific datasets with labeled examples. Think of it as specialized education:

- **Pre-training** teaches the model general language understanding (like learning to read)
- **Supervised fine-tuning** teaches specific skills and behaviors (like learning to be a helpful assistant)

### The SmolLM3 SFT Journey

SmolLM3's instruction-following capabilities come from a sophisticated SFT process:

1. **Base Model** (`SmolLM3-3B-Base`): Trained on 11T tokens of general text
2. **SFT Training**: Fine-tuned on curated instruction datasets including SmolTalk2
3. **Preference Alignment**: Further refined using techniques like APO (Anchored Preference Optimization)

This multi-stage approach creates a model that's both knowledgeable and helpful.

## Why SFT Works: The Science Behind It

SFT is effective because it leverages the rich representations learned during pre-training while adapting the model's behavior patterns. The process works by:

**Behavioral Adaptation**: Teaching the model to recognize instruction patterns and respond appropriately
**Task Specialization**: Focusing the model's vast knowledge on specific use cases  
**Safety Alignment**: Training the model to be helpful, harmless, and honest
**Conversation Skills**: Learning to maintain context and engage naturally

The key insight is that we're not teaching new knowledge from scratch - we're reshaping how existing knowledge is applied.

## When to Use Supervised Fine-Tuning

SFT is your go-to technique when you need to:

### Transform Behavior Patterns
- Convert a base model into a conversational assistant
- Teach specific response styles or formats
- Implement safety guidelines and ethical boundaries

### Domain Specialization
- **Customer Service**: Consistent, professional responses following company guidelines
- **Education**: Tutoring capabilities with step-by-step explanations
- **Technical Support**: Accurate troubleshooting with domain-specific knowledge
- **Creative Writing**: Specific genres, styles, or creative constraints

### Task-Specific Applications
- **Code Generation**: Programming in specific languages or frameworks
- **Data Analysis**: Structured output formats for business intelligence
- **Language Translation**: Improved accuracy for specific language pairs
- **Content Moderation**: Consistent policy enforcement

### Quality Improvements
- **Factual Accuracy**: Reducing hallucinations in specific domains
- **Response Length**: Teaching appropriate verbosity levels
- **Multilingual Support**: Enhancing performance in specific languages
- **Reasoning Skills**: Improving step-by-step problem solving

The key question is: "Does my use case require behavior that differs significantly from general-purpose conversation?" If yes, SFT is likely beneficial.

## The SFT Process: From Data to Deployment

The SFT process follows a systematic approach that ensures high-quality results:

### 1. Dataset Preparation and Selection

**Choose the Right Dataset**:
- **SmolTalk2**: The dataset used to train SmolLM3, containing high-quality instruction-response pairs
- **Domain-specific datasets**: For specialized applications (medical, legal, technical)
- **Custom datasets**: Your own curated examples for specific use cases

**Data Quality Checklist**:
- ✅ Diverse examples covering your use case scenarios
- ✅ High-quality responses that match your desired output style
- ✅ Proper chat template formatting
- ✅ Balanced representation of different task types
- ✅ Clean, well-structured data without errors

### 2. Environment Setup and Configuration

**Hardware Requirements**:
- **Minimum**: 8GB GPU memory (RTX 3080/4070) for basic fine-tuning
- **Recommended**: 16GB+ GPU memory (RTX 4080/4090, A100) for optimal performance
- **Alternative**: Google Colab Pro with A100 access

**Software Stack**:
```bash
pip install transformers>=4.36.0
pip install trl>=0.7.0
pip install datasets>=2.14.0
pip install accelerate>=0.24.0
pip install peft>=0.7.0  # For LoRA/QLoRA
pip install bitsandbytes>=0.41.0  # For quantization
```

### 3. Training Configuration

**Key Hyperparameters**:
- **Learning Rate**: 5e-5 to 1e-4 (start with 5e-5 for SmolLM3)
- **Batch Size**: 4-16 depending on GPU memory
- **Max Sequence Length**: 2048-4096 tokens
- **Training Steps**: 1000-5000 depending on dataset size
- **Warmup Steps**: 10% of total training steps

### 4. Monitoring and Evaluation

Throughout training, monitor:
- **Training Loss**: Should decrease steadily
- **Validation Loss**: Should decrease without overfitting
- **Sample Outputs**: Regular generation quality checks
- **Resource Usage**: GPU memory and training speed

We'll dive deeper into evaluation techniques in [Module 4](../4_evaluation).

## SFT in the Broader Training Pipeline

SFT is the crucial first step in a multi-stage training process:

### Stage 1: Supervised Fine-Tuning (SFT)
- Transform base model into instruction-following assistant
- Learn basic conversation patterns and task understanding
- Establish safety guidelines and helpful behavior

### Stage 2: Preference Alignment
- **DPO (Direct Preference Optimization)**: Learn from preference data
- **ORPO (Odds Ratio Preference Optimization)**: Combine SFT and preference learning
- **APO (Anchored Preference Optimization)**: SmolLM3's approach for stable alignment

SmolLM3 uses APO for preference alignment, which provides more stable training and better performance than traditional RLHF approaches.

## Supervised Fine-Tuning with TRL (Transformer Reinforcement Learning)

**TRL** is the go-to toolkit for training language models, built specifically for instruction tuning and alignment. It's what we'll use throughout this course.

### Why TRL?

- **Production Ready**: Used by major organizations and research labs
- **Comprehensive**: Supports SFT, DPO, ORPO, PPO, and more advanced techniques
- **Efficient**: Optimized for memory usage and training speed
- **Flexible**: Works with any Hugging Face model
- **CLI Support**: Command-line tools for scalable training workflows

### Key Components

**SFTTrainer**: The core class for supervised fine-tuning
**SFTConfig**: Configuration management for training parameters
**CLI Tools**: Command-line interface for production workflows
**Integration**: Seamless integration with Hugging Face Hub, Weights & Biases, and more

### TRL's Architecture

TRL is built on top of the Hugging Face ecosystem:
- **Transformers**: Model loading and inference
- **Datasets**: Data processing and management  
- **Accelerate**: Distributed training and optimization
- **PEFT**: Parameter-efficient fine-tuning (LoRA, QLoRA)

This integrated approach means you get all the benefits of the Hugging Face ecosystem while using state-of-the-art training techniques.

## Hands-On: Your First SmolLM3 Fine-Tune

Ready to put theory into practice? Here's a preview of what you'll build in the exercises. You can use either Python or CLI approach:

<frameworkcontent>
<pythontab>

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset

# Load SmolLM3 base model
model = AutoModelForCausalLM.from_pretrained("HuggingFaceTB/SmolLM3-3B-Base")
tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM3-3B-Base")

# Load SmolTalk2 dataset
dataset = load_dataset("HuggingFaceTB/smoltalk2", "SFT")

# Configure training
config = SFTConfig(
    output_dir="./smollm3-finetuned",
    per_device_train_batch_size=4,
    learning_rate=5e-5,
    max_steps=1000,
)

# Train!
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    args=config,
)
trainer.train()
```

</pythontab>
<clitab>

```bash
# Fine-tune SmolLM3 using TRL CLI
trl sft \
    --model_name_or_path HuggingFaceTB/SmolLM3-3B-Base \
    --dataset_name HuggingFaceTB/smoltalk2 \
    --dataset_config SFT \
    --output_dir ./smollm3-sft-model \
    --per_device_train_batch_size 4 \
    --learning_rate 5e-5 \
    --max_steps 1000 \
    --logging_steps 50 \
    --save_steps 200 \
    --push_to_hub \
    --hub_model_id your-username/smollm3-custom
```

</clitab>
</frameworkcontent>

## Key Takeaways

1. **SFT is Essential**: It's the bridge between base models and instruction-following assistants
2. **Data Quality Matters**: High-quality datasets lead to better fine-tuned models
3. **TRL Simplifies Everything**: From research to production, TRL provides the tools you need
4. **SmolLM3 is Perfect for Learning**: Powerful enough to be useful, small enough to be accessible
5. **Multiple Approaches**: Both programmatic and CLI workflows for different use cases

## Next Steps

Now that you understand the theory, it's time for hands-on practice:

⏭️ [Hands-On Exercises](./4.md) - Fine-tune your own SmolLM3 model

## Resources and Further Reading

- [TRL Documentation](https://huggingface.co/docs/trl) - Comprehensive guide to all TRL features
- [SFTTrainer API Reference](https://huggingface.co/docs/trl/sft_trainer) - Detailed parameter documentation
- [SmolTalk2 Dataset](https://huggingface.co/datasets/HuggingFaceTB/smoltalk2) - The dataset that trained SmolLM3
- [SmolLM3 Training Blog Post](https://huggingface.co/blog/smollm3) - Deep dive into SmolLM3's training process
- [TRL CLI Documentation](https://huggingface.co/docs/trl/clis) - Command-line interface guide
